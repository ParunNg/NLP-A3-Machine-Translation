{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Translation + Transformer\n",
    "\n",
    "<img src = \"../figures/transformer1.png\" >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Neo\\anaconda3\\envs\\dsai\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch, torchdata, torchtext\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import random, math, time\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "#make our work comparable if restarted the kernel\n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.1+cu118'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.16.1+cpu'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchtext.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 1. ETL: Loading the dataset\n",
    "\n",
    "**Note**: Here I chose to translate English to German, simply it is easier for myself, since I don't understand German so it is difficult for me to imagine a sentence during inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Neo\\anaconda3\\envs\\dsai\\lib\\site-packages\\datasets\\load.py:1429: FutureWarning: The repository for opus100 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/opus100\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "SRC_LANGUAGE = 'en'\n",
    "TRG_LANGUAGE = 'th'\n",
    "\n",
    "dataset = datasets.load_dataset(\"opus100\", \"en-th\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 1000000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 1000000/1000000 [00:17<00:00, 56796.62 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from numpy.random import default_rng\n",
    "\n",
    "rng = default_rng(seed=SEED)\n",
    "# create a list of non-repeated indices of size 10000 and use it to select the training samples\n",
    "select_idx = rng.choice(len(dataset['train']), size=1000, replace=False)\n",
    "dataset['train'] = dataset['train'].filter(lambda example, idx: idx in select_idx, with_indices=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1000/1000 [00:00<00:00, 7631.74 examples/s]\n",
      "Map: 100%|██████████| 1000/1000 [00:00<00:00, 10098.77 examples/s]\n"
     ]
    }
   ],
   "source": [
    "get_lang_col = lambda example, lang: {lang: example['translation'][lang]}\n",
    "dataset = dataset.map(get_lang_col, fn_kwargs={'lang': \"th\"})\n",
    "dataset = dataset.map(get_lang_col, remove_columns=['translation'], fn_kwargs={'lang': \"en\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'th': '-ชั้นทำอยู่', 'en': '- I am blending.'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['th', 'en'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['th', 'en'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['th', 'en'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val, test = dataset['train'], dataset['test'], dataset['validation']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 3. Preprocessing \n",
    "\n",
    "### Tokenizing\n",
    "\n",
    "**Note**: the models must first be downloaded using the following on the command line: \n",
    "```\n",
    "python3 -m spacy download en_core_web_sm\n",
    "python3 -m spacy download de_core_news_sm\n",
    "```\n",
    "\n",
    "First, since we have two languages, let's create some constants to represent that.  Also, let's create two dicts: one for holding our tokenizers and one for holding all the vocabs with assigned numbers for each unique word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pythainlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place-holders\n",
    "token_transform = {}\n",
    "vocab_transform = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from pythainlp.tokenize import Tokenizer\n",
    "\n",
    "token_transform[SRC_LANGUAGE] = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "token_transform[TRG_LANGUAGE] = Tokenizer(engine='newmm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence:  -ชั้นทำอยู่\n",
      "Tokenization:  ['-', 'ชั้น', 'ทำ', 'อยู่']\n"
     ]
    }
   ],
   "source": [
    "#example of tokenization of the thai part\n",
    "print(\"Sentence: \", dataset['train'][TRG_LANGUAGE][2])\n",
    "print(\"Tokenization: \", token_transform[TRG_LANGUAGE].word_tokenize(dataset['train'][TRG_LANGUAGE][2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1000/1000 [00:00<00:00, 3205.14 examples/s]\n",
      "Map: 100%|██████████| 2000/2000 [00:00<00:00, 4866.19 examples/s]\n",
      "Map: 100%|██████████| 2000/2000 [00:00<00:00, 4555.47 examples/s]\n",
      "Map: 100%|██████████| 1000/1000 [00:00<00:00, 4201.70 examples/s]\n",
      "Map: 100%|██████████| 2000/2000 [00:00<00:00, 4854.27 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def tokenize_data(example, lang):\n",
    "    try:\n",
    "        return {lang: token_transform[lang](example[lang].lower())}\n",
    "    except:\n",
    "        return {lang: token_transform[lang].word_tokenize(example[lang].lower())}\n",
    "    \n",
    "tokenized_dataset = dataset.map(tokenize_data, remove_columns=[SRC_LANGUAGE], fn_kwargs={'lang': SRC_LANGUAGE})\n",
    "tokenized_dataset = tokenized_dataset.map(tokenize_data, remove_columns=[TRG_LANGUAGE], fn_kwargs={'lang': TRG_LANGUAGE})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'th': ['พระองค์',\n",
       "  'คือ',\n",
       "  'ผู้ทรง',\n",
       "  'ประทาน',\n",
       "  'คัมภีร์',\n",
       "  'ลงมา',\n",
       "  'แก่',\n",
       "  'เจ้า',\n",
       "  ' ',\n",
       "  'โดยที่',\n",
       "  'ส่วนหนึ่ง',\n",
       "  'จาก',\n",
       "  'คัมภีร์',\n",
       "  'นั้น',\n",
       "  'มี',\n",
       "  'บรรดา',\n",
       "  'โฮ',\n",
       "  'ง',\n",
       "  'การ',\n",
       "  'ที่',\n",
       "  'มี',\n",
       "  'ข้อความ',\n",
       "  'รัดกุม',\n",
       "  'ชัดเจน',\n",
       "  ' ',\n",
       "  'ซึ่ง',\n",
       "  'โองการ',\n",
       "  'เหล่านั้น',\n",
       "  ' ',\n",
       "  'คือ',\n",
       "  'รากฐาน',\n",
       "  'ของ',\n",
       "  'คัมภีร์',\n",
       "  ' ',\n",
       "  'และ',\n",
       "  'มี',\n",
       "  'โองการ',\n",
       "  'อื่น',\n",
       "  ' ',\n",
       "  'ไ',\n",
       "  ' ',\n",
       "  'อีก',\n",
       "  'ที่',\n",
       "  'มี',\n",
       "  'ข้อความ',\n",
       "  'เป็นนัย',\n",
       "  ' ',\n",
       "  'ส่วน',\n",
       "  'บรรดา',\n",
       "  'ผู้',\n",
       "  'ที่',\n",
       "  'ใน',\n",
       "  'หัวใจ',\n",
       "  'ของ',\n",
       "  'พวกเขา',\n",
       "  'มี',\n",
       "  'การ',\n",
       "  'เอนเอียง',\n",
       "  'ออกจาก',\n",
       "  'ความจริง',\n",
       "  'นั้น',\n",
       "  ' ',\n",
       "  'เขา',\n",
       "  'จะ',\n",
       "  'ติดตาม',\n",
       "  'โองการ',\n",
       "  'ที่',\n",
       "  'มี',\n",
       "  'ข้อความ',\n",
       "  'เป็นนัย',\n",
       "  'จาก',\n",
       "  'คัมภีร์',\n",
       "  ' ',\n",
       "  'ทั้งนี้',\n",
       "  ' ',\n",
       "  'เพื่อ',\n",
       "  'แสวงหา',\n",
       "  'ความวุ่นวาย',\n",
       "  ' ',\n",
       "  'และ',\n",
       "  'เพื่อ',\n",
       "  'แสวงหา',\n",
       "  'การ',\n",
       "  'ตี',\n",
       "  'ความใน',\n",
       "  'โองการ',\n",
       "  'นั้น',\n",
       "  ' ',\n",
       "  'แล',\n",
       "  'ไม่',\n",
       "  'มี',\n",
       "  'ใคร',\n",
       "  'รู้',\n",
       "  'ใน',\n",
       "  'การ',\n",
       "  'ตีความ',\n",
       "  'โองการ',\n",
       "  'นั้น',\n",
       "  'ได้',\n",
       "  'นอกจาก',\n",
       "  'อัลลอฮ์',\n",
       "  ' ',\n",
       "  'และ',\n",
       "  'บรรดา',\n",
       "  'ผู้',\n",
       "  'ที่',\n",
       "  'มั่นคง',\n",
       "  'ใน',\n",
       "  'ความรู้',\n",
       "  'เท่านั้น',\n",
       "  ' ',\n",
       "  'โดยที่',\n",
       "  'พวกเขา',\n",
       "  'จะ',\n",
       "  'กล่าวว่า',\n",
       "  ' ',\n",
       "  'พวกเรา',\n",
       "  'ศรัทธา',\n",
       "  'ต่อ',\n",
       "  'โองการ',\n",
       "  'นั้น',\n",
       "  ' ',\n",
       "  'ทั้งหมด',\n",
       "  'นั้น',\n",
       "  'มาจาก',\n",
       "  'ที่',\n",
       "  'ที่',\n",
       "  'พระผู้เป็นเจ้า',\n",
       "  'ของ',\n",
       "  'เรา',\n",
       "  'ทั้งสิ้น',\n",
       "  ' ',\n",
       "  'และ',\n",
       "  'ไม่',\n",
       "  'มี',\n",
       "  'ใคร',\n",
       "  'ที่จะ',\n",
       "  'รับ',\n",
       "  'คำตักเตือน',\n",
       "  'นอกจาก',\n",
       "  'บรรดา',\n",
       "  'ผู้',\n",
       "  'ที่',\n",
       "  'มีสติปัญญา',\n",
       "  'เท่านั้น'],\n",
       " 'en': ['it',\n",
       "  'is',\n",
       "  'he',\n",
       "  'who',\n",
       "  'has',\n",
       "  'sent',\n",
       "  'down',\n",
       "  'to',\n",
       "  'you',\n",
       "  '(',\n",
       "  'muhammad',\n",
       "  'saw',\n",
       "  ')',\n",
       "  'the',\n",
       "  'book',\n",
       "  '(',\n",
       "  'this',\n",
       "  'quran',\n",
       "  ')',\n",
       "  '.',\n",
       "  'in',\n",
       "  'it',\n",
       "  'are',\n",
       "  'verses',\n",
       "  'that',\n",
       "  'are',\n",
       "  'entirely',\n",
       "  'clear',\n",
       "  ',',\n",
       "  'they',\n",
       "  'are',\n",
       "  'the',\n",
       "  'foundations',\n",
       "  'of',\n",
       "  'the',\n",
       "  'book',\n",
       "  '[',\n",
       "  'and',\n",
       "  'those',\n",
       "  'are',\n",
       "  'the',\n",
       "  'verses',\n",
       "  'of',\n",
       "  'al',\n",
       "  '-',\n",
       "  'ahkam',\n",
       "  '(',\n",
       "  'commandments',\n",
       "  ',',\n",
       "  'etc',\n",
       "  '.',\n",
       "  ')',\n",
       "  ',',\n",
       "  'al',\n",
       "  '-',\n",
       "  \"fara'id\",\n",
       "  '(',\n",
       "  'obligatory',\n",
       "  'duties',\n",
       "  ')',\n",
       "  'and',\n",
       "  'al',\n",
       "  '-',\n",
       "  'hudud',\n",
       "  '(',\n",
       "  'legal',\n",
       "  'laws',\n",
       "  'for',\n",
       "  'the',\n",
       "  'punishment',\n",
       "  'of',\n",
       "  'thieves',\n",
       "  ',',\n",
       "  'adulterers',\n",
       "  ',',\n",
       "  'etc',\n",
       "  '.',\n",
       "  ')',\n",
       "  ']',\n",
       "  ';',\n",
       "  'and',\n",
       "  'others',\n",
       "  'not',\n",
       "  'entirely',\n",
       "  'clear',\n",
       "  '.',\n",
       "  'so',\n",
       "  'as',\n",
       "  'for',\n",
       "  'those',\n",
       "  'in',\n",
       "  'whose',\n",
       "  'hearts',\n",
       "  'there',\n",
       "  'is',\n",
       "  'a',\n",
       "  'deviation',\n",
       "  '(',\n",
       "  'from',\n",
       "  'the',\n",
       "  'truth',\n",
       "  ')',\n",
       "  'they',\n",
       "  'follow',\n",
       "  'that',\n",
       "  'which',\n",
       "  'is',\n",
       "  'not',\n",
       "  'entirely',\n",
       "  'clear',\n",
       "  'thereof',\n",
       "  ',',\n",
       "  'seeking',\n",
       "  'al',\n",
       "  '-',\n",
       "  'fitnah',\n",
       "  '(',\n",
       "  'polytheism',\n",
       "  'and',\n",
       "  'trials',\n",
       "  ',',\n",
       "  'etc',\n",
       "  '.',\n",
       "  ')',\n",
       "  ',',\n",
       "  'and',\n",
       "  'seeking',\n",
       "  'for',\n",
       "  'its',\n",
       "  'hidden',\n",
       "  'meanings',\n",
       "  ',',\n",
       "  'but',\n",
       "  'none',\n",
       "  'knows',\n",
       "  'its',\n",
       "  'hidden',\n",
       "  'meanings',\n",
       "  'save',\n",
       "  'allah',\n",
       "  '.',\n",
       "  'and',\n",
       "  'those',\n",
       "  'who',\n",
       "  'are',\n",
       "  'firmly',\n",
       "  'grounded',\n",
       "  'in',\n",
       "  'knowledge',\n",
       "  'say',\n",
       "  ':',\n",
       "  '\"',\n",
       "  'we',\n",
       "  'believe',\n",
       "  'in',\n",
       "  'it',\n",
       "  ';',\n",
       "  'the',\n",
       "  'whole',\n",
       "  'of',\n",
       "  'it',\n",
       "  '(',\n",
       "  'clear',\n",
       "  'and',\n",
       "  'unclear',\n",
       "  'verses',\n",
       "  ')',\n",
       "  'are',\n",
       "  'from',\n",
       "  'our',\n",
       "  'lord',\n",
       "  '.',\n",
       "  '\"',\n",
       "  'and',\n",
       "  'none',\n",
       "  'receive',\n",
       "  'admonition',\n",
       "  'except',\n",
       "  'men',\n",
       "  'of',\n",
       "  'understanding',\n",
       "  '.',\n",
       "  '(',\n",
       "  'tafsir',\n",
       "  'at',\n",
       "  '-',\n",
       "  'tabari',\n",
       "  ')',\n",
       "  '.']}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define special symbols and indices\n",
    "UNK_IDX, PAD_IDX, SOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
    "special_symbols = ['<unk>', '<pad>', '<sos>', '<eos>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lang in [SRC_LANGUAGE, TRG_LANGUAGE]:\n",
    "    vocab_transform[lang] = torchtext.vocab.build_vocab_from_iterator(tokenized_dataset['train'][lang],\n",
    "                                                                      min_freq=3,   #if not, everything will be treated as UNK\n",
    "                                                                      specials=special_symbols,\n",
    "                                                                      special_first=True) #indicates whether to insert symbols at the beginning or at the end)\n",
    "    # Set UNK_IDX as the default index. This index is returned when the token is not found. \n",
    "    # If not set, it throws RuntimeError when the queried token is not found in the Vocabulary. \n",
    "    vocab_transform[lang].set_default_index(UNK_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[40, 0]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#see some example\n",
    "vocab_transform[SRC_LANGUAGE](['my', 'precious'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[13, 202, 13, 78]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#see some example\n",
    "vocab_transform[TRG_LANGUAGE](['ของ', 'รัก', 'ของ', 'ข้า'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ของ'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we can reverse it....\n",
    "mapping = vocab_transform[TRG_LANGUAGE].get_itos()\n",
    "\n",
    "#print 1816, for example\n",
    "mapping[13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<unk>'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's try unknown vocab\n",
    "mapping[0]\n",
    "#they will all map to <unk> which has 0 as integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<pad>', '<sos>', '<eos>')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's try special symbols\n",
    "mapping[1], mapping[2], mapping[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "458"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check unique vocabularies\n",
    "len(mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4. Preparing the dataloader\n",
    "\n",
    "One thing we change here is the <code>collate_fn</code> which now also returns the length of sentence.  This is required for <code>packed_padded_sequence</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# helper function to club together sequential operations\n",
    "def sequential_transforms(*transforms):\n",
    "    def func(txt_input):\n",
    "        for transform in transforms:\n",
    "            try:\n",
    "                txt_input = transform(txt_input)\n",
    "            except TypeError:\n",
    "                txt_input = transform.word_tokenize(txt_input)\n",
    "        return txt_input\n",
    "    return func\n",
    "\n",
    "# function to add BOS/EOS and create tensor for input sequence indices\n",
    "def tensor_transform(token_ids):\n",
    "    return torch.cat((torch.tensor([SOS_IDX]), \n",
    "                      torch.tensor(token_ids), \n",
    "                      torch.tensor([EOS_IDX])))\n",
    "\n",
    "# src and trg language text transforms to convert raw strings into tensors indices\n",
    "text_transform = {}\n",
    "for ln in [SRC_LANGUAGE, TRG_LANGUAGE]:\n",
    "    text_transform[ln] = sequential_transforms(token_transform[ln], #Tokenization\n",
    "                                               vocab_transform[ln], #Numericalization\n",
    "                                               tensor_transform) # Add BOS/EOS and create tensor\n",
    "\n",
    "\n",
    "# function to collate data samples into batch tesors\n",
    "def collate_batch(batch):\n",
    "    src_batch, src_len_batch, trg_batch = [], [], []\n",
    "    for sample in batch:\n",
    "        processed_text = text_transform[SRC_LANGUAGE](sample[SRC_LANGUAGE].rstrip(\"\\n\"))\n",
    "        src_batch.append(processed_text)\n",
    "        trg_batch.append(text_transform[TRG_LANGUAGE](sample[TRG_LANGUAGE].rstrip(\"\\n\")))\n",
    "        src_len_batch.append(processed_text.size(0))\n",
    "\n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX, batch_first = True) #<----need this because we use linear layers mostly\n",
    "    trg_batch = pad_sequence(trg_batch, padding_value=PAD_IDX, batch_first = True)\n",
    "    return src_batch, torch.tensor(src_len_batch, dtype=torch.int64), trg_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create train, val, and test dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_loader = DataLoader(train, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
    "valid_loader = DataLoader(val,   batch_size=batch_size, shuffle=False, collate_fn=collate_batch)\n",
    "test_loader  = DataLoader(test,  batch_size=batch_size, shuffle=False, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the train loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for en, _, th in train_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  2,  78, 437,  ...,   1,   1,   1],\n",
       "        [  2,   0,   0,  ...,   1,   1,   1],\n",
       "        [  2,   0,  15,  ...,   1,   1,   1],\n",
       "        ...,\n",
       "        [  2, 153,  55,  ...,   1,   1,   1],\n",
       "        [  2,   0,   3,  ...,   1,   1,   1],\n",
       "        [  2, 187, 294,  ...,   1,   1,   1]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English shape:  torch.Size([64, 32])\n",
      "Thai shape:  torch.Size([64, 37])\n"
     ]
    }
   ],
   "source": [
    "print(\"English shape: \", en.shape)  # (batch_size, seq len)\n",
    "print(\"Thai shape: \", th.shape)   # (batch_size, seq len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Design the model\n",
    "\n",
    "<img src=\"../figures/transformer-encoder.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mEncoderLayer\u001b[39;00m(\u001b[43mnn\u001b[49m\u001b[38;5;241m.\u001b[39mModule):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, hid_dim, n_heads, pf_dim, dropout, attn_variant, device):\n\u001b[0;32m      3\u001b[0m         \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, pf_dim, dropout, attn_variant, device):\n",
    "        super().__init__()\n",
    "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.ff_layer_norm        = nn.LayerNorm(hid_dim)\n",
    "        self.self_attention       = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, attn_variant, device)\n",
    "        self.feedforward          = PositionwiseFeedforwardLayer(hid_dim, pf_dim, dropout)\n",
    "        self.dropout              = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src, src_mask):\n",
    "        #src = [batch size, src len, hid dim]\n",
    "        #src_mask = [batch size, 1, 1, src len]   #if the token is padding, it will be 1, otherwise 0\n",
    "        _src, _ = self.self_attention(src, src, src, src_mask)\n",
    "        src     = self.self_attn_layer_norm(src + self.dropout(_src))\n",
    "        #src: [batch_size, src len, hid dim]\n",
    "        \n",
    "        _src    = self.feedforward(src)\n",
    "        src     = self.ff_layer_norm(src + self.dropout(_src))\n",
    "        #src: [batch_size, src len, hid dim]\n",
    "        \n",
    "        return src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hid_dim, n_layers, n_heads, pf_dim, dropout, attn_variant, device, max_length = 500):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.attn_variant = attn_variant\n",
    "        self.tok_embedding = nn.Embedding(input_dim, hid_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
    "        self.layers        = nn.ModuleList([EncoderLayer(hid_dim, n_heads, pf_dim, dropout, attn_variant, device)\n",
    "                                           for _ in range(n_layers)])\n",
    "        self.dropout       = nn.Dropout(dropout)\n",
    "        self.scale         = torch.sqrt(torch.FloatTensor([hid_dim])).to(self.device)\n",
    "        \n",
    "    def forward(self, src, src_mask):\n",
    "        \n",
    "        #src = [batch size, src len]\n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "        \n",
    "        batch_size = src.shape[0]\n",
    "        src_len    = src.shape[1]\n",
    "        \n",
    "        pos        = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
    "        #pos: [batch_size, src_len]\n",
    "        \n",
    "        src        = self.dropout((self.tok_embedding(src) * self.scale) + self.pos_embedding(pos))\n",
    "        #src: [batch_size, src_len, hid_dim]\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            src = layer(src, src_mask)\n",
    "        #src: [batch_size, src_len, hid_dim]\n",
    "        \n",
    "        return src\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mutli Head Attention Layer\n",
    "\n",
    "<img src = \"../figures/transformer-attention.png\" width=\"700\">\n",
    "\n",
    "$$ \\text{Attention}(Q, K, V) = \\text{Softmax} \\big( \\frac{QK^T}{\\sqrt{d_k}} \\big)V $$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdditiveAttention(nn.Module):\n",
    "    def __init__(self, hid_dim, device):\n",
    "        super(AdditiveAttention, self).__init__()\n",
    "        self.v = torch.nn.Parameter(\n",
    "            torch.FloatTensor(self.hid_dim).uniform_(-0.1, 0.1))\n",
    "        self.linear_in = nn.Linear(hid_dim * 2, hid_dim)  # Adjusted input size\n",
    "        self.linear_out = nn.Linear(hid_dim, hid_dim)  # Adjusted output size\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, query, key):\n",
    "        combined = torch.cat([query, key], dim=-1)\n",
    "        energy = torch.tanh(self.linear_in(combined))\n",
    "        scores = self.linear_out(energy) @ self.v\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mMultiHeadAttentionLayer\u001b[39;00m(\u001b[43mnn\u001b[49m\u001b[38;5;241m.\u001b[39mModule):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, hid_dim, n_heads, dropout, attn_variant, device):\n\u001b[0;32m      3\u001b[0m         \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class MultiHeadAttentionLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, dropout, attn_variant, device):\n",
    "        super().__init__()\n",
    "        assert hid_dim % n_heads == 0\n",
    "        self.hid_dim  = hid_dim\n",
    "        self.n_heads  = n_heads\n",
    "        self.head_dim = hid_dim // n_heads\n",
    "        self.attn_variant = attn_variant\n",
    "        \n",
    "        self.fc_q     = nn.Linear(hid_dim, hid_dim)\n",
    "        self.fc_k     = nn.Linear(hid_dim, hid_dim)\n",
    "        self.fc_v     = nn.Linear(hid_dim, hid_dim)\n",
    "        \n",
    "        self.fc_o     = nn.Linear(hid_dim, hid_dim)\n",
    "        \n",
    "        self.dropout  = nn.Dropout(dropout)\n",
    "        \n",
    "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
    "\n",
    "        self.additive_attention = AdditiveAttention(hid_dim, device)\n",
    "                \n",
    "    def forward(self, query, key, value, mask = None):\n",
    "        #src, src, src, src_mask\n",
    "        #query = [batch size, query len, hid dim]\n",
    "        #key = [batch size, key len, hid dim]\n",
    "        #value = [batch size, value len, hid dim]\n",
    "        \n",
    "        batch_size = query.shape[0]\n",
    "        \n",
    "        Q = self.fc_q(query)\n",
    "        K = self.fc_k(key)\n",
    "        V = self.fc_v(value)\n",
    "        #Q=K=V: [batch_size, src len, hid_dim]\n",
    "        \n",
    "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        #Q = [batch_size, n heads, query len, head_dim]\n",
    "        \n",
    "        if self.attn_variant == \"multiplicative\":\n",
    "            energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n",
    "            #Q = [batch_size, n heads, query len, head_dim] @ K = [batch_size, n heads, head_dim, key len]\n",
    "            #energy = [batch_size, n heads, query len, key len]\n",
    "\n",
    "        elif self.attn_variant == \"general\":\n",
    "            energy = torch.matmul(Q, K.permute(0, 1, 3, 2))\n",
    "\n",
    "        elif self.attn_variant == \"additive\":\n",
    "            energy = self.additive_attention(Q, K)\n",
    "\n",
    "        else:\n",
    "            raise Exception(\"Incorrect value for attention variant. Must be one of the following: \\\n",
    "                            multiplicative, additive, general\")\n",
    "        \n",
    "        #for making attention to padding to 0\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, -1e10)\n",
    "            \n",
    "        attention = torch.softmax(energy, dim = -1)\n",
    "        #attention = [batch_size, n heads, query len, key len]\n",
    "\n",
    "        x = torch.matmul(self.dropout(attention), V)\n",
    "        #[batch_size, n heads, query len, key len] @ [batch_size, n heads, value len, head_dim]\n",
    "        #x = [batch_size, n heads, query len, head dim]\n",
    "        \n",
    "        x = x.permute(0, 2, 1, 3).contiguous()  #we can perform .view\n",
    "        #x = [batch_size, query len, n heads, head dim]\n",
    "        \n",
    "        x = x.view(batch_size, -1, self.hid_dim)\n",
    "        #x = [batch_size, query len, hid dim]\n",
    "        \n",
    "        x = self.fc_o(x)\n",
    "        #x = [batch_size, query len, hid dim]\n",
    "        \n",
    "        return x, attention\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Position-wise Feedforward Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mPositionwiseFeedforwardLayer\u001b[39;00m(\u001b[43mnn\u001b[49m\u001b[38;5;241m.\u001b[39mModule):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, hid_dim, pf_dim, dropout):\n\u001b[0;32m      3\u001b[0m         \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class PositionwiseFeedforwardLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, pf_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(hid_dim, pf_dim)\n",
    "        self.fc2 = nn.Linear(pf_dim, hid_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #x = [batch size, src len, hid dim]\n",
    "        x = self.dropout(torch.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Decoder Layer\n",
    "\n",
    "<img src = \"../figures/transformer-decoder.png\" >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mDecoderLayer\u001b[39;00m(\u001b[43mnn\u001b[49m\u001b[38;5;241m.\u001b[39mModule):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, hid_dim, n_heads, pf_dim, dropout, attn_variant, device):\n\u001b[0;32m      3\u001b[0m         \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, pf_dim, dropout, attn_variant, device):\n",
    "        super().__init__()\n",
    "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.enc_attn_layer_norm  = nn.LayerNorm(hid_dim)\n",
    "        self.ff_layer_norm        = nn.LayerNorm(hid_dim)\n",
    "        self.self_attention       = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, attn_variant, device)\n",
    "        self.encoder_attention    = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, attn_variant, device)\n",
    "        self.feedforward          = PositionwiseFeedforwardLayer(hid_dim, pf_dim, dropout)\n",
    "        self.dropout              = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "        \n",
    "        #trg = [batch size, trg len, hid dim]\n",
    "        #enc_src = [batch size, src len, hid dim]\n",
    "        #trg_mask = [batch size, 1, trg len, trg len]\n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "        \n",
    "        _trg, _ = self.self_attention(trg, trg, trg, trg_mask)\n",
    "        trg     = self.self_attn_layer_norm(trg + self.dropout(_trg))\n",
    "        #trg = [batch_size, trg len, hid dim]\n",
    "        \n",
    "        _trg, attention = self.encoder_attention(trg, enc_src, enc_src, src_mask)\n",
    "        trg             = self.enc_attn_layer_norm(trg + self.dropout(_trg))\n",
    "        #trg = [batch_size, trg len, hid dim]\n",
    "        #attention = [batch_size, n heads, trg len, src len]\n",
    "        \n",
    "        _trg = self.feedforward(trg)\n",
    "        trg  = self.ff_layer_norm(trg + self.dropout(_trg))\n",
    "        #trg = [batch_size, trg len, hid dim]\n",
    "        \n",
    "        return trg, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mDecoder\u001b[39;00m(\u001b[43mnn\u001b[49m\u001b[38;5;241m.\u001b[39mModule):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, output_dim, hid_dim, n_layers, n_heads, \n\u001b[0;32m      3\u001b[0m                  pf_dim, dropout, attn_variant, device, max_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m500\u001b[39m):\n\u001b[0;32m      4\u001b[0m         \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, hid_dim, n_layers, n_heads, \n",
    "                 pf_dim, dropout, attn_variant, device, max_length = 500):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.tok_embedding = nn.Embedding(output_dim, hid_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
    "        self.layers        = nn.ModuleList([DecoderLayer(hid_dim, n_heads, pf_dim, dropout, attn_variant, device)\n",
    "                                            for _ in range(n_layers)])\n",
    "        self.fc_out        = nn.Linear(hid_dim, output_dim)\n",
    "        self.dropout       = nn.Dropout(dropout)\n",
    "        self.scale         = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
    "        \n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "        \n",
    "        #trg = [batch size, trg len]\n",
    "        #enc_src = [batch size, src len, hid dim]\n",
    "        #trg_mask = [batch size, 1, trg len, trg len]\n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "        \n",
    "        batch_size = trg.shape[0]\n",
    "        trg_len    = trg.shape[1]\n",
    "        \n",
    "        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
    "        #pos: [batch_size, trg len]\n",
    "        \n",
    "        trg = self.dropout((self.tok_embedding(trg) * self.scale) + self.pos_embedding(pos))\n",
    "        #trg: [batch_size, trg len, hid dim]\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            trg, attention = layer(trg, enc_src, trg_mask, src_mask)\n",
    "            \n",
    "        #trg: [batch_size, trg len, hid dim]\n",
    "        #attention: [batch_size, n heads, trg len, src len]\n",
    "        \n",
    "        output = self.fc_out(trg)\n",
    "        #output = [batch_size, trg len, output_dim]\n",
    "        \n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting them together (become Seq2Seq!)\n",
    "\n",
    "Our `trg_sub_mask` will look something like this (for a target with 5 tokens):\n",
    "\n",
    "$$\\begin{matrix}\n",
    "1 & 0 & 0 & 0 & 0\\\\\n",
    "1 & 1 & 0 & 0 & 0\\\\\n",
    "1 & 1 & 1 & 0 & 0\\\\\n",
    "1 & 1 & 1 & 1 & 0\\\\\n",
    "1 & 1 & 1 & 1 & 1\\\\\n",
    "\\end{matrix}$$\n",
    "\n",
    "The \"subsequent\" mask is then logically anded with the padding mask, this combines the two masks ensuring both the subsequent tokens and the padding tokens cannot be attended to. For example if the last two tokens were `<pad>` tokens the mask would look like:\n",
    "\n",
    "$$\\begin{matrix}\n",
    "1 & 0 & 0 & 0 & 0\\\\\n",
    "1 & 1 & 0 & 0 & 0\\\\\n",
    "1 & 1 & 1 & 0 & 0\\\\\n",
    "1 & 1 & 1 & 0 & 0\\\\\n",
    "1 & 1 & 1 & 0 & 0\\\\\n",
    "\\end{matrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mSeq2SeqTransformer\u001b[39;00m(\u001b[43mnn\u001b[49m\u001b[38;5;241m.\u001b[39mModule):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, encoder, decoder, src_pad_idx, trg_pad_idx, device):\n\u001b[0;32m      3\u001b[0m         \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(self, encoder, decoder, src_pad_idx, trg_pad_idx, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        self.device = device\n",
    "        \n",
    "    def make_src_mask(self, src):\n",
    "        \n",
    "        #src = [batch size, src len]\n",
    "        \n",
    "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "\n",
    "        return src_mask\n",
    "    \n",
    "    def make_trg_mask(self, trg):\n",
    "        \n",
    "        #trg = [batch size, trg len]\n",
    "        \n",
    "        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        #trg_pad_mask = [batch size, 1, 1, trg len]\n",
    "        \n",
    "        trg_len = trg.shape[1]\n",
    "        \n",
    "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device = self.device)).bool()\n",
    "        #trg_sub_mask = [trg len, trg len]\n",
    "            \n",
    "        trg_mask = trg_pad_mask & trg_sub_mask\n",
    "        #trg_mask = [batch size, 1, trg len, trg len]\n",
    "        \n",
    "        return trg_mask\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        \n",
    "        #src = [batch size, src len]\n",
    "        #trg = [batch size, trg len]\n",
    "                \n",
    "        src_mask = self.make_src_mask(src)\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "        \n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "        #trg_mask = [batch size, 1, trg len, trg len]\n",
    "        \n",
    "        enc_src = self.encoder(src, src_mask)\n",
    "        #enc_src = [batch size, src len, hid dim]\n",
    "                \n",
    "        output, attention = self.decoder(trg, enc_src, trg_mask, src_mask)\n",
    "        \n",
    "        #output = [batch size, trg len, output dim]\n",
    "        #attention = [batch size, n heads, trg len, src len]\n",
    "        \n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 6. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(m):\n",
    "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
    "        nn.init.xavier_uniform_(m.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2SeqTransformer(\n",
       "  (encoder): Encoder(\n",
       "    (tok_embedding): Embedding(370, 256)\n",
       "    (pos_embedding): Embedding(500, 256)\n",
       "    (layers): ModuleList(\n",
       "      (0-2): 3 x EncoderLayer(\n",
       "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (additive_attention): AdditiveAttention(\n",
       "            (linear_in): Linear(in_features=512, out_features=256, bias=True)\n",
       "            (linear_out): Linear(in_features=256, out_features=8, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (feedforward): PositionwiseFeedforwardLayer(\n",
       "          (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (tok_embedding): Embedding(458, 256)\n",
       "    (pos_embedding): Embedding(500, 256)\n",
       "    (layers): ModuleList(\n",
       "      (0-2): 3 x DecoderLayer(\n",
       "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (enc_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (additive_attention): AdditiveAttention(\n",
       "            (linear_in): Linear(in_features=512, out_features=256, bias=True)\n",
       "            (linear_out): Linear(in_features=256, out_features=8, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (encoder_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (additive_attention): AdditiveAttention(\n",
       "            (linear_in): Linear(in_features=512, out_features=256, bias=True)\n",
       "            (linear_out): Linear(in_features=256, out_features=8, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (feedforward): PositionwiseFeedforwardLayer(\n",
       "          (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (fc_out): Linear(in_features=256, out_features=458, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dim   = len(vocab_transform[SRC_LANGUAGE])\n",
    "output_dim  = len(vocab_transform[TRG_LANGUAGE])\n",
    "hid_dim = 256\n",
    "enc_layers = 3\n",
    "dec_layers = 3\n",
    "enc_heads = 8\n",
    "dec_heads = 8\n",
    "enc_pf_dim = 512\n",
    "dec_pf_dim = 512\n",
    "enc_dropout = 0.1\n",
    "dec_dropout = 0.1\n",
    "attn_variant = 'multiplicative'\n",
    "\n",
    "SRC_PAD_IDX = PAD_IDX\n",
    "TRG_PAD_IDX = PAD_IDX\n",
    "\n",
    "enc = Encoder(input_dim, \n",
    "              hid_dim, \n",
    "              enc_layers, \n",
    "              enc_heads, \n",
    "              enc_pf_dim, \n",
    "              enc_dropout, \n",
    "              attn_variant,\n",
    "              device)\n",
    "\n",
    "dec = Decoder(output_dim, \n",
    "              hid_dim, \n",
    "              dec_layers, \n",
    "              dec_heads, \n",
    "              dec_pf_dim, \n",
    "              enc_dropout, \n",
    "              attn_variant,\n",
    "              device)\n",
    "\n",
    "model = Seq2SeqTransformer(enc, dec, SRC_PAD_IDX, TRG_PAD_IDX, device).to(device)\n",
    "model.apply(initialize_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 94720\n",
      "128000\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      "131072\n",
      "   256\n",
      "  2048\n",
      "     8\n",
      "131072\n",
      "   512\n",
      "131072\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      "131072\n",
      "   256\n",
      "  2048\n",
      "     8\n",
      "131072\n",
      "   512\n",
      "131072\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      "131072\n",
      "   256\n",
      "  2048\n",
      "     8\n",
      "131072\n",
      "   512\n",
      "131072\n",
      "   256\n",
      "117248\n",
      "128000\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      "131072\n",
      "   256\n",
      "  2048\n",
      "     8\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      "131072\n",
      "   256\n",
      "  2048\n",
      "     8\n",
      "131072\n",
      "   512\n",
      "131072\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      "131072\n",
      "   256\n",
      "  2048\n",
      "     8\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      "131072\n",
      "   256\n",
      "  2048\n",
      "     8\n",
      "131072\n",
      "   512\n",
      "131072\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      "131072\n",
      "   256\n",
      "  2048\n",
      "     8\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      "131072\n",
      "   256\n",
      "  2048\n",
      "     8\n",
      "131072\n",
      "   512\n",
      "131072\n",
      "   256\n",
      "117248\n",
      "   458\n",
      "______\n",
      "5739794\n"
     ]
    }
   ],
   "source": [
    "#we can print the complexity by the number of parameters\n",
    "def count_parameters(model):\n",
    "    params = [p.numel() for p in model.parameters() if p.requires_grad]\n",
    "    for item in params:\n",
    "        print(f'{item:>6}')\n",
    "    print(f'______\\n{sum(params):>6}')\n",
    "    \n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "lr = 0.0005\n",
    "\n",
    "#training hyperparameters\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX) #combine softmax with cross entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we'll define our training loop. This is the exact same as the one used in the previous tutorial.\n",
    "\n",
    "As we want our model to predict the `<eos>` token but not have it be an input into our model we simply slice the `<eos>` token off the end of the sequence. Thus:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\text{trg} &= [sos, x_1, x_2, x_3, eos]\\\\\n",
    "\\text{trg[:-1]} &= [sos, x_1, x_2, x_3]\n",
    "\\end{align*}$$\n",
    "\n",
    "$x_i$ denotes actual target sequence element. We then feed this into the model to get a predicted sequence that should hopefully predict the `<eos>` token:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\text{output} &= [y_1, y_2, y_3, eos]\n",
    "\\end{align*}$$\n",
    "\n",
    "$y_i$ denotes predicted target sequence element. We then calculate our loss using the original `trg` tensor with the `<sos>` token sliced off the front, leaving the `<eos>` token:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\text{output} &= [y_1, y_2, y_3, eos]\\\\\n",
    "\\text{trg[1:]} &= [x_1, x_2, x_3, eos]\n",
    "\\end{align*}$$\n",
    "\n",
    "We then calculate our losses and update our parameters as is standard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, optimizer, criterion, clip, loader_length):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for src, src_len, trg in loader:\n",
    "        \n",
    "        src = src.to(device)\n",
    "        trg = trg.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #trg[:, :-1] remove the eos, e.g., \"<sos> I love sushi\" since teaching forcing, the input does not need to have eos\n",
    "        output, _ = model(src, trg[:,:-1])\n",
    "                \n",
    "        #output = [batch size, trg len - 1, output dim]\n",
    "        #trg    = [batch size, trg len]\n",
    "            \n",
    "        output_dim = output.shape[-1]\n",
    "            \n",
    "        output = output.reshape(-1, output_dim)\n",
    "        trg = trg[:,1:].reshape(-1) #trg[:, 1:] remove the sos, e.g., \"i love sushi <eos>\" since in teaching forcing, the output does not have sos\n",
    "                \n",
    "        #output = [batch size * trg len - 1, output dim]\n",
    "        #trg    = [batch size * trg len - 1]\n",
    "            \n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / loader_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our evaluation loop is similar to our training loop, however as we aren't updating any parameters we don't need to pass an optimizer or a clip value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader, criterion, loader_length):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for src, src_len, trg in loader:\n",
    "        \n",
    "            src = src.to(device)\n",
    "            trg = trg.to(device)\n",
    "\n",
    "            output, _ = model(src, trg[:,:-1])\n",
    "            \n",
    "            #output = [batch size, trg len - 1, output dim]\n",
    "            #trg = [batch size, trg len]\n",
    "            \n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            output = output.contiguous().view(-1, output_dim)\n",
    "            trg = trg[:,1:].contiguous().view(-1)\n",
    "            \n",
    "            #output = [batch size * trg len - 1, output dim]\n",
    "            #trg = [batch size * trg len - 1]\n",
    "            \n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / loader_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting everything together\n",
    "\n",
    "Finally, we train our actual model. This model is almost 3x faster than the convolutional sequence-to-sequence model and also achieves a lower validation perplexity!\n",
    "\n",
    "**Note: similar to CNN, this model always has a teacher forcing ratio of 1, i.e. it will always use the ground truth next token from the target sequence (this is simply because CNN do everything in parallel so we cannot have the next token). This means we cannot compare perplexity values against the previous models when they are using a teacher forcing ratio that is not 1. To understand this, try run previous tutorials with teaching forcing ratio of 1, you will get very low perplexity.  **   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_length = len(list(iter(train_loader)))\n",
    "val_loader_length   = len(list(iter(valid_loader)))\n",
    "test_loader_length  = len(list(iter(test_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### additive #####\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (19456x64 and 512x256)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[160], line 37\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m     35\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 37\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclip\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m     valid_loss \u001b[38;5;241m=\u001b[39m evaluate(model, valid_loader, criterion, val_loader_length)\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;66;03m#for plotting\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[156], line 15\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, loader, optimizer, criterion, clip, loader_length)\u001b[0m\n\u001b[0;32m     12\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m#trg[:, :-1] remove the eos, e.g., \"<sos> I love sushi\" since teaching forcing, the input does not need to have eos\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m output, _ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrg\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m#output = [batch size, trg len - 1, output dim]\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m#trg    = [batch size, trg len]\u001b[39;00m\n\u001b[0;32m     20\u001b[0m output_dim \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[151], line 48\u001b[0m, in \u001b[0;36mSeq2SeqTransformer.forward\u001b[1;34m(self, src, trg)\u001b[0m\n\u001b[0;32m     43\u001b[0m trg_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_trg_mask(trg)\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m#src_mask = [batch size, 1, 1, src len]\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m#trg_mask = [batch size, 1, trg len, trg len]\u001b[39;00m\n\u001b[1;32m---> 48\u001b[0m enc_src \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m#enc_src = [batch size, src len, hid dim]\u001b[39;00m\n\u001b[0;32m     51\u001b[0m output, attention \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(trg, enc_src, trg_mask, src_mask)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[145], line 28\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[1;34m(self, src, src_mask)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m#src: [batch_size, src_len, hid_dim]\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m---> 28\u001b[0m     src \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m#src: [batch_size, src_len, hid_dim]\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m src\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[144], line 13\u001b[0m, in \u001b[0;36mEncoderLayer.forward\u001b[1;34m(self, src, src_mask)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, src, src_mask):\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;66;03m#src = [batch size, src len, hid dim]\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;66;03m#src_mask = [batch size, 1, 1, src len]   #if the token is padding, it will be 1, otherwise 0\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m     _src, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m     src     \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn_layer_norm(src \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(_src))\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;66;03m#src: [batch_size, src len, hid dim]\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[147], line 49\u001b[0m, in \u001b[0;36mMultiHeadAttentionLayer.forward\u001b[1;34m(self, query, key, value, mask)\u001b[0m\n\u001b[0;32m     46\u001b[0m     energy \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(Q, K\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m2\u001b[39m))\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn_variant \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madditive\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 49\u001b[0m     energy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madditive_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mQ\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mK\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncorrect value for attention variant. Must be one of the following: \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;124m                    multiplicative, additive, general\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[146], line 12\u001b[0m, in \u001b[0;36mAdditiveAttention.forward\u001b[1;34m(self, query, key)\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLast dimensions of query and key must be the same for additive attention.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     11\u001b[0m combined \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([query, key], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 12\u001b[0m energy \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtanh(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear_in\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcombined\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     13\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear_out(energy)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m scores\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (19456x64 and 512x256)"
     ]
    }
   ],
   "source": [
    "num_epochs = 1\n",
    "clip       = 1\n",
    "\n",
    "for attn_variant in ['additive', 'general', 'multiplicative']:\n",
    "    enc = Encoder(input_dim,\n",
    "                  hid_dim, \n",
    "                  enc_layers, \n",
    "                  enc_heads, \n",
    "                  enc_pf_dim, \n",
    "                  enc_dropout, \n",
    "                  attn_variant, \n",
    "                  device)\n",
    "\n",
    "    dec = Decoder(output_dim, \n",
    "                  hid_dim, \n",
    "                  dec_layers, \n",
    "                  dec_heads, \n",
    "                  dec_pf_dim, \n",
    "                  enc_dropout, \n",
    "                  attn_variant, \n",
    "                  device)\n",
    "\n",
    "    model = Seq2SeqTransformer(enc, dec, SRC_PAD_IDX, TRG_PAD_IDX, device).to(device)\n",
    "    model.apply(initialize_weights)\n",
    "    save_path = f'models/{attn_variant}_{model.__class__.__name__}.pt'\n",
    "\n",
    "    best_valid_loss = float('inf')\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "\n",
    "    print(f'##### {attn_variant} #####')\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        start_time = time.time()\n",
    "\n",
    "        train_loss = train(model, train_loader, optimizer, criterion, clip, train_loader_length)\n",
    "        valid_loss = evaluate(model, valid_loader, criterion, val_loader_length)\n",
    "        \n",
    "        #for plotting\n",
    "        train_losses.append(train_loss)\n",
    "        valid_losses.append(valid_loss)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        \n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "        \n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "        \n",
    "        print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "        print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
    "        \n",
    "        #lower perplexity is better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'loss')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(5, 3))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.plot(train_losses, label = 'train loss')\n",
    "ax.plot(valid_losses, label = 'valid loss')\n",
    "plt.legend()\n",
    "ax.set_xlabel('updates')\n",
    "ax.set_ylabel('loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Test Loss: 4.005 | Test PPL:  54.885 |\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(save_path))\n",
    "test_loss = evaluate(model, test_loader, criterion, test_loader_length)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test on some random news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Did you get that thesis from the Dude?'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[0]['en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'คุณได้ข้อเสนอนี้ จากเพื่อนคนนั้นเหรอ?'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[0]['th']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2,  0,  6, 56, 16,  0, 77,  8,  0,  9,  3], device='cuda:0')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_text = text_transform[SRC_LANGUAGE](test[0]['en']).to(device)\n",
    "src_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   2,    7,   10, 1850,   45,    4,   64,  185,   42,   49,   59,   16,\n",
       "           3], device='cuda:0')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trg_text = text_transform[TRG_LANGUAGE](test[0]['th']).to(device)\n",
    "trg_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_text = src_text.reshape(1, -1)  #because batch_size is 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trg_text = trg_text.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 11]), torch.Size([1, 13]))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_text.shape, trg_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_length = torch.tensor([src_text.size(0)]).to(dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(save_path))\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output, attentions = model(src_text, trg_text) #turn off teacher forcing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 13, 7378])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape #batch_size, trg_len, trg_output_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since batch size is 1, we just take off that dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = output.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([13, 7378])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall remove the first token since it's zeroes anyway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 7378])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = output[1:]\n",
    "output.shape #trg_len, trg_output_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we just take the top token with highest probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_max = output.argmax(1) #returns max indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([234, 234,  64,  83,  64,   0,  14,  49,  83,   3,   3,   3],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the mapping of the target language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = vocab_transform[TRG_LANGUAGE].get_itos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "มาจาก\n",
      "มาจาก\n",
      "จาก\n",
      "ไหม\n",
      "จาก\n",
      "<unk>\n",
      "ของ\n",
      "นั้น\n",
      "ไหม\n",
      "<eos>\n",
      "<eos>\n",
      "<eos>\n"
     ]
    }
   ],
   "source": [
    "for token in output_max:\n",
    "    print(mapping[token.item()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Attention\n",
    "\n",
    "Let's display the attentions to understand how the source text links with the generated text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 13, 11])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attentions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are 8 heads, we can look at just 1 head for sake of simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([13, 11])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention = attentions[0, 0, :, :]\n",
    "attention.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<sos>',\n",
       " 'Did',\n",
       " 'you',\n",
       " 'get',\n",
       " 'that',\n",
       " 'thesis',\n",
       " 'from',\n",
       " 'the',\n",
       " 'Dude',\n",
       " '?',\n",
       " '<eos>']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_tokens = ['<sos>'] + token_transform[SRC_LANGUAGE](test[0]['en']) + ['<eos>']\n",
    "src_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<sos>',\n",
       " 'มาจาก',\n",
       " 'มาจาก',\n",
       " 'จาก',\n",
       " 'ไหม',\n",
       " 'จาก',\n",
       " '<unk>',\n",
       " 'ของ',\n",
       " 'นั้น',\n",
       " 'ไหม',\n",
       " '<eos>',\n",
       " '<eos>',\n",
       " '<eos>']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trg_tokens = ['<sos>'] + [mapping[token.item()] for token in output_max]\n",
    "trg_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker as ticker\n",
    "\n",
    "def display_attention(sentence, translation, attention):\n",
    "    \n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(111)\n",
    "    \n",
    "    attention = attention.squeeze(1).cpu().detach().numpy()\n",
    "    \n",
    "    cax = ax.matshow(attention, cmap='bone')\n",
    "   \n",
    "    ax.tick_params(labelsize=10)\n",
    "    \n",
    "    y_ticks =  [''] + translation\n",
    "    x_ticks =  [''] + sentence \n",
    "     \n",
    "    ax.set_xticklabels(x_ticks, rotation=45)\n",
    "    ax.set_yticklabels(y_ticks)\n",
    "\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Neo\\AppData\\Local\\Temp\\ipykernel_29420\\59549304.py:17: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_xticklabels(x_ticks, rotation=45)\n",
      "C:\\Users\\Neo\\AppData\\Local\\Temp\\ipykernel_29420\\59549304.py:18: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_yticklabels(y_ticks)\n",
      "c:\\Users\\Neo\\anaconda3\\envs\\dsai\\lib\\site-packages\\IPython\\core\\pylabtools.py:152: UserWarning: Glyph 3617 (\\N{THAI CHARACTER MO MA}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\Neo\\anaconda3\\envs\\dsai\\lib\\site-packages\\IPython\\core\\pylabtools.py:152: UserWarning: Glyph 3634 (\\N{THAI CHARACTER SARA AA}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\Neo\\anaconda3\\envs\\dsai\\lib\\site-packages\\IPython\\core\\pylabtools.py:152: UserWarning: Glyph 3592 (\\N{THAI CHARACTER CHO CHAN}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\Neo\\anaconda3\\envs\\dsai\\lib\\site-packages\\IPython\\core\\pylabtools.py:152: UserWarning: Glyph 3585 (\\N{THAI CHARACTER KO KAI}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\Neo\\anaconda3\\envs\\dsai\\lib\\site-packages\\IPython\\core\\pylabtools.py:152: UserWarning: Glyph 3652 (\\N{THAI CHARACTER SARA AI MAIMALAI}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\Neo\\anaconda3\\envs\\dsai\\lib\\site-packages\\IPython\\core\\pylabtools.py:152: UserWarning: Glyph 3627 (\\N{THAI CHARACTER HO HIP}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\Neo\\anaconda3\\envs\\dsai\\lib\\site-packages\\IPython\\core\\pylabtools.py:152: UserWarning: Glyph 3586 (\\N{THAI CHARACTER KHO KHAI}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\Neo\\anaconda3\\envs\\dsai\\lib\\site-packages\\IPython\\core\\pylabtools.py:152: UserWarning: Glyph 3629 (\\N{THAI CHARACTER O ANG}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\Neo\\anaconda3\\envs\\dsai\\lib\\site-packages\\IPython\\core\\pylabtools.py:152: UserWarning: Glyph 3591 (\\N{THAI CHARACTER NGO NGU}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\Neo\\anaconda3\\envs\\dsai\\lib\\site-packages\\IPython\\core\\pylabtools.py:152: UserWarning: Glyph 3609 (\\N{THAI CHARACTER NO NU}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\Neo\\anaconda3\\envs\\dsai\\lib\\site-packages\\IPython\\core\\pylabtools.py:152: UserWarning: Glyph 3633 (\\N{THAI CHARACTER MAI HAN-AKAT}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\Neo\\anaconda3\\envs\\dsai\\lib\\site-packages\\IPython\\core\\pylabtools.py:152: UserWarning: Glyph 3657 (\\N{THAI CHARACTER MAI THO}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_attention(src_tokens, trg_tokens, attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "714d3f4db9a58ba7d2f2a9a4fffe577af3df8551aebd380095064812e2e0a6a4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
