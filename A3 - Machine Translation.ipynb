{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Translation + Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\anaconda3\\envs\\dsai\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch, torchdata, torchtext\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.optim as optim\n",
    "import random, math, time\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "#make our work comparable if restarted the kernel\n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.1+cu118'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.16.1+cpu'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchtext.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 1. ETL: Loading the dataset\n",
    "\n",
    "**Note**: Here I chose to translate English to German, simply it is easier for myself, since I don't understand German so it is difficult for me to imagine a sentence during inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "SRC_LANGUAGE = 'en'\n",
    "TRG_LANGUAGE = 'th'\n",
    "\n",
    "dataset = datasets.load_dataset(\"opus100\", \"en-th\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 1000000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 1000000/1000000 [00:17<00:00, 55600.16 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from numpy.random import default_rng\n",
    "\n",
    "rng = default_rng(seed=SEED)\n",
    "# create a list of non-repeated indices of size 50000 and use it to select the training samples\n",
    "select_idx = rng.choice(len(dataset['train']), size=50000, replace=False)\n",
    "dataset['train'] = dataset['train'].filter(lambda example, idx: idx in select_idx, with_indices=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 50000/50000 [00:02<00:00, 23011.51 examples/s]\n",
      "Map: 100%|██████████| 50000/50000 [00:01<00:00, 39246.42 examples/s]\n"
     ]
    }
   ],
   "source": [
    "get_lang_col = lambda example, lang: {lang: example['translation'][lang]}\n",
    "dataset = dataset.map(get_lang_col, fn_kwargs={'lang': \"th\"})\n",
    "dataset = dataset.map(get_lang_col, remove_columns=['translation'], fn_kwargs={'lang': \"en\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'th': 'ครอบครัวโอปี้ไม่สามารถรับเรื่องร้ายได้อีก',\n",
       " 'en': \"Ope's family can't take another hit.\"}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['th', 'en'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['th', 'en'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['th', 'en'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, val_set, test_set = dataset['train'], dataset['validation'], dataset['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 3. Preprocessing \n",
    "\n",
    "### Tokenizing\n",
    "\n",
    "**Note**: the models must first be downloaded using the following on the command line: \n",
    "```\n",
    "python3 -m spacy download en_core_web_sm\n",
    "python3 -m spacy download de_core_news_sm\n",
    "```\n",
    "\n",
    "First, since we have two languages, let's create some constants to represent that.  Also, let's create two dicts: one for holding our tokenizers and one for holding all the vocabs with assigned numbers for each unique word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pythainlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place-holders\n",
    "token_transform = {}\n",
    "vocab_transform = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from pythainlp.tokenize import Tokenizer\n",
    "\n",
    "token_transform[SRC_LANGUAGE] = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "token_transform[TRG_LANGUAGE] = Tokenizer(engine='newmm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence:  ครอบครัวโอปี้ไม่สามารถรับเรื่องร้ายได้อีก\n",
      "Tokenization:  ['ครอบครัว', 'โอ', 'ปี้', 'ไม่', 'สามารถ', 'รับ', 'เรื่อง', 'ร้าย', 'ได้', 'อีก']\n"
     ]
    }
   ],
   "source": [
    "#example of tokenization of the thai part\n",
    "print(\"Sentence: \", dataset['train'][TRG_LANGUAGE][2])\n",
    "print(\"Tokenization: \", token_transform[TRG_LANGUAGE].word_tokenize(dataset['train'][TRG_LANGUAGE][2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 50000/50000 [00:02<00:00, 17289.55 examples/s]\n",
      "Map: 100%|██████████| 2000/2000 [00:00<00:00, 17241.39 examples/s]\n",
      "Map: 100%|██████████| 2000/2000 [00:00<00:00, 10638.22 examples/s]\n",
      "Map: 100%|██████████| 50000/50000 [00:04<00:00, 11297.24 examples/s]\n",
      "Map: 100%|██████████| 2000/2000 [00:00<00:00, 11173.24 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def tokenize_data(example, lang):\n",
    "    try:\n",
    "        return {lang: token_transform[lang](example[lang].lower())}\n",
    "    except:\n",
    "        return {lang: token_transform[lang].word_tokenize(example[lang].lower())}\n",
    "    \n",
    "tokenized_dataset = dataset.map(tokenize_data, remove_columns=[SRC_LANGUAGE], fn_kwargs={'lang': SRC_LANGUAGE})\n",
    "tokenized_dataset = tokenized_dataset.map(tokenize_data, remove_columns=[TRG_LANGUAGE], fn_kwargs={'lang': TRG_LANGUAGE})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define special symbols and indices\n",
    "UNK_IDX, PAD_IDX, SOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
    "special_symbols = ['<unk>', '<pad>', '<sos>', '<eos>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lang in [SRC_LANGUAGE, TRG_LANGUAGE]:\n",
    "    vocab_transform[lang] = torchtext.vocab.build_vocab_from_iterator(tokenized_dataset['train'][lang],\n",
    "                                                                      min_freq=3,   #if not, everything will be treated as UNK\n",
    "                                                                      specials=special_symbols,\n",
    "                                                                      special_first=True) #indicates whether to insert symbols at the beginning or at the end)\n",
    "    # Set UNK_IDX as the default index. This index is returned when the token is not found. \n",
    "    # If not set, it throws RuntimeError when the queried token is not found in the Vocabulary. \n",
    "    vocab_transform[lang].set_default_index(UNK_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save vocab\n",
    "torch.save(vocab_transform, './models/vocab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[36, 2906]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#see some example\n",
    "vocab_transform[SRC_LANGUAGE](['my', 'precious'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[14, 184, 14, 73]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#see some example\n",
    "vocab_transform[TRG_LANGUAGE](['ของ', 'รัก', 'ของ', 'ข้า'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ผม'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we can reverse it....\n",
    "mapping = vocab_transform[TRG_LANGUAGE].get_itos()\n",
    "\n",
    "#print 1816, for example\n",
    "mapping[13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<unk>'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's try unknown vocab\n",
    "mapping[0]\n",
    "#they will all map to <unk> which has 0 as integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<pad>', '<sos>', '<eos>')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's try special symbols\n",
    "mapping[1], mapping[2], mapping[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7378"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check unique vocabularies\n",
    "len(mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4. Preparing the dataloader\n",
    "\n",
    "One thing we change here is the <code>collate_fn</code> which now also returns the length of sentence.  This is required for <code>packed_padded_sequence</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# helper function to club together sequential operations\n",
    "def sequential_transforms(*transforms):\n",
    "    def func(txt_input):\n",
    "        for transform in transforms:\n",
    "            try:\n",
    "                txt_input = transform(txt_input)\n",
    "            except TypeError:\n",
    "                txt_input = transform.word_tokenize(txt_input)\n",
    "        return txt_input\n",
    "    return func\n",
    "\n",
    "# function to add BOS/EOS and create tensor for input sequence indices\n",
    "def tensor_transform(token_ids):\n",
    "    return torch.cat((torch.tensor([SOS_IDX]), \n",
    "                      torch.tensor(token_ids), \n",
    "                      torch.tensor([EOS_IDX])))\n",
    "\n",
    "# src and trg language text transforms to convert raw strings into tensors indices\n",
    "text_transform = {}\n",
    "for ln in [SRC_LANGUAGE, TRG_LANGUAGE]:\n",
    "    text_transform[ln] = sequential_transforms(token_transform[ln], #Tokenization\n",
    "                                               vocab_transform[ln], #Numericalization\n",
    "                                               tensor_transform) # Add BOS/EOS and create tensor\n",
    "\n",
    "\n",
    "# function to collate data samples into batch tesors\n",
    "def collate_batch(batch):\n",
    "    src_batch, src_len_batch, trg_batch = [], [], []\n",
    "    for sample in batch:\n",
    "        processed_text = text_transform[SRC_LANGUAGE](sample[SRC_LANGUAGE].rstrip(\"\\n\"))\n",
    "        src_batch.append(processed_text)\n",
    "        trg_batch.append(text_transform[TRG_LANGUAGE](sample[TRG_LANGUAGE].rstrip(\"\\n\")))\n",
    "        src_len_batch.append(processed_text.size(0))\n",
    "\n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX, batch_first=True)\n",
    "    trg_batch = pad_sequence(trg_batch, padding_value=PAD_IDX, batch_first=True)\n",
    "    return src_batch, torch.tensor(src_len_batch, dtype=torch.int64), trg_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create train, val, and test dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
    "valid_loader = DataLoader(val_set,   batch_size=batch_size, shuffle=False, collate_fn=collate_batch)\n",
    "test_loader  = DataLoader(test_set,  batch_size=batch_size, shuffle=False, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the train loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for en, _, th in train_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English shape:  torch.Size([64, 29])\n",
      "Thai shape:  torch.Size([64, 38])\n"
     ]
    }
   ],
   "source": [
    "print(\"English shape: \", en.shape)  # (batch_size, seq len)\n",
    "print(\"Thai shape: \", th.shape)   # (batch_size, seq len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Design the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, pf_dim, dropout, attn_variant, device):\n",
    "        super().__init__()\n",
    "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.ff_layer_norm        = nn.LayerNorm(hid_dim)\n",
    "        self.self_attention       = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, attn_variant, device)\n",
    "        self.feedforward          = PositionwiseFeedforwardLayer(hid_dim, pf_dim, dropout)\n",
    "        self.dropout              = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src, src_mask):\n",
    "        #src = [batch size, src len, hid dim]\n",
    "        #src_mask = [batch size, 1, 1, src len]   #if the token is padding, it will be 1, otherwise 0\n",
    "        _src, _ = self.self_attention(src, src, src, src_mask)\n",
    "        src     = self.self_attn_layer_norm(src + self.dropout(_src))\n",
    "        #src: [batch_size, src len, hid dim]\n",
    "        \n",
    "        _src    = self.feedforward(src)\n",
    "        src     = self.ff_layer_norm(src + self.dropout(_src))\n",
    "        #src: [batch_size, src len, hid dim]\n",
    "        \n",
    "        return src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hid_dim, n_layers, n_heads, pf_dim, dropout, attn_variant, device, max_length = 500):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.attn_variant = attn_variant\n",
    "        self.tok_embedding = nn.Embedding(input_dim, hid_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
    "        self.layers        = nn.ModuleList([EncoderLayer(hid_dim, n_heads, pf_dim, dropout, attn_variant, device)\n",
    "                                           for _ in range(n_layers)])\n",
    "        self.dropout       = nn.Dropout(dropout)\n",
    "        self.scale         = torch.sqrt(torch.FloatTensor([hid_dim])).to(self.device)\n",
    "        \n",
    "    def forward(self, src, src_mask):\n",
    "        \n",
    "        #src = [batch size, src len]\n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "        \n",
    "        batch_size = src.shape[0]\n",
    "        src_len    = src.shape[1]\n",
    "        \n",
    "        pos        = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
    "        #pos: [batch_size, src_len]\n",
    "        \n",
    "        src        = self.dropout((self.tok_embedding(src) * self.scale) + self.pos_embedding(pos))\n",
    "        #src: [batch_size, src_len, hid_dim]\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            src = layer(src, src_mask)\n",
    "        #src: [batch_size, src_len, hid_dim]\n",
    "        \n",
    "        return src\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mutli Head Attention Layer\n",
    "\n",
    "$$ \\text{Attention}(Q, K, V) = \\text{Softmax} \\big( \\frac{QK^T}{\\sqrt{d_k}} \\big)V $$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default Transformer attention variant\n",
    "class ScaledAttention(nn.Module):\n",
    "    def __init__(self, head_dim):\n",
    "        super().__init__()\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([head_dim])).to(device)\n",
    "\n",
    "    def forward(self, Q, K):\n",
    "        scores = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n",
    "        # scores: [batch_size, n_heads, query len, key len]\n",
    "\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiplicativeAttention(nn.Module):\n",
    "    def __init__(self, head_dim):\n",
    "        super().__init__()\n",
    "        self.W1 = nn.Linear(head_dim, head_dim)\n",
    "\n",
    "    def forward(self, Q, K):\n",
    "        scores = torch.matmul(self.W1(Q), K.permute(0, 1, 3, 2))\n",
    "        # scores: [batch_size, n_heads, query len, key len]\n",
    "\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdditiveAttention(nn.Module):\n",
    "    def __init__(self, head_dim):\n",
    "        super().__init__()\n",
    "        self.W1 = nn.Linear(head_dim, head_dim)\n",
    "        self.W2 = nn.Linear(head_dim, head_dim)\n",
    "        self.V = nn.Linear(head_dim, 1)\n",
    "\n",
    "    def forward(self, Q, K):\n",
    "        Q = Q.unsqueeze(3)  # Q: [batch_size, n_heads, query len, head_dim] => [batch_size, n_heads, query len, 1, head_dim]\n",
    "        K = K.unsqueeze(2)  # Q: [batch_size, n_heads, key len, head_dim] => [batch_size, n_heads, 1, key len, head_dim]\n",
    "        features = torch.tanh(self.W1(Q) + self.W2(K))\n",
    "        # features: [batch_size, n_heads, query len, key len, head_dim]\n",
    "\n",
    "        scores = self.V(features).squeeze(-1)\n",
    "        # scores: [batch_size, n_heads, query len, key len]\n",
    "\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, dropout, attn_variant, device):\n",
    "        super().__init__()\n",
    "        assert hid_dim % n_heads == 0\n",
    "        self.hid_dim  = hid_dim\n",
    "        self.n_heads  = n_heads\n",
    "        self.head_dim = hid_dim // n_heads\n",
    "        self.attn_variant = attn_variant\n",
    "        \n",
    "        self.fc_q     = nn.Linear(hid_dim, hid_dim)\n",
    "        self.fc_k     = nn.Linear(hid_dim, hid_dim)\n",
    "        self.fc_v     = nn.Linear(hid_dim, hid_dim)\n",
    "        \n",
    "        self.fc_o     = nn.Linear(hid_dim, hid_dim)\n",
    "        \n",
    "        self.dropout  = nn.Dropout(dropout)\n",
    "\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
    "\n",
    "        if attn_variant == 'scaled':\n",
    "            self.scaled_attention = ScaledAttention(self.head_dim)\n",
    "\n",
    "        elif attn_variant == 'multiplicative':\n",
    "            self.multiplicative_attention = MultiplicativeAttention(self.head_dim)\n",
    "        \n",
    "        elif attn_variant == 'additive':\n",
    "            self.additive_attention = AdditiveAttention(self.head_dim)\n",
    "                \n",
    "    def forward(self, query, key, value, mask = None):\n",
    "        #src, src, src, src_mask\n",
    "        #query = [batch size, query len, hid dim]\n",
    "        #key = [batch size, key len, hid dim]\n",
    "        #value = [batch size, value len, hid dim]\n",
    "        \n",
    "        batch_size = query.shape[0]\n",
    "        \n",
    "        Q = self.fc_q(query)\n",
    "        K = self.fc_k(key)\n",
    "        V = self.fc_v(value)\n",
    "        # Q: [batch_size, seq len, hid_dim]\n",
    "        \n",
    "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        # Q=K=V: [batch_size, n heads, seq len, head_dim]\n",
    "        \n",
    "        # energy = self.additive_attention(Q, K)\n",
    "        if self.attn_variant == \"general\":\n",
    "            energy = torch.matmul(Q, K.permute(0, 1, 3, 2))\n",
    "\n",
    "        elif self.attn_variant == \"scaled\":\n",
    "            energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n",
    "            # energy = self.scaled_attention(Q, K)\n",
    "\n",
    "        elif self.attn_variant == \"multiplicative\":\n",
    "            energy = self.multiplicative_attention(Q, K)\n",
    "\n",
    "        elif self.attn_variant == \"additive\":\n",
    "            energy = self.additive_attention(Q, K)\n",
    "            \n",
    "        else:\n",
    "            raise Exception(\"Incorrect value for attention variant. Must be one of the following: \\\n",
    "                            scaled, general, multiplicative, additive\")\n",
    "        \n",
    "        #for making attention to padding to 0\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, -1e10)\n",
    "            \n",
    "        attention = torch.softmax(energy, dim = -1)\n",
    "        #attention = [batch_size, n heads, query len, key len]\n",
    "\n",
    "        x = torch.matmul(self.dropout(attention), V)\n",
    "        #[batch_size, n heads, query len, key len] @ [batch_size, n heads, value len, head_dim]\n",
    "        #x = [batch_size, n heads, query len, head dim]\n",
    "        \n",
    "        x = x.permute(0, 2, 1, 3).contiguous()  #we can perform .view\n",
    "        #x = [batch_size, query len, n heads, head dim]\n",
    "        \n",
    "        x = x.view(batch_size, -1, self.hid_dim)\n",
    "        #x = [batch_size, query len, hid dim]\n",
    "        \n",
    "        x = self.fc_o(x)\n",
    "        #x = [batch_size, query len, hid dim]\n",
    "        \n",
    "        return x, attention\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Position-wise Feedforward Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedforwardLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, pf_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(hid_dim, pf_dim)\n",
    "        self.fc2 = nn.Linear(pf_dim, hid_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #x = [batch size, src len, hid dim]\n",
    "        x = self.dropout(torch.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, pf_dim, dropout, attn_variant, device):\n",
    "        super().__init__()\n",
    "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.enc_attn_layer_norm  = nn.LayerNorm(hid_dim)\n",
    "        self.ff_layer_norm        = nn.LayerNorm(hid_dim)\n",
    "        self.self_attention       = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, attn_variant, device)\n",
    "        self.encoder_attention    = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, attn_variant, device)\n",
    "        self.feedforward          = PositionwiseFeedforwardLayer(hid_dim, pf_dim, dropout)\n",
    "        self.dropout              = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "        \n",
    "        #trg = [batch size, trg len, hid dim]\n",
    "        #enc_src = [batch size, src len, hid dim]\n",
    "        #trg_mask = [batch size, 1, trg len, trg len]\n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "        \n",
    "        _trg, _ = self.self_attention(trg, trg, trg, trg_mask)\n",
    "        trg     = self.self_attn_layer_norm(trg + self.dropout(_trg))\n",
    "        #trg = [batch_size, trg len, hid dim]\n",
    "        \n",
    "        _trg, attention = self.encoder_attention(trg, enc_src, enc_src, src_mask)\n",
    "        trg             = self.enc_attn_layer_norm(trg + self.dropout(_trg))\n",
    "        #trg = [batch_size, trg len, hid dim]\n",
    "        #attention = [batch_size, n heads, trg len, src len]\n",
    "        \n",
    "        _trg = self.feedforward(trg)\n",
    "        trg  = self.ff_layer_norm(trg + self.dropout(_trg))\n",
    "        #trg = [batch_size, trg len, hid dim]\n",
    "        \n",
    "        return trg, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, hid_dim, n_layers, n_heads, \n",
    "                 pf_dim, dropout, attn_variant, device, max_length = 500):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.tok_embedding = nn.Embedding(output_dim, hid_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
    "        self.layers        = nn.ModuleList([DecoderLayer(hid_dim, n_heads, pf_dim, dropout, attn_variant, device)\n",
    "                                            for _ in range(n_layers)])\n",
    "        self.fc_out        = nn.Linear(hid_dim, output_dim)\n",
    "        self.dropout       = nn.Dropout(dropout)\n",
    "        self.scale         = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
    "        \n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "        \n",
    "        #trg = [batch size, trg len]\n",
    "        #enc_src = [batch size, src len, hid dim]\n",
    "        #trg_mask = [batch size, 1, trg len, trg len]\n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "        \n",
    "        batch_size = trg.shape[0]\n",
    "        trg_len    = trg.shape[1]\n",
    "        \n",
    "        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
    "        #pos: [batch_size, trg len]\n",
    "        \n",
    "        trg = self.dropout((self.tok_embedding(trg) * self.scale) + self.pos_embedding(pos))\n",
    "        #trg: [batch_size, trg len, hid dim]\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            trg, attention = layer(trg, enc_src, trg_mask, src_mask)\n",
    "            \n",
    "        #trg: [batch_size, trg len, hid dim]\n",
    "        #attention: [batch_size, n heads, trg len, src len]\n",
    "        \n",
    "        output = self.fc_out(trg)\n",
    "        #output = [batch_size, trg len, output_dim]\n",
    "        \n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting them together (become Seq2Seq!)\n",
    "\n",
    "Our `trg_sub_mask` will look something like this (for a target with 5 tokens):\n",
    "\n",
    "$$\\begin{matrix}\n",
    "1 & 0 & 0 & 0 & 0\\\\\n",
    "1 & 1 & 0 & 0 & 0\\\\\n",
    "1 & 1 & 1 & 0 & 0\\\\\n",
    "1 & 1 & 1 & 1 & 0\\\\\n",
    "1 & 1 & 1 & 1 & 1\\\\\n",
    "\\end{matrix}$$\n",
    "\n",
    "The \"subsequent\" mask is then logically anded with the padding mask, this combines the two masks ensuring both the subsequent tokens and the padding tokens cannot be attended to. For example if the last two tokens were `<pad>` tokens the mask would look like:\n",
    "\n",
    "$$\\begin{matrix}\n",
    "1 & 0 & 0 & 0 & 0\\\\\n",
    "1 & 1 & 0 & 0 & 0\\\\\n",
    "1 & 1 & 1 & 0 & 0\\\\\n",
    "1 & 1 & 1 & 0 & 0\\\\\n",
    "1 & 1 & 1 & 0 & 0\\\\\n",
    "\\end{matrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(self, encoder, decoder, src_pad_idx, trg_pad_idx, device):\n",
    "        super().__init__()\n",
    "        # store the input parameters so we can retrive them later for model inferencing\n",
    "        self.params = {'encoder': encoder, 'decoder': decoder,\n",
    "                       'src_pad_idx': src_pad_idx, 'trg_pad_idx': trg_pad_idx}\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        self.device = device\n",
    "        \n",
    "    def make_src_mask(self, src):\n",
    "        \n",
    "        #src = [batch size, src len]\n",
    "        \n",
    "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "\n",
    "        return src_mask\n",
    "    \n",
    "    def make_trg_mask(self, trg):\n",
    "        \n",
    "        #trg = [batch size, trg len]\n",
    "        \n",
    "        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        #trg_pad_mask = [batch size, 1, 1, trg len]\n",
    "        \n",
    "        trg_len = trg.shape[1]\n",
    "        \n",
    "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device = self.device)).bool()\n",
    "        #trg_sub_mask = [trg len, trg len]\n",
    "            \n",
    "        trg_mask = trg_pad_mask & trg_sub_mask\n",
    "        #trg_mask = [batch size, 1, trg len, trg len]\n",
    "        \n",
    "        return trg_mask\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        \n",
    "        #src = [batch size, src len]\n",
    "        #trg = [batch size, trg len]\n",
    "                \n",
    "        src_mask = self.make_src_mask(src)\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "        \n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "        #trg_mask = [batch size, 1, trg len, trg len]\n",
    "        \n",
    "        enc_src = self.encoder(src, src_mask)\n",
    "        #enc_src = [batch size, src len, hid dim]\n",
    "                \n",
    "        output, attention = self.decoder(trg, enc_src, trg_mask, src_mask)\n",
    "        \n",
    "        #output = [batch size, trg len, output dim]\n",
    "        #attention = [batch size, n heads, trg len, src len]\n",
    "        \n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(m):\n",
    "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
    "        nn.init.xavier_uniform_(m.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2SeqTransformer(\n",
       "  (encoder): Encoder(\n",
       "    (tok_embedding): Embedding(6848, 256)\n",
       "    (pos_embedding): Embedding(500, 256)\n",
       "    (layers): ModuleList(\n",
       "      (0-2): 3 x EncoderLayer(\n",
       "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (additive_attention): AdditiveAttention(\n",
       "            (W1): Linear(in_features=32, out_features=32, bias=True)\n",
       "            (W2): Linear(in_features=32, out_features=32, bias=True)\n",
       "            (V): Linear(in_features=32, out_features=1, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (feedforward): PositionwiseFeedforwardLayer(\n",
       "          (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (tok_embedding): Embedding(7378, 256)\n",
       "    (pos_embedding): Embedding(500, 256)\n",
       "    (layers): ModuleList(\n",
       "      (0-2): 3 x DecoderLayer(\n",
       "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (enc_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (additive_attention): AdditiveAttention(\n",
       "            (W1): Linear(in_features=32, out_features=32, bias=True)\n",
       "            (W2): Linear(in_features=32, out_features=32, bias=True)\n",
       "            (V): Linear(in_features=32, out_features=1, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (encoder_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (additive_attention): AdditiveAttention(\n",
       "            (W1): Linear(in_features=32, out_features=32, bias=True)\n",
       "            (W2): Linear(in_features=32, out_features=32, bias=True)\n",
       "            (V): Linear(in_features=32, out_features=1, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (feedforward): PositionwiseFeedforwardLayer(\n",
       "          (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (fc_out): Linear(in_features=256, out_features=7378, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dim   = len(vocab_transform[SRC_LANGUAGE])\n",
    "output_dim  = len(vocab_transform[TRG_LANGUAGE])\n",
    "hid_dim = 256\n",
    "enc_layers = 3\n",
    "dec_layers = 3\n",
    "enc_heads = 8\n",
    "dec_heads = 8\n",
    "enc_pf_dim = 512\n",
    "dec_pf_dim = 512\n",
    "enc_dropout = 0.1\n",
    "dec_dropout = 0.1\n",
    "attn_variant = 'additive'\n",
    "\n",
    "SRC_PAD_IDX = PAD_IDX\n",
    "TRG_PAD_IDX = PAD_IDX\n",
    "\n",
    "enc = Encoder(input_dim, \n",
    "              hid_dim, \n",
    "              enc_layers, \n",
    "              enc_heads, \n",
    "              enc_pf_dim, \n",
    "              enc_dropout, \n",
    "              attn_variant,\n",
    "              device)\n",
    "\n",
    "dec = Decoder(output_dim, \n",
    "              hid_dim, \n",
    "              dec_layers, \n",
    "              dec_heads, \n",
    "              dec_pf_dim, \n",
    "              enc_dropout, \n",
    "              attn_variant,\n",
    "              device)\n",
    "\n",
    "model = Seq2SeqTransformer(enc, dec, SRC_PAD_IDX, TRG_PAD_IDX, device).to(device)\n",
    "model.apply(initialize_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1753088\n",
      "128000\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      "  1024\n",
      "    32\n",
      "  1024\n",
      "    32\n",
      "    32\n",
      "     1\n",
      "131072\n",
      "   512\n",
      "131072\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      "  1024\n",
      "    32\n",
      "  1024\n",
      "    32\n",
      "    32\n",
      "     1\n",
      "131072\n",
      "   512\n",
      "131072\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      "  1024\n",
      "    32\n",
      "  1024\n",
      "    32\n",
      "    32\n",
      "     1\n",
      "131072\n",
      "   512\n",
      "131072\n",
      "   256\n",
      "1888768\n",
      "128000\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      "  1024\n",
      "    32\n",
      "  1024\n",
      "    32\n",
      "    32\n",
      "     1\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      "  1024\n",
      "    32\n",
      "  1024\n",
      "    32\n",
      "    32\n",
      "     1\n",
      "131072\n",
      "   512\n",
      "131072\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      "  1024\n",
      "    32\n",
      "  1024\n",
      "    32\n",
      "    32\n",
      "     1\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      "  1024\n",
      "    32\n",
      "  1024\n",
      "    32\n",
      "    32\n",
      "     1\n",
      "131072\n",
      "   512\n",
      "131072\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      "  1024\n",
      "    32\n",
      "  1024\n",
      "    32\n",
      "    32\n",
      "     1\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      "  1024\n",
      "    32\n",
      "  1024\n",
      "    32\n",
      "    32\n",
      "     1\n",
      "131072\n",
      "   512\n",
      "131072\n",
      "   256\n",
      "1888768\n",
      "  7378\n",
      "______\n",
      "9766971\n"
     ]
    }
   ],
   "source": [
    "#we can print the complexity by the number of parameters\n",
    "def count_parameters(model):\n",
    "    params = [p.numel() for p in model.parameters() if p.requires_grad]\n",
    "    for item in params:\n",
    "        print(f'{item:>6}')\n",
    "    print(f'______\\n{sum(params):>6}')\n",
    "    \n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we'll define our training loop. This is the exact same as the one used in the previous tutorial.\n",
    "\n",
    "As we want our model to predict the `<eos>` token but not have it be an input into our model we simply slice the `<eos>` token off the end of the sequence. Thus:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\text{trg} &= [sos, x_1, x_2, x_3, eos]\\\\\n",
    "\\text{trg[:-1]} &= [sos, x_1, x_2, x_3]\n",
    "\\end{align*}$$\n",
    "\n",
    "$x_i$ denotes actual target sequence element. We then feed this into the model to get a predicted sequence that should hopefully predict the `<eos>` token:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\text{output} &= [y_1, y_2, y_3, eos]\n",
    "\\end{align*}$$\n",
    "\n",
    "$y_i$ denotes predicted target sequence element. We then calculate our loss using the original `trg` tensor with the `<sos>` token sliced off the front, leaving the `<eos>` token:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\text{output} &= [y_1, y_2, y_3, eos]\\\\\n",
    "\\text{trg[1:]} &= [x_1, x_2, x_3, eos]\n",
    "\\end{align*}$$\n",
    "\n",
    "We then calculate our losses and update our parameters as is standard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, optimizer, criterion, clip, loader_length):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for src, src_len, trg in loader:\n",
    "        \n",
    "        src = src.to(device)\n",
    "        trg = trg.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #trg[:, :-1] remove the eos, e.g., \"<sos> I love sushi\" since teaching forcing, the input does not need to have eos\n",
    "        output, _ = model(src, trg[:,:-1])\n",
    "                \n",
    "        #output = [batch size, trg len - 1, output dim]\n",
    "        #trg    = [batch size, trg len]\n",
    "            \n",
    "        output_dim = output.shape[-1]\n",
    "            \n",
    "        output = output.reshape(-1, output_dim)\n",
    "        trg = trg[:,1:].reshape(-1) #trg[:, 1:] remove the sos, e.g., \"i love sushi <eos>\" since in teaching forcing, the output does not have sos\n",
    "                \n",
    "        #output = [batch size * trg len - 1, output dim]\n",
    "        #trg    = [batch size * trg len - 1]\n",
    "            \n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / loader_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our evaluation loop is similar to our training loop, however as we aren't updating any parameters we don't need to pass an optimizer or a clip value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader, criterion, loader_length):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for src, src_len, trg in loader:\n",
    "        \n",
    "            src = src.to(device)\n",
    "            trg = trg.to(device)\n",
    "\n",
    "            output, _ = model(src, trg[:,:-1])\n",
    "            \n",
    "            #output = [batch size, trg len - 1, output dim]\n",
    "            #trg = [batch size, trg len]\n",
    "            \n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            output = output.contiguous().view(-1, output_dim)\n",
    "            trg = trg[:,1:].contiguous().view(-1)\n",
    "            \n",
    "            #output = [batch size * trg len - 1, output dim]\n",
    "            #trg = [batch size * trg len - 1]\n",
    "            \n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / loader_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting everything together\n",
    "\n",
    "Finally, we train our actual model. This model is almost 3x faster than the convolutional sequence-to-sequence model and also achieves a lower validation perplexity!\n",
    "\n",
    "**Note: similar to CNN, this model always has a teacher forcing ratio of 1, i.e. it will always use the ground truth next token from the target sequence (this is simply because CNN do everything in parallel so we cannot have the next token). This means we cannot compare perplexity values against the previous models when they are using a teacher forcing ratio that is not 1. To understand this, try run previous tutorials with teaching forcing ratio of 1, you will get very low perplexity.  **   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== general =====\n",
      "Epoch: 01 | Time: 0m 26s\n",
      "\tTrain Loss: 5.504 | Train PPL: 245.602\n",
      "\t Val. Loss: 5.127 |  Val. PPL: 168.470\n",
      "Epoch: 02 | Time: 0m 25s\n",
      "\tTrain Loss: 4.947 | Train PPL: 140.727\n",
      "\t Val. Loss: 4.870 |  Val. PPL: 130.324\n",
      "Epoch: 03 | Time: 0m 25s\n",
      "\tTrain Loss: 4.704 | Train PPL: 110.354\n",
      "\t Val. Loss: 4.742 |  Val. PPL: 114.620\n",
      "Epoch: 04 | Time: 0m 25s\n",
      "\tTrain Loss: 4.542 | Train PPL:  93.924\n",
      "\t Val. Loss: 4.673 |  Val. PPL: 106.974\n",
      "Epoch: 05 | Time: 0m 25s\n",
      "\tTrain Loss: 4.429 | Train PPL:  83.885\n",
      "\t Val. Loss: 4.620 |  Val. PPL: 101.511\n",
      "Epoch: 06 | Time: 0m 25s\n",
      "\tTrain Loss: 4.337 | Train PPL:  76.508\n",
      "\t Val. Loss: 4.585 |  Val. PPL:  97.966\n",
      "Epoch: 07 | Time: 0m 25s\n",
      "\tTrain Loss: 4.265 | Train PPL:  71.130\n",
      "\t Val. Loss: 4.555 |  Val. PPL:  95.137\n",
      "Epoch: 08 | Time: 0m 25s\n",
      "\tTrain Loss: 4.204 | Train PPL:  66.968\n",
      "\t Val. Loss: 4.549 |  Val. PPL:  94.573\n",
      "Epoch: 09 | Time: 0m 25s\n",
      "\tTrain Loss: 4.155 | Train PPL:  63.750\n",
      "\t Val. Loss: 4.534 |  Val. PPL:  93.097\n",
      "Epoch: 10 | Time: 0m 25s\n",
      "\tTrain Loss: 4.111 | Train PPL:  61.007\n",
      "\t Val. Loss: 4.523 |  Val. PPL:  92.114\n",
      "\n",
      "===== scaled =====\n",
      "Epoch: 01 | Time: 0m 26s\n",
      "\tTrain Loss: 5.144 | Train PPL: 171.321\n",
      "\t Val. Loss: 4.665 |  Val. PPL: 106.163\n",
      "Epoch: 02 | Time: 0m 25s\n",
      "\tTrain Loss: 4.407 | Train PPL:  82.063\n",
      "\t Val. Loss: 4.353 |  Val. PPL:  77.675\n",
      "Epoch: 03 | Time: 0m 25s\n",
      "\tTrain Loss: 4.033 | Train PPL:  56.423\n",
      "\t Val. Loss: 4.170 |  Val. PPL:  64.707\n",
      "Epoch: 04 | Time: 0m 25s\n",
      "\tTrain Loss: 3.740 | Train PPL:  42.095\n",
      "\t Val. Loss: 4.063 |  Val. PPL:  58.172\n",
      "Epoch: 05 | Time: 0m 25s\n",
      "\tTrain Loss: 3.497 | Train PPL:  33.031\n",
      "\t Val. Loss: 4.023 |  Val. PPL:  55.872\n",
      "Epoch: 06 | Time: 0m 25s\n",
      "\tTrain Loss: 3.285 | Train PPL:  26.697\n",
      "\t Val. Loss: 4.002 |  Val. PPL:  54.726\n",
      "Epoch: 07 | Time: 0m 25s\n",
      "\tTrain Loss: 3.095 | Train PPL:  22.082\n",
      "\t Val. Loss: 4.037 |  Val. PPL:  56.659\n",
      "Epoch: 08 | Time: 0m 25s\n",
      "\tTrain Loss: 2.927 | Train PPL:  18.672\n",
      "\t Val. Loss: 4.065 |  Val. PPL:  58.272\n",
      "Epoch: 09 | Time: 0m 25s\n",
      "\tTrain Loss: 2.771 | Train PPL:  15.978\n",
      "\t Val. Loss: 4.105 |  Val. PPL:  60.617\n",
      "Epoch: 10 | Time: 0m 26s\n",
      "\tTrain Loss: 2.633 | Train PPL:  13.917\n",
      "\t Val. Loss: 4.166 |  Val. PPL:  64.462\n",
      "\n",
      "===== multiplicative =====\n",
      "Epoch: 01 | Time: 0m 27s\n",
      "\tTrain Loss: 5.495 | Train PPL: 243.438\n",
      "\t Val. Loss: 5.143 |  Val. PPL: 171.236\n",
      "Epoch: 02 | Time: 0m 26s\n",
      "\tTrain Loss: 4.960 | Train PPL: 142.623\n",
      "\t Val. Loss: 4.900 |  Val. PPL: 134.356\n",
      "Epoch: 03 | Time: 0m 26s\n",
      "\tTrain Loss: 4.742 | Train PPL: 114.673\n",
      "\t Val. Loss: 4.779 |  Val. PPL: 119.011\n",
      "Epoch: 04 | Time: 0m 26s\n",
      "\tTrain Loss: 4.611 | Train PPL: 100.560\n",
      "\t Val. Loss: 4.711 |  Val. PPL: 111.189\n",
      "Epoch: 05 | Time: 0m 26s\n",
      "\tTrain Loss: 4.514 | Train PPL:  91.304\n",
      "\t Val. Loss: 4.671 |  Val. PPL: 106.790\n",
      "Epoch: 06 | Time: 0m 26s\n",
      "\tTrain Loss: 4.445 | Train PPL:  85.199\n",
      "\t Val. Loss: 4.639 |  Val. PPL: 103.426\n",
      "Epoch: 07 | Time: 0m 26s\n",
      "\tTrain Loss: 4.399 | Train PPL:  81.403\n",
      "\t Val. Loss: 4.639 |  Val. PPL: 103.466\n",
      "Epoch: 08 | Time: 0m 26s\n",
      "\tTrain Loss: 4.350 | Train PPL:  77.513\n",
      "\t Val. Loss: 4.625 |  Val. PPL: 102.031\n",
      "Epoch: 09 | Time: 0m 26s\n",
      "\tTrain Loss: 4.330 | Train PPL:  75.916\n",
      "\t Val. Loss: 4.630 |  Val. PPL: 102.563\n",
      "Epoch: 10 | Time: 0m 26s\n",
      "\tTrain Loss: 4.307 | Train PPL:  74.194\n",
      "\t Val. Loss: 4.619 |  Val. PPL: 101.436\n",
      "\n",
      "===== additive =====\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "input_dim   = len(vocab_transform[SRC_LANGUAGE])\n",
    "output_dim  = len(vocab_transform[TRG_LANGUAGE])\n",
    "\n",
    "batch_size = 64\n",
    "lr = 0.0005\n",
    "hid_dim = 256\n",
    "enc_layers = 3\n",
    "dec_layers = 3\n",
    "enc_heads = 8\n",
    "dec_heads = 8\n",
    "enc_pf_dim = 512\n",
    "dec_pf_dim = 512\n",
    "enc_dropout = 0.1\n",
    "dec_dropout = 0.1\n",
    "\n",
    "SRC_PAD_IDX = PAD_IDX\n",
    "TRG_PAD_IDX = PAD_IDX\n",
    "\n",
    "num_epochs = 10\n",
    "clip       = 1\n",
    "\n",
    "for attn_variant in ['general', 'scaled', 'multiplicative', 'additive']:\n",
    "\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
    "    valid_loader = DataLoader(val_set,   batch_size=batch_size, shuffle=False, collate_fn=collate_batch)\n",
    "    test_loader  = DataLoader(test_set,  batch_size=batch_size, shuffle=False, collate_fn=collate_batch)\n",
    "\n",
    "    train_loader_length = len(list(iter(train_loader)))\n",
    "    val_loader_length   = len(list(iter(valid_loader)))\n",
    "    test_loader_length  = len(list(iter(test_loader)))\n",
    "\n",
    "    enc = Encoder(input_dim,\n",
    "                  hid_dim, \n",
    "                  enc_layers, \n",
    "                  enc_heads, \n",
    "                  enc_pf_dim, \n",
    "                  enc_dropout, \n",
    "                  attn_variant, \n",
    "                  device)\n",
    "\n",
    "    dec = Decoder(output_dim, \n",
    "                  hid_dim, \n",
    "                  dec_layers, \n",
    "                  dec_heads, \n",
    "                  dec_pf_dim, \n",
    "                  enc_dropout, \n",
    "                  attn_variant, \n",
    "                  device)\n",
    "\n",
    "    model = Seq2SeqTransformer(enc, dec, SRC_PAD_IDX, TRG_PAD_IDX, device).to(device)\n",
    "    model.apply(initialize_weights)\n",
    "\n",
    "    #training hyperparameters\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX) #combine softmax with cross entropy\n",
    "\n",
    "    save_path = f'models/{attn_variant}_{model.__class__.__name__}.pt'\n",
    "\n",
    "    best_valid_loss = float('inf')\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "\n",
    "    print(f'\\n===== {attn_variant} =====')\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        start_time = time.time()\n",
    "\n",
    "        train_loss = train(model, train_loader, optimizer, criterion, clip, train_loader_length)\n",
    "        valid_loss = evaluate(model, valid_loader, criterion, val_loader_length)\n",
    "        \n",
    "        #for plotting\n",
    "        train_losses.append(train_loss)\n",
    "        valid_losses.append(valid_loss)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        \n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "        \n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save([model.params, model.state_dict()], save_path)\n",
    "            # torch.save([model.params, model.state_dict()], save_path)\n",
    "        \n",
    "        print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "        print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
    "        \n",
    "        #lower perplexity is better\n",
    "\n",
    "    # empty gpu cache to clear memory\n",
    "    del enc\n",
    "    del dec\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'loss')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(5, 3))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.plot(train_losses, label = 'train loss')\n",
    "ax.plot(valid_losses, label = 'valid loss')\n",
    "plt.legend()\n",
    "ax.set_xlabel('updates')\n",
    "ax.set_ylabel('loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Test Loss: 6.514 | Test PPL: 674.742 |\n"
     ]
    }
   ],
   "source": [
    "params, state = torch.load(save_path)\n",
    "model = Seq2SeqTransformer(**params, device=device).to(device)\n",
    "model.load_state_dict(state)\n",
    "test_loss = evaluate(model, test_loader, criterion, test_loader_length)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test on some random news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Did you get that thesis from the Dude?'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set[0]['en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'คุณได้ข้อเสนอนี้ จากเพื่อนคนนั้นเหรอ?'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set[0]['th']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2,  0,  6, 53, 15,  0, 64,  8,  0,  9,  3], device='cuda:0')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_text = text_transform[SRC_LANGUAGE](test_set[0]['en']).to(device)\n",
    "src_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  2,   8,  11,   0,  52,   4,  50, 167,  41,  40,  54,  19,   3],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trg_text = text_transform[TRG_LANGUAGE](test_set[0]['th']).to(device)\n",
    "trg_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_text = src_text.reshape(1, -1)  #because batch_size is 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trg_text = trg_text.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 11]), torch.Size([1, 13]))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_text.shape, trg_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_length = torch.tensor([src_text.size(0)]).to(dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's pick one of our model, in this case the additive model\n",
    "load_path = 'models/additive_Seq2SeqTransformer.pt'\n",
    "\n",
    "params, state = torch.load(save_path)\n",
    "model = Seq2SeqTransformer(**params, device=device).to(device)\n",
    "model.load_state_dict(state)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output, attentions = model(src_text, trg_text) #turn off teacher forcing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 13, 458])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape #batch_size, trg_len, trg_output_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since batch size is 1, we just take off that dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = output.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([13, 458])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall remove the first token since it's zeroes anyway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 458])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = output[1:]\n",
    "output.shape #trg_len, trg_output_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we just take the top token with highest probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_max = output.argmax(1) #returns max indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([343,   0,   0,   0,   0, 178,   0, 178, 104,   0,   0,   0],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the mapping of the target language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = vocab_transform[TRG_LANGUAGE].get_itos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "กำลังจะ\n",
      "<unk>\n",
      "<unk>\n",
      "<unk>\n",
      "<unk>\n",
      "ริ\n",
      "<unk>\n",
      "ริ\n",
      "ที่นี่\n",
      "<unk>\n",
      "<unk>\n",
      "<unk>\n"
     ]
    }
   ],
   "source": [
    "for token in output_max:\n",
    "    print(mapping[token.item()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Attention\n",
    "\n",
    "Let's display the attentions to understand how the source text links with the generated text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 13, 11])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attentions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are 8 heads, we can look at just 1 head for sake of simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([13, 11])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention = attentions[0, 0, :, :]\n",
    "attention.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<sos>',\n",
       " 'Did',\n",
       " 'you',\n",
       " 'get',\n",
       " 'that',\n",
       " 'thesis',\n",
       " 'from',\n",
       " 'the',\n",
       " 'Dude',\n",
       " '?',\n",
       " '<eos>']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_tokens = ['<sos>'] + token_transform[SRC_LANGUAGE](test_set[0]['en']) + ['<eos>']\n",
    "src_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<sos>',\n",
       " 'กำลังจะ',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " 'ริ',\n",
       " '<unk>',\n",
       " 'ริ',\n",
       " 'ที่นี่',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trg_tokens = ['<sos>'] + [mapping[token.item()] for token in output_max]\n",
    "trg_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker as ticker\n",
    "\n",
    "def display_attention(sentence, translation, attention):\n",
    "    \n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(111)\n",
    "    \n",
    "    attention = attention.squeeze(1).cpu().detach().numpy()\n",
    "    \n",
    "    cax = ax.matshow(attention, cmap='bone')\n",
    "   \n",
    "    ax.tick_params(labelsize=10)\n",
    "    \n",
    "    y_ticks =  [''] + translation\n",
    "    x_ticks =  [''] + sentence \n",
    "     \n",
    "    ax.set_xticklabels(x_ticks, rotation=45)\n",
    "    ax.set_yticklabels(y_ticks)\n",
    "\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Neo\\AppData\\Local\\Temp\\ipykernel_32\\59549304.py:17: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_xticklabels(x_ticks, rotation=45)\n",
      "C:\\Users\\Neo\\AppData\\Local\\Temp\\ipykernel_32\\59549304.py:18: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_yticklabels(y_ticks)\n",
      "c:\\Users\\Neo\\anaconda3\\envs\\dsai\\lib\\site-packages\\IPython\\core\\pylabtools.py:152: UserWarning: Glyph 3585 (\\N{THAI CHARACTER KO KAI}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\Neo\\anaconda3\\envs\\dsai\\lib\\site-packages\\IPython\\core\\pylabtools.py:152: UserWarning: Glyph 3635 (\\N{THAI CHARACTER SARA AM}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\Neo\\anaconda3\\envs\\dsai\\lib\\site-packages\\IPython\\core\\pylabtools.py:152: UserWarning: Glyph 3621 (\\N{THAI CHARACTER LO LING}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\Neo\\anaconda3\\envs\\dsai\\lib\\site-packages\\IPython\\core\\pylabtools.py:152: UserWarning: Glyph 3633 (\\N{THAI CHARACTER MAI HAN-AKAT}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\Neo\\anaconda3\\envs\\dsai\\lib\\site-packages\\IPython\\core\\pylabtools.py:152: UserWarning: Glyph 3591 (\\N{THAI CHARACTER NGO NGU}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\Neo\\anaconda3\\envs\\dsai\\lib\\site-packages\\IPython\\core\\pylabtools.py:152: UserWarning: Glyph 3592 (\\N{THAI CHARACTER CHO CHAN}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\Neo\\anaconda3\\envs\\dsai\\lib\\site-packages\\IPython\\core\\pylabtools.py:152: UserWarning: Glyph 3632 (\\N{THAI CHARACTER SARA A}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\Neo\\anaconda3\\envs\\dsai\\lib\\site-packages\\IPython\\core\\pylabtools.py:152: UserWarning: Glyph 3619 (\\N{THAI CHARACTER RO RUA}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\Neo\\anaconda3\\envs\\dsai\\lib\\site-packages\\IPython\\core\\pylabtools.py:152: UserWarning: Glyph 3636 (\\N{THAI CHARACTER SARA I}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\Neo\\anaconda3\\envs\\dsai\\lib\\site-packages\\IPython\\core\\pylabtools.py:152: UserWarning: Glyph 3607 (\\N{THAI CHARACTER THO THAHAN}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\Neo\\anaconda3\\envs\\dsai\\lib\\site-packages\\IPython\\core\\pylabtools.py:152: UserWarning: Glyph 3637 (\\N{THAI CHARACTER SARA II}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\Neo\\anaconda3\\envs\\dsai\\lib\\site-packages\\IPython\\core\\pylabtools.py:152: UserWarning: Glyph 3656 (\\N{THAI CHARACTER MAI EK}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\Neo\\anaconda3\\envs\\dsai\\lib\\site-packages\\IPython\\core\\pylabtools.py:152: UserWarning: Glyph 3609 (\\N{THAI CHARACTER NO NU}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_attention(src_tokens, trg_tokens, attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "714d3f4db9a58ba7d2f2a9a4fffe577af3df8551aebd380095064812e2e0a6a4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
