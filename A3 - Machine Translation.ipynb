{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Translation + Transformer\n",
    "\n",
    "<img src = \"../figures/transformer1.png\" >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Neo\\anaconda3\\envs\\dsai\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch, torchdata, torchtext\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import random, math, time\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "#make our work comparable if restarted the kernel\n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.1+cu118'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.16.1+cpu'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchtext.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 1. ETL: Loading the dataset\n",
    "\n",
    "**Note**: Here I chose to translate English to German, simply it is easier for myself, since I don't understand German so it is difficult for me to imagine a sentence during inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "SRC_LANGUAGE = 'en'\n",
    "TRG_LANGUAGE = 'th'\n",
    "\n",
    "dataset = datasets.load_dataset(\"opus100\", \"en-th\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 1000000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import default_rng\n",
    "\n",
    "rng = default_rng(seed=SEED)\n",
    "# create a list of non-repeated indices of size 10000 and use it to select the training samples\n",
    "select_idx = rng.choice(len(dataset['train']), size=1000, replace=False)\n",
    "dataset['train'] = dataset['train'].filter(lambda example, idx: idx in select_idx, with_indices=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_lang_col = lambda example, lang: {lang: example['translation'][lang]}\n",
    "dataset = dataset.map(get_lang_col, fn_kwargs={'lang': \"th\"})\n",
    "dataset = dataset.map(get_lang_col, remove_columns=['translation'], fn_kwargs={'lang': \"en\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'th': '-ชั้นทำอยู่', 'en': '- I am blending.'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['th', 'en'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['th', 'en'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['th', 'en'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val, test = dataset['train'], dataset['test'], dataset['validation']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 3. Preprocessing \n",
    "\n",
    "### Tokenizing\n",
    "\n",
    "**Note**: the models must first be downloaded using the following on the command line: \n",
    "```\n",
    "python3 -m spacy download en_core_web_sm\n",
    "python3 -m spacy download de_core_news_sm\n",
    "```\n",
    "\n",
    "First, since we have two languages, let's create some constants to represent that.  Also, let's create two dicts: one for holding our tokenizers and one for holding all the vocabs with assigned numbers for each unique word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pythainlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place-holders\n",
    "token_transform = {}\n",
    "vocab_transform = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from pythainlp.tokenize import Tokenizer\n",
    "\n",
    "token_transform[SRC_LANGUAGE] = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "token_transform[TRG_LANGUAGE] = Tokenizer(engine='newmm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence:  -ชั้นทำอยู่\n",
      "Tokenization:  ['-', 'ชั้น', 'ทำ', 'อยู่']\n"
     ]
    }
   ],
   "source": [
    "#example of tokenization of the thai part\n",
    "print(\"Sentence: \", dataset['train'][TRG_LANGUAGE][2])\n",
    "print(\"Tokenization: \", token_transform[TRG_LANGUAGE].word_tokenize(dataset['train'][TRG_LANGUAGE][2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2000/2000 [00:00<00:00, 4376.23 examples/s]\n",
      "Map: 100%|██████████| 2000/2000 [00:00<00:00, 5493.95 examples/s]\n",
      "Map: 100%|██████████| 1000/1000 [00:00<00:00, 3520.88 examples/s]\n",
      "Map: 100%|██████████| 2000/2000 [00:00<00:00, 5305.04 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def tokenize_data(example, lang):\n",
    "    try:\n",
    "        return {lang: token_transform[lang](example[lang].lower())}\n",
    "    except:\n",
    "        return {lang: token_transform[lang].word_tokenize(example[lang].lower())}\n",
    "    \n",
    "tokenized_dataset = dataset.map(tokenize_data, remove_columns=[SRC_LANGUAGE], fn_kwargs={'lang': SRC_LANGUAGE})\n",
    "tokenized_dataset = tokenized_dataset.map(tokenize_data, remove_columns=[TRG_LANGUAGE], fn_kwargs={'lang': TRG_LANGUAGE})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'th': ['พระองค์',\n",
       "  'คือ',\n",
       "  'ผู้ทรง',\n",
       "  'ประทาน',\n",
       "  'คัมภีร์',\n",
       "  'ลงมา',\n",
       "  'แก่',\n",
       "  'เจ้า',\n",
       "  ' ',\n",
       "  'โดยที่',\n",
       "  'ส่วนหนึ่ง',\n",
       "  'จาก',\n",
       "  'คัมภีร์',\n",
       "  'นั้น',\n",
       "  'มี',\n",
       "  'บรรดา',\n",
       "  'โฮ',\n",
       "  'ง',\n",
       "  'การ',\n",
       "  'ที่',\n",
       "  'มี',\n",
       "  'ข้อความ',\n",
       "  'รัดกุม',\n",
       "  'ชัดเจน',\n",
       "  ' ',\n",
       "  'ซึ่ง',\n",
       "  'โองการ',\n",
       "  'เหล่านั้น',\n",
       "  ' ',\n",
       "  'คือ',\n",
       "  'รากฐาน',\n",
       "  'ของ',\n",
       "  'คัมภีร์',\n",
       "  ' ',\n",
       "  'และ',\n",
       "  'มี',\n",
       "  'โองการ',\n",
       "  'อื่น',\n",
       "  ' ',\n",
       "  'ไ',\n",
       "  ' ',\n",
       "  'อีก',\n",
       "  'ที่',\n",
       "  'มี',\n",
       "  'ข้อความ',\n",
       "  'เป็นนัย',\n",
       "  ' ',\n",
       "  'ส่วน',\n",
       "  'บรรดา',\n",
       "  'ผู้',\n",
       "  'ที่',\n",
       "  'ใน',\n",
       "  'หัวใจ',\n",
       "  'ของ',\n",
       "  'พวกเขา',\n",
       "  'มี',\n",
       "  'การ',\n",
       "  'เอนเอียง',\n",
       "  'ออกจาก',\n",
       "  'ความจริง',\n",
       "  'นั้น',\n",
       "  ' ',\n",
       "  'เขา',\n",
       "  'จะ',\n",
       "  'ติดตาม',\n",
       "  'โองการ',\n",
       "  'ที่',\n",
       "  'มี',\n",
       "  'ข้อความ',\n",
       "  'เป็นนัย',\n",
       "  'จาก',\n",
       "  'คัมภีร์',\n",
       "  ' ',\n",
       "  'ทั้งนี้',\n",
       "  ' ',\n",
       "  'เพื่อ',\n",
       "  'แสวงหา',\n",
       "  'ความวุ่นวาย',\n",
       "  ' ',\n",
       "  'และ',\n",
       "  'เพื่อ',\n",
       "  'แสวงหา',\n",
       "  'การ',\n",
       "  'ตี',\n",
       "  'ความใน',\n",
       "  'โองการ',\n",
       "  'นั้น',\n",
       "  ' ',\n",
       "  'แล',\n",
       "  'ไม่',\n",
       "  'มี',\n",
       "  'ใคร',\n",
       "  'รู้',\n",
       "  'ใน',\n",
       "  'การ',\n",
       "  'ตีความ',\n",
       "  'โองการ',\n",
       "  'นั้น',\n",
       "  'ได้',\n",
       "  'นอกจาก',\n",
       "  'อัลลอฮ์',\n",
       "  ' ',\n",
       "  'และ',\n",
       "  'บรรดา',\n",
       "  'ผู้',\n",
       "  'ที่',\n",
       "  'มั่นคง',\n",
       "  'ใน',\n",
       "  'ความรู้',\n",
       "  'เท่านั้น',\n",
       "  ' ',\n",
       "  'โดยที่',\n",
       "  'พวกเขา',\n",
       "  'จะ',\n",
       "  'กล่าวว่า',\n",
       "  ' ',\n",
       "  'พวกเรา',\n",
       "  'ศรัทธา',\n",
       "  'ต่อ',\n",
       "  'โองการ',\n",
       "  'นั้น',\n",
       "  ' ',\n",
       "  'ทั้งหมด',\n",
       "  'นั้น',\n",
       "  'มาจาก',\n",
       "  'ที่',\n",
       "  'ที่',\n",
       "  'พระผู้เป็นเจ้า',\n",
       "  'ของ',\n",
       "  'เรา',\n",
       "  'ทั้งสิ้น',\n",
       "  ' ',\n",
       "  'และ',\n",
       "  'ไม่',\n",
       "  'มี',\n",
       "  'ใคร',\n",
       "  'ที่จะ',\n",
       "  'รับ',\n",
       "  'คำตักเตือน',\n",
       "  'นอกจาก',\n",
       "  'บรรดา',\n",
       "  'ผู้',\n",
       "  'ที่',\n",
       "  'มีสติปัญญา',\n",
       "  'เท่านั้น'],\n",
       " 'en': ['it',\n",
       "  'is',\n",
       "  'he',\n",
       "  'who',\n",
       "  'has',\n",
       "  'sent',\n",
       "  'down',\n",
       "  'to',\n",
       "  'you',\n",
       "  '(',\n",
       "  'muhammad',\n",
       "  'saw',\n",
       "  ')',\n",
       "  'the',\n",
       "  'book',\n",
       "  '(',\n",
       "  'this',\n",
       "  'quran',\n",
       "  ')',\n",
       "  '.',\n",
       "  'in',\n",
       "  'it',\n",
       "  'are',\n",
       "  'verses',\n",
       "  'that',\n",
       "  'are',\n",
       "  'entirely',\n",
       "  'clear',\n",
       "  ',',\n",
       "  'they',\n",
       "  'are',\n",
       "  'the',\n",
       "  'foundations',\n",
       "  'of',\n",
       "  'the',\n",
       "  'book',\n",
       "  '[',\n",
       "  'and',\n",
       "  'those',\n",
       "  'are',\n",
       "  'the',\n",
       "  'verses',\n",
       "  'of',\n",
       "  'al',\n",
       "  '-',\n",
       "  'ahkam',\n",
       "  '(',\n",
       "  'commandments',\n",
       "  ',',\n",
       "  'etc',\n",
       "  '.',\n",
       "  ')',\n",
       "  ',',\n",
       "  'al',\n",
       "  '-',\n",
       "  \"fara'id\",\n",
       "  '(',\n",
       "  'obligatory',\n",
       "  'duties',\n",
       "  ')',\n",
       "  'and',\n",
       "  'al',\n",
       "  '-',\n",
       "  'hudud',\n",
       "  '(',\n",
       "  'legal',\n",
       "  'laws',\n",
       "  'for',\n",
       "  'the',\n",
       "  'punishment',\n",
       "  'of',\n",
       "  'thieves',\n",
       "  ',',\n",
       "  'adulterers',\n",
       "  ',',\n",
       "  'etc',\n",
       "  '.',\n",
       "  ')',\n",
       "  ']',\n",
       "  ';',\n",
       "  'and',\n",
       "  'others',\n",
       "  'not',\n",
       "  'entirely',\n",
       "  'clear',\n",
       "  '.',\n",
       "  'so',\n",
       "  'as',\n",
       "  'for',\n",
       "  'those',\n",
       "  'in',\n",
       "  'whose',\n",
       "  'hearts',\n",
       "  'there',\n",
       "  'is',\n",
       "  'a',\n",
       "  'deviation',\n",
       "  '(',\n",
       "  'from',\n",
       "  'the',\n",
       "  'truth',\n",
       "  ')',\n",
       "  'they',\n",
       "  'follow',\n",
       "  'that',\n",
       "  'which',\n",
       "  'is',\n",
       "  'not',\n",
       "  'entirely',\n",
       "  'clear',\n",
       "  'thereof',\n",
       "  ',',\n",
       "  'seeking',\n",
       "  'al',\n",
       "  '-',\n",
       "  'fitnah',\n",
       "  '(',\n",
       "  'polytheism',\n",
       "  'and',\n",
       "  'trials',\n",
       "  ',',\n",
       "  'etc',\n",
       "  '.',\n",
       "  ')',\n",
       "  ',',\n",
       "  'and',\n",
       "  'seeking',\n",
       "  'for',\n",
       "  'its',\n",
       "  'hidden',\n",
       "  'meanings',\n",
       "  ',',\n",
       "  'but',\n",
       "  'none',\n",
       "  'knows',\n",
       "  'its',\n",
       "  'hidden',\n",
       "  'meanings',\n",
       "  'save',\n",
       "  'allah',\n",
       "  '.',\n",
       "  'and',\n",
       "  'those',\n",
       "  'who',\n",
       "  'are',\n",
       "  'firmly',\n",
       "  'grounded',\n",
       "  'in',\n",
       "  'knowledge',\n",
       "  'say',\n",
       "  ':',\n",
       "  '\"',\n",
       "  'we',\n",
       "  'believe',\n",
       "  'in',\n",
       "  'it',\n",
       "  ';',\n",
       "  'the',\n",
       "  'whole',\n",
       "  'of',\n",
       "  'it',\n",
       "  '(',\n",
       "  'clear',\n",
       "  'and',\n",
       "  'unclear',\n",
       "  'verses',\n",
       "  ')',\n",
       "  'are',\n",
       "  'from',\n",
       "  'our',\n",
       "  'lord',\n",
       "  '.',\n",
       "  '\"',\n",
       "  'and',\n",
       "  'none',\n",
       "  'receive',\n",
       "  'admonition',\n",
       "  'except',\n",
       "  'men',\n",
       "  'of',\n",
       "  'understanding',\n",
       "  '.',\n",
       "  '(',\n",
       "  'tafsir',\n",
       "  'at',\n",
       "  '-',\n",
       "  'tabari',\n",
       "  ')',\n",
       "  '.']}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define special symbols and indices\n",
    "UNK_IDX, PAD_IDX, SOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
    "special_symbols = ['<unk>', '<pad>', '<sos>', '<eos>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lang in [SRC_LANGUAGE, TRG_LANGUAGE]:\n",
    "    vocab_transform[lang] = torchtext.vocab.build_vocab_from_iterator(tokenized_dataset['train'][lang],\n",
    "                                                                      min_freq=3,   #if not, everything will be treated as UNK\n",
    "                                                                      specials=special_symbols,\n",
    "                                                                      special_first=True) #indicates whether to insert symbols at the beginning or at the end)\n",
    "    # Set UNK_IDX as the default index. This index is returned when the token is not found. \n",
    "    # If not set, it throws RuntimeError when the queried token is not found in the Vocabulary. \n",
    "    vocab_transform[lang].set_default_index(UNK_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[40, 0]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#see some example\n",
    "vocab_transform[SRC_LANGUAGE](['my', 'precious'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[13, 202, 13, 78]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#see some example\n",
    "vocab_transform[TRG_LANGUAGE](['ของ', 'รัก', 'ของ', 'ข้า'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ของ'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we can reverse it....\n",
    "mapping = vocab_transform[TRG_LANGUAGE].get_itos()\n",
    "\n",
    "#print 1816, for example\n",
    "mapping[13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<unk>'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's try unknown vocab\n",
    "mapping[0]\n",
    "#they will all map to <unk> which has 0 as integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<pad>', '<sos>', '<eos>')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's try special symbols\n",
    "mapping[1], mapping[2], mapping[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "458"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check unique vocabularies\n",
    "len(mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4. Preparing the dataloader\n",
    "\n",
    "One thing we change here is the <code>collate_fn</code> which now also returns the length of sentence.  This is required for <code>packed_padded_sequence</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# helper function to club together sequential operations\n",
    "def sequential_transforms(*transforms):\n",
    "    def func(txt_input):\n",
    "        for transform in transforms:\n",
    "            try:\n",
    "                txt_input = transform(txt_input)\n",
    "            except TypeError:\n",
    "                txt_input = transform.word_tokenize(txt_input)\n",
    "        return txt_input\n",
    "    return func\n",
    "\n",
    "# function to add BOS/EOS and create tensor for input sequence indices\n",
    "def tensor_transform(token_ids):\n",
    "    return torch.cat((torch.tensor([SOS_IDX]), \n",
    "                      torch.tensor(token_ids), \n",
    "                      torch.tensor([EOS_IDX])))\n",
    "\n",
    "# src and trg language text transforms to convert raw strings into tensors indices\n",
    "text_transform = {}\n",
    "for ln in [SRC_LANGUAGE, TRG_LANGUAGE]:\n",
    "    text_transform[ln] = sequential_transforms(token_transform[ln], #Tokenization\n",
    "                                               vocab_transform[ln], #Numericalization\n",
    "                                               tensor_transform) # Add BOS/EOS and create tensor\n",
    "\n",
    "\n",
    "# function to collate data samples into batch tesors\n",
    "def collate_batch(batch):\n",
    "    src_batch, src_len_batch, trg_batch = [], [], []\n",
    "    for sample in batch:\n",
    "        processed_text = text_transform[SRC_LANGUAGE](sample[SRC_LANGUAGE].rstrip(\"\\n\"))\n",
    "        src_batch.append(processed_text)\n",
    "        trg_batch.append(text_transform[TRG_LANGUAGE](sample[TRG_LANGUAGE].rstrip(\"\\n\")))\n",
    "        src_len_batch.append(processed_text.size(0))\n",
    "\n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX, batch_first=True)\n",
    "    trg_batch = pad_sequence(trg_batch, padding_value=PAD_IDX, batch_first=True)\n",
    "    return src_batch, torch.tensor(src_len_batch, dtype=torch.int64), trg_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create train, val, and test dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(train, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
    "valid_loader = DataLoader(val,   batch_size=batch_size, shuffle=False, collate_fn=collate_batch)\n",
    "test_loader  = DataLoader(test,  batch_size=batch_size, shuffle=False, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the train loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for en, _, th in train_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English shape:  torch.Size([32, 19])\n",
      "Thai shape:  torch.Size([32, 28])\n"
     ]
    }
   ],
   "source": [
    "print(\"English shape: \", en.shape)  # (batch_size, seq len)\n",
    "print(\"Thai shape: \", th.shape)   # (batch_size, seq len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Design the model\n",
    "\n",
    "<img src=\"../figures/transformer-encoder.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, pf_dim, dropout, attn_variant, device):\n",
    "        super().__init__()\n",
    "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.ff_layer_norm        = nn.LayerNorm(hid_dim)\n",
    "        self.self_attention       = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, attn_variant, device)\n",
    "        self.feedforward          = PositionwiseFeedforwardLayer(hid_dim, pf_dim, dropout)\n",
    "        self.dropout              = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src, src_mask):\n",
    "        #src = [batch size, src len, hid dim]\n",
    "        #src_mask = [batch size, 1, 1, src len]   #if the token is padding, it will be 1, otherwise 0\n",
    "        _src, _ = self.self_attention(src, src, src, src_mask)\n",
    "        src     = self.self_attn_layer_norm(src + self.dropout(_src))\n",
    "        #src: [batch_size, src len, hid dim]\n",
    "        \n",
    "        _src    = self.feedforward(src)\n",
    "        src     = self.ff_layer_norm(src + self.dropout(_src))\n",
    "        #src: [batch_size, src len, hid dim]\n",
    "        \n",
    "        return src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hid_dim, n_layers, n_heads, pf_dim, dropout, attn_variant, device, max_length = 500):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.attn_variant = attn_variant\n",
    "        self.tok_embedding = nn.Embedding(input_dim, hid_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
    "        self.layers        = nn.ModuleList([EncoderLayer(hid_dim, n_heads, pf_dim, dropout, attn_variant, device)\n",
    "                                           for _ in range(n_layers)])\n",
    "        self.dropout       = nn.Dropout(dropout)\n",
    "        self.scale         = torch.sqrt(torch.FloatTensor([hid_dim])).to(self.device)\n",
    "        \n",
    "    def forward(self, src, src_mask):\n",
    "        \n",
    "        #src = [batch size, src len]\n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "        \n",
    "        batch_size = src.shape[0]\n",
    "        src_len    = src.shape[1]\n",
    "        \n",
    "        pos        = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
    "        #pos: [batch_size, src_len]\n",
    "        \n",
    "        src        = self.dropout((self.tok_embedding(src) * self.scale) + self.pos_embedding(pos))\n",
    "        #src: [batch_size, src_len, hid_dim]\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            src = layer(src, src_mask)\n",
    "        #src: [batch_size, src_len, hid_dim]\n",
    "        \n",
    "        return src\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mutli Head Attention Layer\n",
    "\n",
    "<img src = \"../figures/transformer-attention.png\" width=\"700\">\n",
    "\n",
    "$$ \\text{Attention}(Q, K, V) = \\text{Softmax} \\big( \\frac{QK^T}{\\sqrt{d_k}} \\big)V $$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledAttention(nn.Module):\n",
    "    def __init__(self, head_dim):\n",
    "        super(ScaledAttention, self).__init__()\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([head_dim])).to(device)\n",
    "\n",
    "    def forward(self, Q, K):\n",
    "        scores = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n",
    "        # scores: [batch_size, n_heads, query len, key len]\n",
    "\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiplicativeAttention(nn.Module):\n",
    "    def __init__(self, head_dim):\n",
    "        super(MultiplicativeAttention, self).__init__()\n",
    "        self.W1 = nn.Linear(head_dim, head_dim)\n",
    "\n",
    "    def forward(self, Q, K):\n",
    "        scores = torch.matmul(self.W1(Q), K.permute(0, 1, 3, 2))\n",
    "        # scores: [batch_size, n_heads, query len, key len]\n",
    "\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdditiveAttention(nn.Module):\n",
    "    def __init__(self, head_dim):\n",
    "        super(AdditiveAttention, self).__init__()\n",
    "        self.W1 = nn.Linear(head_dim, head_dim)\n",
    "        self.W2 = nn.Linear(head_dim, head_dim)\n",
    "        self.V = nn.Linear(head_dim, 1)\n",
    "\n",
    "    def forward(self, Q, K):\n",
    "        Q = Q.unsqueeze(3)  # Q: [batch_size, n_heads, query len, head_dim] => [batch_size, n_heads, query len, 1, head_dim]\n",
    "        K = K.unsqueeze(2)  # Q: [batch_size, n_heads, key len, head_dim] => [batch_size, n_heads, 1, key len, head_dim]\n",
    "        features = torch.tanh(self.W1(Q) + self.W2(K))\n",
    "        # features: [batch_size, n_heads, query len, key len, head_dim]\n",
    "\n",
    "        scores = self.V(features).squeeze(-1)\n",
    "        # scores: [batch_size, n_heads, query len, key len]\n",
    "\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, dropout, attn_variant, device):\n",
    "        super().__init__()\n",
    "        assert hid_dim % n_heads == 0\n",
    "        self.hid_dim  = hid_dim\n",
    "        self.n_heads  = n_heads\n",
    "        self.head_dim = hid_dim // n_heads\n",
    "        self.attn_variant = attn_variant\n",
    "        \n",
    "        self.fc_q     = nn.Linear(hid_dim, hid_dim)\n",
    "        self.fc_k     = nn.Linear(hid_dim, hid_dim)\n",
    "        self.fc_v     = nn.Linear(hid_dim, hid_dim)\n",
    "        \n",
    "        self.fc_o     = nn.Linear(hid_dim, hid_dim)\n",
    "        \n",
    "        self.dropout  = nn.Dropout(dropout)\n",
    "\n",
    "        if attn_variant == 'scaled':\n",
    "            self.scaled_attention = ScaledAttention(self.head_dim)\n",
    "\n",
    "        elif attn_variant == 'multiplicative':\n",
    "            self.multiplicative_attention = MultiplicativeAttention(self.head_dim)\n",
    "        \n",
    "        elif attn_variant == 'additive':\n",
    "            self.additive_attention = AdditiveAttention(self.head_dim)\n",
    "                \n",
    "    def forward(self, query, key, value, mask = None):\n",
    "        #src, src, src, src_mask\n",
    "        #query = [batch size, query len, hid dim]\n",
    "        #key = [batch size, key len, hid dim]\n",
    "        #value = [batch size, value len, hid dim]\n",
    "        \n",
    "        batch_size = query.shape[0]\n",
    "        \n",
    "        Q = self.fc_q(query)\n",
    "        K = self.fc_k(key)\n",
    "        V = self.fc_v(value)\n",
    "        # Q: [batch_size, seq len, hid_dim]\n",
    "        \n",
    "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        # Q=K=V: [batch_size, n heads, seq len, head_dim]\n",
    "        \n",
    "        # energy = self.additive_attention(Q, K)\n",
    "        if self.attn_variant == \"general\":\n",
    "            energy = torch.matmul(Q, K.permute(0, 1, 3, 2))\n",
    "\n",
    "        elif self.attn_variant == \"scaled\":\n",
    "            energy = self.scaled_attention(Q, K)\n",
    "\n",
    "        elif self.attn_variant == \"multiplicative\":\n",
    "            energy = self.multiplicative_attention(Q, K)\n",
    "\n",
    "        elif self.attn_variant == \"additive\":\n",
    "            energy = self.additive_attention(Q, K)\n",
    "            \n",
    "        else:\n",
    "            raise Exception(\"Incorrect value for attention variant. Must be one of the following: \\\n",
    "                            scaled, general, multiplicative, additive\")\n",
    "        \n",
    "        #for making attention to padding to 0\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, -1e10)\n",
    "            \n",
    "        attention = torch.softmax(energy, dim = -1)\n",
    "        #attention = [batch_size, n heads, query len, key len]\n",
    "\n",
    "        x = torch.matmul(self.dropout(attention), V)\n",
    "        #[batch_size, n heads, query len, key len] @ [batch_size, n heads, value len, head_dim]\n",
    "        #x = [batch_size, n heads, query len, head dim]\n",
    "        \n",
    "        x = x.permute(0, 2, 1, 3).contiguous()  #we can perform .view\n",
    "        #x = [batch_size, query len, n heads, head dim]\n",
    "        \n",
    "        x = x.view(batch_size, -1, self.hid_dim)\n",
    "        #x = [batch_size, query len, hid dim]\n",
    "        \n",
    "        x = self.fc_o(x)\n",
    "        #x = [batch_size, query len, hid dim]\n",
    "        \n",
    "        return x, attention\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Position-wise Feedforward Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedforwardLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, pf_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(hid_dim, pf_dim)\n",
    "        self.fc2 = nn.Linear(pf_dim, hid_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #x = [batch size, src len, hid dim]\n",
    "        x = self.dropout(torch.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder Layer\n",
    "\n",
    "<img src = \"../figures/transformer-decoder.png\" >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, pf_dim, dropout, attn_variant, device):\n",
    "        super().__init__()\n",
    "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.enc_attn_layer_norm  = nn.LayerNorm(hid_dim)\n",
    "        self.ff_layer_norm        = nn.LayerNorm(hid_dim)\n",
    "        self.self_attention       = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, attn_variant, device)\n",
    "        self.encoder_attention    = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, attn_variant, device)\n",
    "        self.feedforward          = PositionwiseFeedforwardLayer(hid_dim, pf_dim, dropout)\n",
    "        self.dropout              = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "        \n",
    "        #trg = [batch size, trg len, hid dim]\n",
    "        #enc_src = [batch size, src len, hid dim]\n",
    "        #trg_mask = [batch size, 1, trg len, trg len]\n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "        \n",
    "        _trg, _ = self.self_attention(trg, trg, trg, trg_mask)\n",
    "        trg     = self.self_attn_layer_norm(trg + self.dropout(_trg))\n",
    "        #trg = [batch_size, trg len, hid dim]\n",
    "        \n",
    "        _trg, attention = self.encoder_attention(trg, enc_src, enc_src, src_mask)\n",
    "        trg             = self.enc_attn_layer_norm(trg + self.dropout(_trg))\n",
    "        #trg = [batch_size, trg len, hid dim]\n",
    "        #attention = [batch_size, n heads, trg len, src len]\n",
    "        \n",
    "        _trg = self.feedforward(trg)\n",
    "        trg  = self.ff_layer_norm(trg + self.dropout(_trg))\n",
    "        #trg = [batch_size, trg len, hid dim]\n",
    "        \n",
    "        return trg, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, hid_dim, n_layers, n_heads, \n",
    "                 pf_dim, dropout, attn_variant, device, max_length = 500):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.tok_embedding = nn.Embedding(output_dim, hid_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
    "        self.layers        = nn.ModuleList([DecoderLayer(hid_dim, n_heads, pf_dim, dropout, attn_variant, device)\n",
    "                                            for _ in range(n_layers)])\n",
    "        self.fc_out        = nn.Linear(hid_dim, output_dim)\n",
    "        self.dropout       = nn.Dropout(dropout)\n",
    "        self.scale         = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
    "        \n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "        \n",
    "        #trg = [batch size, trg len]\n",
    "        #enc_src = [batch size, src len, hid dim]\n",
    "        #trg_mask = [batch size, 1, trg len, trg len]\n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "        \n",
    "        batch_size = trg.shape[0]\n",
    "        trg_len    = trg.shape[1]\n",
    "        \n",
    "        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
    "        #pos: [batch_size, trg len]\n",
    "        \n",
    "        trg = self.dropout((self.tok_embedding(trg) * self.scale) + self.pos_embedding(pos))\n",
    "        #trg: [batch_size, trg len, hid dim]\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            trg, attention = layer(trg, enc_src, trg_mask, src_mask)\n",
    "            \n",
    "        #trg: [batch_size, trg len, hid dim]\n",
    "        #attention: [batch_size, n heads, trg len, src len]\n",
    "        \n",
    "        output = self.fc_out(trg)\n",
    "        #output = [batch_size, trg len, output_dim]\n",
    "        \n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting them together (become Seq2Seq!)\n",
    "\n",
    "Our `trg_sub_mask` will look something like this (for a target with 5 tokens):\n",
    "\n",
    "$$\\begin{matrix}\n",
    "1 & 0 & 0 & 0 & 0\\\\\n",
    "1 & 1 & 0 & 0 & 0\\\\\n",
    "1 & 1 & 1 & 0 & 0\\\\\n",
    "1 & 1 & 1 & 1 & 0\\\\\n",
    "1 & 1 & 1 & 1 & 1\\\\\n",
    "\\end{matrix}$$\n",
    "\n",
    "The \"subsequent\" mask is then logically anded with the padding mask, this combines the two masks ensuring both the subsequent tokens and the padding tokens cannot be attended to. For example if the last two tokens were `<pad>` tokens the mask would look like:\n",
    "\n",
    "$$\\begin{matrix}\n",
    "1 & 0 & 0 & 0 & 0\\\\\n",
    "1 & 1 & 0 & 0 & 0\\\\\n",
    "1 & 1 & 1 & 0 & 0\\\\\n",
    "1 & 1 & 1 & 0 & 0\\\\\n",
    "1 & 1 & 1 & 0 & 0\\\\\n",
    "\\end{matrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(self, encoder, decoder, src_pad_idx, trg_pad_idx, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        self.device = device\n",
    "        \n",
    "    def make_src_mask(self, src):\n",
    "        \n",
    "        #src = [batch size, src len]\n",
    "        \n",
    "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "\n",
    "        return src_mask\n",
    "    \n",
    "    def make_trg_mask(self, trg):\n",
    "        \n",
    "        #trg = [batch size, trg len]\n",
    "        \n",
    "        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        #trg_pad_mask = [batch size, 1, 1, trg len]\n",
    "        \n",
    "        trg_len = trg.shape[1]\n",
    "        \n",
    "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device = self.device)).bool()\n",
    "        #trg_sub_mask = [trg len, trg len]\n",
    "            \n",
    "        trg_mask = trg_pad_mask & trg_sub_mask\n",
    "        #trg_mask = [batch size, 1, trg len, trg len]\n",
    "        \n",
    "        return trg_mask\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        \n",
    "        #src = [batch size, src len]\n",
    "        #trg = [batch size, trg len]\n",
    "                \n",
    "        src_mask = self.make_src_mask(src)\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "        \n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "        #trg_mask = [batch size, 1, trg len, trg len]\n",
    "        \n",
    "        enc_src = self.encoder(src, src_mask)\n",
    "        #enc_src = [batch size, src len, hid dim]\n",
    "                \n",
    "        output, attention = self.decoder(trg, enc_src, trg_mask, src_mask)\n",
    "        \n",
    "        #output = [batch size, trg len, output dim]\n",
    "        #attention = [batch size, n heads, trg len, src len]\n",
    "        \n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(m):\n",
    "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
    "        nn.init.xavier_uniform_(m.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2SeqTransformer(\n",
       "  (encoder): Encoder(\n",
       "    (tok_embedding): Embedding(370, 256)\n",
       "    (pos_embedding): Embedding(500, 256)\n",
       "    (layers): ModuleList(\n",
       "      (0-2): 3 x EncoderLayer(\n",
       "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (multiplicative_attention): MultiplicativeAttention(\n",
       "            (W1): Linear(in_features=32, out_features=32, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (feedforward): PositionwiseFeedforwardLayer(\n",
       "          (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (tok_embedding): Embedding(458, 256)\n",
       "    (pos_embedding): Embedding(500, 256)\n",
       "    (layers): ModuleList(\n",
       "      (0-2): 3 x DecoderLayer(\n",
       "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (enc_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (multiplicative_attention): MultiplicativeAttention(\n",
       "            (W1): Linear(in_features=32, out_features=32, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (encoder_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (multiplicative_attention): MultiplicativeAttention(\n",
       "            (W1): Linear(in_features=32, out_features=32, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (feedforward): PositionwiseFeedforwardLayer(\n",
       "          (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (fc_out): Linear(in_features=256, out_features=458, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dim   = len(vocab_transform[SRC_LANGUAGE])\n",
    "output_dim  = len(vocab_transform[TRG_LANGUAGE])\n",
    "hid_dim = 256\n",
    "enc_layers = 3\n",
    "dec_layers = 3\n",
    "enc_heads = 8\n",
    "dec_heads = 8\n",
    "enc_pf_dim = 512\n",
    "dec_pf_dim = 512\n",
    "enc_dropout = 0.1\n",
    "dec_dropout = 0.1\n",
    "attn_variant = 'multiplicative'\n",
    "\n",
    "SRC_PAD_IDX = PAD_IDX\n",
    "TRG_PAD_IDX = PAD_IDX\n",
    "\n",
    "enc = Encoder(input_dim, \n",
    "              hid_dim, \n",
    "              enc_layers, \n",
    "              enc_heads, \n",
    "              enc_pf_dim, \n",
    "              enc_dropout, \n",
    "              attn_variant,\n",
    "              device)\n",
    "\n",
    "dec = Decoder(output_dim, \n",
    "              hid_dim, \n",
    "              dec_layers, \n",
    "              dec_heads, \n",
    "              dec_pf_dim, \n",
    "              enc_dropout, \n",
    "              attn_variant,\n",
    "              device)\n",
    "\n",
    "model = Seq2SeqTransformer(enc, dec, SRC_PAD_IDX, TRG_PAD_IDX, device).to(device)\n",
    "model.apply(initialize_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 94720\n",
      "128000\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      "  1024\n",
      "    32\n",
      "131072\n",
      "   512\n",
      "131072\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      "  1024\n",
      "    32\n",
      "131072\n",
      "   512\n",
      "131072\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      "  1024\n",
      "    32\n",
      "131072\n",
      "   512\n",
      "131072\n",
      "   256\n",
      "117248\n",
      "128000\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      "  1024\n",
      "    32\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      "  1024\n",
      "    32\n",
      "131072\n",
      "   512\n",
      "131072\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      "  1024\n",
      "    32\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      "  1024\n",
      "    32\n",
      "131072\n",
      "   512\n",
      "131072\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      "  1024\n",
      "    32\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      "  1024\n",
      "    32\n",
      "131072\n",
      "   512\n",
      "131072\n",
      "   256\n",
      "117248\n",
      "   458\n",
      "______\n",
      "4548842\n"
     ]
    }
   ],
   "source": [
    "#we can print the complexity by the number of parameters\n",
    "def count_parameters(model):\n",
    "    params = [p.numel() for p in model.parameters() if p.requires_grad]\n",
    "    for item in params:\n",
    "        print(f'{item:>6}')\n",
    "    print(f'______\\n{sum(params):>6}')\n",
    "    \n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "lr = 0.0005\n",
    "\n",
    "#training hyperparameters\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX) #combine softmax with cross entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we'll define our training loop. This is the exact same as the one used in the previous tutorial.\n",
    "\n",
    "As we want our model to predict the `<eos>` token but not have it be an input into our model we simply slice the `<eos>` token off the end of the sequence. Thus:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\text{trg} &= [sos, x_1, x_2, x_3, eos]\\\\\n",
    "\\text{trg[:-1]} &= [sos, x_1, x_2, x_3]\n",
    "\\end{align*}$$\n",
    "\n",
    "$x_i$ denotes actual target sequence element. We then feed this into the model to get a predicted sequence that should hopefully predict the `<eos>` token:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\text{output} &= [y_1, y_2, y_3, eos]\n",
    "\\end{align*}$$\n",
    "\n",
    "$y_i$ denotes predicted target sequence element. We then calculate our loss using the original `trg` tensor with the `<sos>` token sliced off the front, leaving the `<eos>` token:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\text{output} &= [y_1, y_2, y_3, eos]\\\\\n",
    "\\text{trg[1:]} &= [x_1, x_2, x_3, eos]\n",
    "\\end{align*}$$\n",
    "\n",
    "We then calculate our losses and update our parameters as is standard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, optimizer, criterion, clip, loader_length):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for src, src_len, trg in loader:\n",
    "        \n",
    "        src = src.to(device)\n",
    "        trg = trg.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #trg[:, :-1] remove the eos, e.g., \"<sos> I love sushi\" since teaching forcing, the input does not need to have eos\n",
    "        output, _ = model(src, trg[:,:-1])\n",
    "                \n",
    "        #output = [batch size, trg len - 1, output dim]\n",
    "        #trg    = [batch size, trg len]\n",
    "            \n",
    "        output_dim = output.shape[-1]\n",
    "            \n",
    "        output = output.reshape(-1, output_dim)\n",
    "        trg = trg[:,1:].reshape(-1) #trg[:, 1:] remove the sos, e.g., \"i love sushi <eos>\" since in teaching forcing, the output does not have sos\n",
    "                \n",
    "        #output = [batch size * trg len - 1, output dim]\n",
    "        #trg    = [batch size * trg len - 1]\n",
    "            \n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / loader_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our evaluation loop is similar to our training loop, however as we aren't updating any parameters we don't need to pass an optimizer or a clip value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader, criterion, loader_length):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for src, src_len, trg in loader:\n",
    "        \n",
    "            src = src.to(device)\n",
    "            trg = trg.to(device)\n",
    "\n",
    "            output, _ = model(src, trg[:,:-1])\n",
    "            \n",
    "            #output = [batch size, trg len - 1, output dim]\n",
    "            #trg = [batch size, trg len]\n",
    "            \n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            output = output.contiguous().view(-1, output_dim)\n",
    "            trg = trg[:,1:].contiguous().view(-1)\n",
    "            \n",
    "            #output = [batch size * trg len - 1, output dim]\n",
    "            #trg = [batch size * trg len - 1]\n",
    "            \n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / loader_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting everything together\n",
    "\n",
    "Finally, we train our actual model. This model is almost 3x faster than the convolutional sequence-to-sequence model and also achieves a lower validation perplexity!\n",
    "\n",
    "**Note: similar to CNN, this model always has a teacher forcing ratio of 1, i.e. it will always use the ground truth next token from the target sequence (this is simply because CNN do everything in parallel so we cannot have the next token). This means we cannot compare perplexity values against the previous models when they are using a teacher forcing ratio that is not 1. To understand this, try run previous tutorials with teaching forcing ratio of 1, you will get very low perplexity.  **   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_length = len(list(iter(train_loader)))\n",
    "val_loader_length   = len(list(iter(valid_loader)))\n",
    "test_loader_length  = len(list(iter(test_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### general #####\n",
      "Epoch: 01 | Time: 0m 5s\n",
      "\tTrain Loss: 6.313 | Train PPL: 551.560\n",
      "\t Val. Loss: 6.236 |  Val. PPL: 510.596\n",
      "##### scaled #####\n",
      "Epoch: 01 | Time: 0m 3s\n",
      "\tTrain Loss: 6.611 | Train PPL: 743.139\n",
      "\t Val. Loss: 6.713 |  Val. PPL: 823.300\n",
      "##### multiplicative #####\n",
      "Epoch: 01 | Time: 0m 2s\n",
      "\tTrain Loss: 6.481 | Train PPL: 652.477\n",
      "\t Val. Loss: 6.426 |  Val. PPL: 617.604\n",
      "##### additive #####\n",
      "Epoch: 01 | Time: 0m 15s\n",
      "\tTrain Loss: 6.151 | Train PPL: 469.011\n",
      "\t Val. Loss: 5.946 |  Val. PPL: 382.107\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 1\n",
    "clip       = 1\n",
    "\n",
    "for attn_variant in ['general', 'scaled', 'multiplicative', 'additive']:\n",
    "    enc = Encoder(input_dim,\n",
    "                  hid_dim, \n",
    "                  enc_layers, \n",
    "                  enc_heads, \n",
    "                  enc_pf_dim, \n",
    "                  enc_dropout, \n",
    "                  attn_variant, \n",
    "                  device)\n",
    "\n",
    "    dec = Decoder(output_dim, \n",
    "                  hid_dim, \n",
    "                  dec_layers, \n",
    "                  dec_heads, \n",
    "                  dec_pf_dim, \n",
    "                  enc_dropout, \n",
    "                  attn_variant, \n",
    "                  device)\n",
    "\n",
    "    model = Seq2SeqTransformer(enc, dec, SRC_PAD_IDX, TRG_PAD_IDX, device).to(device)\n",
    "    model.apply(initialize_weights)\n",
    "    save_path = f'models/{attn_variant}_{model.__class__.__name__}.pt'\n",
    "\n",
    "    best_valid_loss = float('inf')\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "\n",
    "    print(f'##### {attn_variant} #####')\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        start_time = time.time()\n",
    "\n",
    "        train_loss = train(model, train_loader, optimizer, criterion, clip, train_loader_length)\n",
    "        valid_loss = evaluate(model, valid_loader, criterion, val_loader_length)\n",
    "        \n",
    "        #for plotting\n",
    "        train_losses.append(train_loss)\n",
    "        valid_losses.append(valid_loss)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        \n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "        \n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "        \n",
    "        print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "        print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
    "        \n",
    "        #lower perplexity is better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'loss')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(5, 3))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.plot(train_losses, label = 'train loss')\n",
    "ax.plot(valid_losses, label = 'valid loss')\n",
    "plt.legend()\n",
    "ax.set_xlabel('updates')\n",
    "ax.set_ylabel('loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Test Loss: 5.961 | Test PPL: 388.155 |\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(save_path))\n",
    "test_loss = evaluate(model, test_loader, criterion, test_loader_length)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test on some random news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Did you get that thesis from the Dude?'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[0]['en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'คุณได้ข้อเสนอนี้ จากเพื่อนคนนั้นเหรอ?'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[0]['th']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2,  0,  6, 53, 15,  0, 64,  8,  0,  9,  3], device='cuda:0')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_text = text_transform[SRC_LANGUAGE](test[0]['en']).to(device)\n",
    "src_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  2,   8,  11,   0,  52,   4,  50, 167,  41,  40,  54,  19,   3],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trg_text = text_transform[TRG_LANGUAGE](test[0]['th']).to(device)\n",
    "trg_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_text = src_text.reshape(1, -1)  #because batch_size is 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "trg_text = trg_text.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 11]), torch.Size([1, 13]))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_text.shape, trg_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_length = torch.tensor([src_text.size(0)]).to(dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(save_path))\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output, attentions = model(src_text, trg_text) #turn off teacher forcing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 13, 458])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape #batch_size, trg_len, trg_output_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since batch size is 1, we just take off that dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = output.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([13, 458])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall remove the first token since it's zeroes anyway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 458])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = output[1:]\n",
    "output.shape #trg_len, trg_output_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we just take the top token with highest probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_max = output.argmax(1) #returns max indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([343,   0,   0,   0,   0, 178,   0, 178, 104,   0,   0,   0],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the mapping of the target language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = vocab_transform[TRG_LANGUAGE].get_itos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "กำลังจะ\n",
      "<unk>\n",
      "<unk>\n",
      "<unk>\n",
      "<unk>\n",
      "ริ\n",
      "<unk>\n",
      "ริ\n",
      "ที่นี่\n",
      "<unk>\n",
      "<unk>\n",
      "<unk>\n"
     ]
    }
   ],
   "source": [
    "for token in output_max:\n",
    "    print(mapping[token.item()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Attention\n",
    "\n",
    "Let's display the attentions to understand how the source text links with the generated text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 13, 11])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attentions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are 8 heads, we can look at just 1 head for sake of simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([13, 11])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention = attentions[0, 0, :, :]\n",
    "attention.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<sos>',\n",
       " 'Did',\n",
       " 'you',\n",
       " 'get',\n",
       " 'that',\n",
       " 'thesis',\n",
       " 'from',\n",
       " 'the',\n",
       " 'Dude',\n",
       " '?',\n",
       " '<eos>']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_tokens = ['<sos>'] + token_transform[SRC_LANGUAGE](test[0]['en']) + ['<eos>']\n",
    "src_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<sos>',\n",
       " 'กำลังจะ',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " 'ริ',\n",
       " '<unk>',\n",
       " 'ริ',\n",
       " 'ที่นี่',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trg_tokens = ['<sos>'] + [mapping[token.item()] for token in output_max]\n",
    "trg_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker as ticker\n",
    "\n",
    "def display_attention(sentence, translation, attention):\n",
    "    \n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(111)\n",
    "    \n",
    "    attention = attention.squeeze(1).cpu().detach().numpy()\n",
    "    \n",
    "    cax = ax.matshow(attention, cmap='bone')\n",
    "   \n",
    "    ax.tick_params(labelsize=10)\n",
    "    \n",
    "    y_ticks =  [''] + translation\n",
    "    x_ticks =  [''] + sentence \n",
    "     \n",
    "    ax.set_xticklabels(x_ticks, rotation=45)\n",
    "    ax.set_yticklabels(y_ticks)\n",
    "\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Neo\\AppData\\Local\\Temp\\ipykernel_32\\59549304.py:17: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_xticklabels(x_ticks, rotation=45)\n",
      "C:\\Users\\Neo\\AppData\\Local\\Temp\\ipykernel_32\\59549304.py:18: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_yticklabels(y_ticks)\n",
      "c:\\Users\\Neo\\anaconda3\\envs\\dsai\\lib\\site-packages\\IPython\\core\\pylabtools.py:152: UserWarning: Glyph 3585 (\\N{THAI CHARACTER KO KAI}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\Neo\\anaconda3\\envs\\dsai\\lib\\site-packages\\IPython\\core\\pylabtools.py:152: UserWarning: Glyph 3635 (\\N{THAI CHARACTER SARA AM}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\Neo\\anaconda3\\envs\\dsai\\lib\\site-packages\\IPython\\core\\pylabtools.py:152: UserWarning: Glyph 3621 (\\N{THAI CHARACTER LO LING}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\Neo\\anaconda3\\envs\\dsai\\lib\\site-packages\\IPython\\core\\pylabtools.py:152: UserWarning: Glyph 3633 (\\N{THAI CHARACTER MAI HAN-AKAT}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\Neo\\anaconda3\\envs\\dsai\\lib\\site-packages\\IPython\\core\\pylabtools.py:152: UserWarning: Glyph 3591 (\\N{THAI CHARACTER NGO NGU}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\Neo\\anaconda3\\envs\\dsai\\lib\\site-packages\\IPython\\core\\pylabtools.py:152: UserWarning: Glyph 3592 (\\N{THAI CHARACTER CHO CHAN}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\Neo\\anaconda3\\envs\\dsai\\lib\\site-packages\\IPython\\core\\pylabtools.py:152: UserWarning: Glyph 3632 (\\N{THAI CHARACTER SARA A}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\Neo\\anaconda3\\envs\\dsai\\lib\\site-packages\\IPython\\core\\pylabtools.py:152: UserWarning: Glyph 3619 (\\N{THAI CHARACTER RO RUA}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\Neo\\anaconda3\\envs\\dsai\\lib\\site-packages\\IPython\\core\\pylabtools.py:152: UserWarning: Glyph 3636 (\\N{THAI CHARACTER SARA I}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\Neo\\anaconda3\\envs\\dsai\\lib\\site-packages\\IPython\\core\\pylabtools.py:152: UserWarning: Glyph 3607 (\\N{THAI CHARACTER THO THAHAN}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\Neo\\anaconda3\\envs\\dsai\\lib\\site-packages\\IPython\\core\\pylabtools.py:152: UserWarning: Glyph 3637 (\\N{THAI CHARACTER SARA II}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\Neo\\anaconda3\\envs\\dsai\\lib\\site-packages\\IPython\\core\\pylabtools.py:152: UserWarning: Glyph 3656 (\\N{THAI CHARACTER MAI EK}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\Neo\\anaconda3\\envs\\dsai\\lib\\site-packages\\IPython\\core\\pylabtools.py:152: UserWarning: Glyph 3609 (\\N{THAI CHARACTER NO NU}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_attention(src_tokens, trg_tokens, attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "714d3f4db9a58ba7d2f2a9a4fffe577af3df8551aebd380095064812e2e0a6a4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
