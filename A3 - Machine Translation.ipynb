{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Translation + Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\anaconda3\\envs\\dsai\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch, torchdata, torchtext\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.optim as optim\n",
    "import random, math, time\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "#make our work comparable if restarted the kernel\n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.1+cu118'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.16.1+cpu'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchtext.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 1. ETL: Loading the dataset\n",
    "\n",
    "**Note**: Here I chose to translate English to German, simply it is easier for myself, since I don't understand German so it is difficult for me to imagine a sentence during inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "SRC_LANGUAGE = 'en'\n",
    "TRG_LANGUAGE = 'th'\n",
    "\n",
    "dataset = datasets.load_dataset(\"opus100\", \"en-th\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 1000000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import default_rng\n",
    "\n",
    "rng = default_rng(seed=SEED)\n",
    "# create a list of non-repeated indices of size 20000 and use it to select the training samples\n",
    "select_idx = rng.choice(len(dataset['train']), size=20000, replace=False)\n",
    "dataset['train'] = dataset['train'].filter(lambda example, idx: idx in select_idx, with_indices=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_lang_col = lambda example, lang: {lang: example['translation'][lang]}\n",
    "dataset = dataset.map(get_lang_col, fn_kwargs={'lang': \"th\"})\n",
    "dataset = dataset.map(get_lang_col, remove_columns=['translation'], fn_kwargs={'lang': \"en\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'th': 'พ่อเธอเคยบอกให้เธอหุบปากบ้างมั๊ยล่ะ?',\n",
       " 'en': 'Has your dad ever told you to shut up?'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['th', 'en'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['th', 'en'],\n",
       "        num_rows: 20000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['th', 'en'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, val_set, test_set = dataset['train'], dataset['validation'], dataset['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 3. Preprocessing \n",
    "\n",
    "### Tokenizing\n",
    "\n",
    "**Note**: the models must first be downloaded using the following on the command line: \n",
    "```\n",
    "python3 -m spacy download en_core_web_sm\n",
    "python3 -m spacy download de_core_news_sm\n",
    "```\n",
    "\n",
    "First, since we have two languages, let's create some constants to represent that.  Also, let's create two dicts: one for holding our tokenizers and one for holding all the vocabs with assigned numbers for each unique word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pythainlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place-holders\n",
    "token_transform = {}\n",
    "vocab_transform = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from pythainlp.tokenize import Tokenizer\n",
    "\n",
    "token_transform[SRC_LANGUAGE] = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "token_transform[TRG_LANGUAGE] = Tokenizer(engine='newmm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence:  พ่อเธอเคยบอกให้เธอหุบปากบ้างมั๊ยล่ะ?\n",
      "Tokenization:  ['พ่อ', 'เธอ', 'เคย', 'บอก', 'ให้', 'เธอ', 'หุบปาก', 'บ้าง', 'มั๊ย', 'ล่ะ', '?']\n"
     ]
    }
   ],
   "source": [
    "#example of tokenization of the thai part\n",
    "print(\"Sentence: \", dataset['train'][TRG_LANGUAGE][2])\n",
    "print(\"Tokenization: \", token_transform[TRG_LANGUAGE].word_tokenize(dataset['train'][TRG_LANGUAGE][2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_data(example, lang):\n",
    "    try:\n",
    "        return {lang: token_transform[lang](example[lang].lower())}\n",
    "    except:\n",
    "        return {lang: token_transform[lang].word_tokenize(example[lang].lower())}\n",
    "    \n",
    "tokenized_dataset = dataset.map(tokenize_data, remove_columns=[SRC_LANGUAGE], fn_kwargs={'lang': SRC_LANGUAGE})\n",
    "tokenized_dataset = tokenized_dataset.map(tokenize_data, remove_columns=[TRG_LANGUAGE], fn_kwargs={'lang': TRG_LANGUAGE})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define special symbols and indices\n",
    "UNK_IDX, PAD_IDX, SOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
    "special_symbols = ['<unk>', '<pad>', '<sos>', '<eos>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lang in [SRC_LANGUAGE, TRG_LANGUAGE]:\n",
    "    vocab_transform[lang] = torchtext.vocab.build_vocab_from_iterator(tokenized_dataset['train'][lang],\n",
    "                                                                      min_freq=3,   #if not, everything will be treated as UNK\n",
    "                                                                      specials=special_symbols,\n",
    "                                                                      special_first=True) #indicates whether to insert symbols at the beginning or at the end)\n",
    "    # Set UNK_IDX as the default index. This index is returned when the token is not found. \n",
    "    # If not set, it throws RuntimeError when the queried token is not found in the Vocabulary. \n",
    "    vocab_transform[lang].set_default_index(UNK_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save vocab\n",
    "torch.save(vocab_transform, './models/vocab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[36, 2573]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#see some example\n",
    "vocab_transform[SRC_LANGUAGE](['my', 'precious'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[13, 162, 13, 70]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#see some example\n",
    "vocab_transform[TRG_LANGUAGE](['ของ', 'รัก', 'ของ', 'ข้า'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ของ'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we can reverse it....\n",
    "mapping = vocab_transform[TRG_LANGUAGE].get_itos()\n",
    "\n",
    "#print 1816, for example\n",
    "mapping[13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<unk>'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's try unknown vocab\n",
    "mapping[0]\n",
    "#they will all map to <unk> which has 0 as integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<pad>', '<sos>', '<eos>')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's try special symbols\n",
    "mapping[1], mapping[2], mapping[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4243"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check unique vocabularies\n",
    "len(mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4. Preparing the dataloader\n",
    "\n",
    "One thing we change here is the <code>collate_fn</code> which now also returns the length of sentence.  This is required for <code>packed_padded_sequence</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# helper function to club together sequential operations\n",
    "def sequential_transforms(*transforms):\n",
    "    def func(txt_input):\n",
    "        for transform in transforms:\n",
    "            try:\n",
    "                txt_input = transform(txt_input)\n",
    "            except TypeError:\n",
    "                txt_input = transform.word_tokenize(txt_input)\n",
    "        return txt_input\n",
    "    return func\n",
    "\n",
    "# function to add BOS/EOS and create tensor for input sequence indices\n",
    "def tensor_transform(token_ids):\n",
    "    return torch.cat((torch.tensor([SOS_IDX]), \n",
    "                      torch.tensor(token_ids), \n",
    "                      torch.tensor([EOS_IDX])))\n",
    "\n",
    "# src and trg language text transforms to convert raw strings into tensors indices\n",
    "text_transform = {}\n",
    "for ln in [SRC_LANGUAGE, TRG_LANGUAGE]:\n",
    "    text_transform[ln] = sequential_transforms(token_transform[ln], #Tokenization\n",
    "                                               vocab_transform[ln], #Numericalization\n",
    "                                               tensor_transform) # Add BOS/EOS and create tensor\n",
    "\n",
    "\n",
    "# function to collate data samples into batch tesors\n",
    "def collate_batch(batch):\n",
    "    src_batch, src_len_batch, trg_batch = [], [], []\n",
    "    for sample in batch:\n",
    "        processed_text = text_transform[SRC_LANGUAGE](sample[SRC_LANGUAGE].rstrip(\"\\n\"))\n",
    "        src_batch.append(processed_text)\n",
    "        trg_batch.append(text_transform[TRG_LANGUAGE](sample[TRG_LANGUAGE].rstrip(\"\\n\")))\n",
    "        src_len_batch.append(processed_text.size(0))\n",
    "\n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX, batch_first=True)\n",
    "    trg_batch = pad_sequence(trg_batch, padding_value=PAD_IDX, batch_first=True)\n",
    "    return src_batch, torch.tensor(src_len_batch, dtype=torch.int64), trg_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create train, val, and test dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
    "valid_loader = DataLoader(val_set,   batch_size=batch_size, shuffle=False, collate_fn=collate_batch)\n",
    "test_loader  = DataLoader(test_set,  batch_size=batch_size, shuffle=False, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the train loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for en, _, th in train_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English shape:  torch.Size([64, 36])\n",
      "Thai shape:  torch.Size([64, 45])\n"
     ]
    }
   ],
   "source": [
    "print(\"English shape: \", en.shape)  # (batch_size, seq len)\n",
    "print(\"Thai shape: \", th.shape)   # (batch_size, seq len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Design the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, pf_dim, dropout, attn_variant, device):\n",
    "        super().__init__()\n",
    "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.ff_layer_norm        = nn.LayerNorm(hid_dim)\n",
    "        self.self_attention       = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, attn_variant, device)\n",
    "        self.feedforward          = PositionwiseFeedforwardLayer(hid_dim, pf_dim, dropout)\n",
    "        self.dropout              = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src, src_mask):\n",
    "        #src = [batch size, src len, hid dim]\n",
    "        #src_mask = [batch size, 1, 1, src len]   #if the token is padding, it will be 1, otherwise 0\n",
    "        _src, _ = self.self_attention(src, src, src, src_mask)\n",
    "        src     = self.self_attn_layer_norm(src + self.dropout(_src))\n",
    "        #src: [batch_size, src len, hid dim]\n",
    "        \n",
    "        _src    = self.feedforward(src)\n",
    "        src     = self.ff_layer_norm(src + self.dropout(_src))\n",
    "        #src: [batch_size, src len, hid dim]\n",
    "        \n",
    "        return src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hid_dim, n_layers, n_heads, pf_dim, dropout, attn_variant, device, max_length = 500):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.attn_variant = attn_variant\n",
    "        self.tok_embedding = nn.Embedding(input_dim, hid_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
    "        self.layers        = nn.ModuleList([EncoderLayer(hid_dim, n_heads, pf_dim, dropout, attn_variant, device)\n",
    "                                           for _ in range(n_layers)])\n",
    "        self.dropout       = nn.Dropout(dropout)\n",
    "        self.scale         = torch.sqrt(torch.FloatTensor([hid_dim])).to(self.device)\n",
    "        \n",
    "    def forward(self, src, src_mask):\n",
    "        \n",
    "        #src = [batch size, src len]\n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "        \n",
    "        batch_size = src.shape[0]\n",
    "        src_len    = src.shape[1]\n",
    "        \n",
    "        pos        = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
    "        #pos: [batch_size, src_len]\n",
    "        \n",
    "        src        = self.dropout((self.tok_embedding(src) * self.scale) + self.pos_embedding(pos))\n",
    "        #src: [batch_size, src_len, hid_dim]\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            src = layer(src, src_mask)\n",
    "        #src: [batch_size, src_len, hid_dim]\n",
    "        \n",
    "        return src\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mutli Head Attention Layer\n",
    "\n",
    "$$ \\text{Attention}(Q, K, V) = \\text{Softmax} \\big( \\frac{QK^T}{\\sqrt{d_k}} \\big)V $$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default Transformer attention variant\n",
    "class ScaledAttention(nn.Module):\n",
    "    def __init__(self, head_dim):\n",
    "        super().__init__()\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([head_dim])).to(device)\n",
    "\n",
    "    def forward(self, Q, K):\n",
    "        scores = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n",
    "        # scores: [batch_size, n_heads, query len, key len]\n",
    "\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiplicativeAttention(nn.Module):\n",
    "    def __init__(self, head_dim):\n",
    "        super().__init__()\n",
    "        self.W1 = nn.Linear(head_dim, head_dim)\n",
    "\n",
    "    def forward(self, Q, K):\n",
    "        scores = torch.matmul(self.W1(Q), K.permute(0, 1, 3, 2))\n",
    "        # scores: [batch_size, n_heads, query len, key len]\n",
    "\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdditiveAttention(nn.Module):\n",
    "    def __init__(self, head_dim):\n",
    "        super().__init__()\n",
    "        self.W1 = nn.Linear(head_dim, head_dim)\n",
    "        self.W2 = nn.Linear(head_dim, head_dim)\n",
    "        self.V = nn.Linear(head_dim, 1)\n",
    "\n",
    "    def forward(self, Q, K):\n",
    "        Q = Q.unsqueeze(3)  # Q: [batch_size, n_heads, query len, head_dim] => [batch_size, n_heads, query len, 1, head_dim]\n",
    "        K = K.unsqueeze(2)  # Q: [batch_size, n_heads, key len, head_dim] => [batch_size, n_heads, 1, key len, head_dim]\n",
    "        features = torch.tanh(self.W1(Q) + self.W2(K))\n",
    "        # features: [batch_size, n_heads, query len, key len, head_dim]\n",
    "\n",
    "        scores = self.V(features).squeeze(-1)\n",
    "        # scores: [batch_size, n_heads, query len, key len]\n",
    "\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, dropout, attn_variant, device):\n",
    "        super().__init__()\n",
    "        assert hid_dim % n_heads == 0\n",
    "        self.hid_dim  = hid_dim\n",
    "        self.n_heads  = n_heads\n",
    "        self.head_dim = hid_dim // n_heads\n",
    "        self.attn_variant = attn_variant\n",
    "        \n",
    "        self.fc_q     = nn.Linear(hid_dim, hid_dim)\n",
    "        self.fc_k     = nn.Linear(hid_dim, hid_dim)\n",
    "        self.fc_v     = nn.Linear(hid_dim, hid_dim)\n",
    "        \n",
    "        self.fc_o     = nn.Linear(hid_dim, hid_dim)\n",
    "        \n",
    "        self.dropout  = nn.Dropout(dropout)\n",
    "\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
    "\n",
    "        if attn_variant == 'scaled':\n",
    "            self.scaled_attention = ScaledAttention(self.head_dim)\n",
    "\n",
    "        elif attn_variant == 'multiplicative':\n",
    "            self.multiplicative_attention = MultiplicativeAttention(self.head_dim)\n",
    "        \n",
    "        elif attn_variant == 'additive':\n",
    "            self.additive_attention = AdditiveAttention(self.head_dim)\n",
    "                \n",
    "    def forward(self, query, key, value, mask = None):\n",
    "        #src, src, src, src_mask\n",
    "        #query = [batch size, query len, hid dim]\n",
    "        #key = [batch size, key len, hid dim]\n",
    "        #value = [batch size, value len, hid dim]\n",
    "        \n",
    "        batch_size = query.shape[0]\n",
    "        \n",
    "        Q = self.fc_q(query)\n",
    "        K = self.fc_k(key)\n",
    "        V = self.fc_v(value)\n",
    "        # Q: [batch_size, seq len, hid_dim]\n",
    "        \n",
    "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        # Q=K=V: [batch_size, n heads, seq len, head_dim]\n",
    "        \n",
    "        # energy = self.additive_attention(Q, K)\n",
    "        if self.attn_variant == \"general\":\n",
    "            energy = torch.matmul(Q, K.permute(0, 1, 3, 2))\n",
    "\n",
    "        elif self.attn_variant == \"scaled\":\n",
    "            energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n",
    "            # energy = self.scaled_attention(Q, K)\n",
    "\n",
    "        elif self.attn_variant == \"multiplicative\":\n",
    "            energy = self.multiplicative_attention(Q, K)\n",
    "\n",
    "        elif self.attn_variant == \"additive\":\n",
    "            energy = self.additive_attention(Q, K)\n",
    "            \n",
    "        else:\n",
    "            raise Exception(\"Incorrect value for attention variant. Must be one of the following: \\\n",
    "                            scaled, general, multiplicative, additive\")\n",
    "        \n",
    "        #for making attention to padding to 0\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, -1e10)\n",
    "            \n",
    "        attention = torch.softmax(energy, dim = -1)\n",
    "        #attention = [batch_size, n heads, query len, key len]\n",
    "\n",
    "        x = torch.matmul(self.dropout(attention), V)\n",
    "        #[batch_size, n heads, query len, key len] @ [batch_size, n heads, value len, head_dim]\n",
    "        #x = [batch_size, n heads, query len, head dim]\n",
    "        \n",
    "        x = x.permute(0, 2, 1, 3).contiguous()  #we can perform .view\n",
    "        #x = [batch_size, query len, n heads, head dim]\n",
    "        \n",
    "        x = x.view(batch_size, -1, self.hid_dim)\n",
    "        #x = [batch_size, query len, hid dim]\n",
    "        \n",
    "        x = self.fc_o(x)\n",
    "        #x = [batch_size, query len, hid dim]\n",
    "        \n",
    "        return x, attention\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Position-wise Feedforward Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedforwardLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, pf_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(hid_dim, pf_dim)\n",
    "        self.fc2 = nn.Linear(pf_dim, hid_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #x = [batch size, src len, hid dim]\n",
    "        x = self.dropout(torch.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, pf_dim, dropout, attn_variant, device):\n",
    "        super().__init__()\n",
    "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.enc_attn_layer_norm  = nn.LayerNorm(hid_dim)\n",
    "        self.ff_layer_norm        = nn.LayerNorm(hid_dim)\n",
    "        self.self_attention       = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, attn_variant, device)\n",
    "        self.encoder_attention    = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, attn_variant, device)\n",
    "        self.feedforward          = PositionwiseFeedforwardLayer(hid_dim, pf_dim, dropout)\n",
    "        self.dropout              = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "        \n",
    "        #trg = [batch size, trg len, hid dim]\n",
    "        #enc_src = [batch size, src len, hid dim]\n",
    "        #trg_mask = [batch size, 1, trg len, trg len]\n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "        \n",
    "        _trg, _ = self.self_attention(trg, trg, trg, trg_mask)\n",
    "        trg     = self.self_attn_layer_norm(trg + self.dropout(_trg))\n",
    "        #trg = [batch_size, trg len, hid dim]\n",
    "        \n",
    "        _trg, attention = self.encoder_attention(trg, enc_src, enc_src, src_mask)\n",
    "        trg             = self.enc_attn_layer_norm(trg + self.dropout(_trg))\n",
    "        #trg = [batch_size, trg len, hid dim]\n",
    "        #attention = [batch_size, n heads, trg len, src len]\n",
    "        \n",
    "        _trg = self.feedforward(trg)\n",
    "        trg  = self.ff_layer_norm(trg + self.dropout(_trg))\n",
    "        #trg = [batch_size, trg len, hid dim]\n",
    "        \n",
    "        return trg, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, hid_dim, n_layers, n_heads, \n",
    "                 pf_dim, dropout, attn_variant, device, max_length = 500):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.tok_embedding = nn.Embedding(output_dim, hid_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
    "        self.layers        = nn.ModuleList([DecoderLayer(hid_dim, n_heads, pf_dim, dropout, attn_variant, device)\n",
    "                                            for _ in range(n_layers)])\n",
    "        self.fc_out        = nn.Linear(hid_dim, output_dim)\n",
    "        self.dropout       = nn.Dropout(dropout)\n",
    "        self.scale         = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
    "        \n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "        \n",
    "        #trg = [batch size, trg len]\n",
    "        #enc_src = [batch size, src len, hid dim]\n",
    "        #trg_mask = [batch size, 1, trg len, trg len]\n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "        \n",
    "        batch_size = trg.shape[0]\n",
    "        trg_len    = trg.shape[1]\n",
    "        \n",
    "        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
    "        #pos: [batch_size, trg len]\n",
    "        \n",
    "        trg = self.dropout((self.tok_embedding(trg) * self.scale) + self.pos_embedding(pos))\n",
    "        #trg: [batch_size, trg len, hid dim]\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            trg, attention = layer(trg, enc_src, trg_mask, src_mask)\n",
    "            \n",
    "        #trg: [batch_size, trg len, hid dim]\n",
    "        #attention: [batch_size, n heads, trg len, src len]\n",
    "        \n",
    "        output = self.fc_out(trg)\n",
    "        #output = [batch_size, trg len, output_dim]\n",
    "        \n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting them together (become Seq2Seq!)\n",
    "\n",
    "Our `trg_sub_mask` will look something like this (for a target with 5 tokens):\n",
    "\n",
    "$$\\begin{matrix}\n",
    "1 & 0 & 0 & 0 & 0\\\\\n",
    "1 & 1 & 0 & 0 & 0\\\\\n",
    "1 & 1 & 1 & 0 & 0\\\\\n",
    "1 & 1 & 1 & 1 & 0\\\\\n",
    "1 & 1 & 1 & 1 & 1\\\\\n",
    "\\end{matrix}$$\n",
    "\n",
    "The \"subsequent\" mask is then logically anded with the padding mask, this combines the two masks ensuring both the subsequent tokens and the padding tokens cannot be attended to. For example if the last two tokens were `<pad>` tokens the mask would look like:\n",
    "\n",
    "$$\\begin{matrix}\n",
    "1 & 0 & 0 & 0 & 0\\\\\n",
    "1 & 1 & 0 & 0 & 0\\\\\n",
    "1 & 1 & 1 & 0 & 0\\\\\n",
    "1 & 1 & 1 & 0 & 0\\\\\n",
    "1 & 1 & 1 & 0 & 0\\\\\n",
    "\\end{matrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(self, encoder, decoder, src_pad_idx, trg_pad_idx, device):\n",
    "        super().__init__()\n",
    "        # store the input parameters so we can retrive them later for model inferencing\n",
    "        self.params = {'encoder': encoder, 'decoder': decoder,\n",
    "                       'src_pad_idx': src_pad_idx, 'trg_pad_idx': trg_pad_idx}\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        self.device = device\n",
    "        \n",
    "    def make_src_mask(self, src):\n",
    "        \n",
    "        #src = [batch size, src len]\n",
    "        \n",
    "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "\n",
    "        return src_mask\n",
    "    \n",
    "    def make_trg_mask(self, trg):\n",
    "        \n",
    "        #trg = [batch size, trg len]\n",
    "        \n",
    "        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        #trg_pad_mask = [batch size, 1, 1, trg len]\n",
    "        \n",
    "        trg_len = trg.shape[1]\n",
    "        \n",
    "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device = self.device)).bool()\n",
    "        #trg_sub_mask = [trg len, trg len]\n",
    "            \n",
    "        trg_mask = trg_pad_mask & trg_sub_mask\n",
    "        #trg_mask = [batch size, 1, trg len, trg len]\n",
    "        \n",
    "        return trg_mask\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        \n",
    "        #src = [batch size, src len]\n",
    "        #trg = [batch size, trg len]\n",
    "                \n",
    "        src_mask = self.make_src_mask(src)\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "        \n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "        #trg_mask = [batch size, 1, trg len, trg len]\n",
    "        \n",
    "        enc_src = self.encoder(src, src_mask)\n",
    "        #enc_src = [batch size, src len, hid dim]\n",
    "                \n",
    "        output, attention = self.decoder(trg, enc_src, trg_mask, src_mask)\n",
    "        \n",
    "        #output = [batch size, trg len, output dim]\n",
    "        #attention = [batch size, n heads, trg len, src len]\n",
    "        \n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(m):\n",
    "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
    "        nn.init.xavier_uniform_(m.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2SeqTransformer(\n",
       "  (encoder): Encoder(\n",
       "    (tok_embedding): Embedding(3666, 256)\n",
       "    (pos_embedding): Embedding(500, 256)\n",
       "    (layers): ModuleList(\n",
       "      (0-2): 3 x EncoderLayer(\n",
       "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (additive_attention): AdditiveAttention(\n",
       "            (W1): Linear(in_features=32, out_features=32, bias=True)\n",
       "            (W2): Linear(in_features=32, out_features=32, bias=True)\n",
       "            (V): Linear(in_features=32, out_features=1, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (feedforward): PositionwiseFeedforwardLayer(\n",
       "          (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (tok_embedding): Embedding(4243, 256)\n",
       "    (pos_embedding): Embedding(500, 256)\n",
       "    (layers): ModuleList(\n",
       "      (0-2): 3 x DecoderLayer(\n",
       "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (enc_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (additive_attention): AdditiveAttention(\n",
       "            (W1): Linear(in_features=32, out_features=32, bias=True)\n",
       "            (W2): Linear(in_features=32, out_features=32, bias=True)\n",
       "            (V): Linear(in_features=32, out_features=1, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (encoder_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (additive_attention): AdditiveAttention(\n",
       "            (W1): Linear(in_features=32, out_features=32, bias=True)\n",
       "            (W2): Linear(in_features=32, out_features=32, bias=True)\n",
       "            (V): Linear(in_features=32, out_features=1, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (feedforward): PositionwiseFeedforwardLayer(\n",
       "          (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (fc_out): Linear(in_features=256, out_features=4243, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dim   = len(vocab_transform[SRC_LANGUAGE])\n",
    "output_dim  = len(vocab_transform[TRG_LANGUAGE])\n",
    "hid_dim = 256\n",
    "enc_layers = 3\n",
    "dec_layers = 3\n",
    "enc_heads = 8\n",
    "dec_heads = 8\n",
    "enc_pf_dim = 512\n",
    "dec_pf_dim = 512\n",
    "enc_dropout = 0.1\n",
    "dec_dropout = 0.1\n",
    "attn_variant = 'additive'\n",
    "\n",
    "SRC_PAD_IDX = PAD_IDX\n",
    "TRG_PAD_IDX = PAD_IDX\n",
    "\n",
    "enc = Encoder(input_dim, \n",
    "              hid_dim, \n",
    "              enc_layers, \n",
    "              enc_heads, \n",
    "              enc_pf_dim, \n",
    "              enc_dropout, \n",
    "              attn_variant,\n",
    "              device)\n",
    "\n",
    "dec = Decoder(output_dim, \n",
    "              hid_dim, \n",
    "              dec_layers, \n",
    "              dec_heads, \n",
    "              dec_pf_dim, \n",
    "              enc_dropout, \n",
    "              attn_variant,\n",
    "              device)\n",
    "\n",
    "model = Seq2SeqTransformer(enc, dec, SRC_PAD_IDX, TRG_PAD_IDX, device).to(device)\n",
    "model.apply(initialize_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938496\n",
      "128000\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      "  1024\n",
      "    32\n",
      "  1024\n",
      "    32\n",
      "    32\n",
      "     1\n",
      "131072\n",
      "   512\n",
      "131072\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      "  1024\n",
      "    32\n",
      "  1024\n",
      "    32\n",
      "    32\n",
      "     1\n",
      "131072\n",
      "   512\n",
      "131072\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      "  1024\n",
      "    32\n",
      "  1024\n",
      "    32\n",
      "    32\n",
      "     1\n",
      "131072\n",
      "   512\n",
      "131072\n",
      "   256\n",
      "1086208\n",
      "128000\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      "  1024\n",
      "    32\n",
      "  1024\n",
      "    32\n",
      "    32\n",
      "     1\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      "  1024\n",
      "    32\n",
      "  1024\n",
      "    32\n",
      "    32\n",
      "     1\n",
      "131072\n",
      "   512\n",
      "131072\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      "  1024\n",
      "    32\n",
      "  1024\n",
      "    32\n",
      "    32\n",
      "     1\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      "  1024\n",
      "    32\n",
      "  1024\n",
      "    32\n",
      "    32\n",
      "     1\n",
      "131072\n",
      "   512\n",
      "131072\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      "  1024\n",
      "    32\n",
      "  1024\n",
      "    32\n",
      "    32\n",
      "     1\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      "  1024\n",
      "    32\n",
      "  1024\n",
      "    32\n",
      "    32\n",
      "     1\n",
      "131072\n",
      "   512\n",
      "131072\n",
      "   256\n",
      "1086208\n",
      "  4243\n",
      "______\n",
      "7344124\n"
     ]
    }
   ],
   "source": [
    "#we can print the complexity by the number of parameters\n",
    "def count_parameters(model):\n",
    "    params = [p.numel() for p in model.parameters() if p.requires_grad]\n",
    "    for item in params:\n",
    "        print(f'{item:>6}')\n",
    "    print(f'______\\n{sum(params):>6}')\n",
    "    \n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we'll define our training loop. This is the exact same as the one used in the previous tutorial.\n",
    "\n",
    "As we want our model to predict the `<eos>` token but not have it be an input into our model we simply slice the `<eos>` token off the end of the sequence. Thus:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\text{trg} &= [sos, x_1, x_2, x_3, eos]\\\\\n",
    "\\text{trg[:-1]} &= [sos, x_1, x_2, x_3]\n",
    "\\end{align*}$$\n",
    "\n",
    "$x_i$ denotes actual target sequence element. We then feed this into the model to get a predicted sequence that should hopefully predict the `<eos>` token:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\text{output} &= [y_1, y_2, y_3, eos]\n",
    "\\end{align*}$$\n",
    "\n",
    "$y_i$ denotes predicted target sequence element. We then calculate our loss using the original `trg` tensor with the `<sos>` token sliced off the front, leaving the `<eos>` token:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\text{output} &= [y_1, y_2, y_3, eos]\\\\\n",
    "\\text{trg[1:]} &= [x_1, x_2, x_3, eos]\n",
    "\\end{align*}$$\n",
    "\n",
    "We then calculate our losses and update our parameters as is standard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, optimizer, criterion, clip, loader_length):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for src, src_len, trg in loader:\n",
    "        \n",
    "        src = src.to(device)\n",
    "        trg = trg.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #trg[:, :-1] remove the eos, e.g., \"<sos> I love sushi\" since teaching forcing, the input does not need to have eos\n",
    "        output, _ = model(src, trg[:,:-1])\n",
    "                \n",
    "        #output = [batch size, trg len - 1, output dim]\n",
    "        #trg    = [batch size, trg len]\n",
    "            \n",
    "        output_dim = output.shape[-1]\n",
    "            \n",
    "        output = output.reshape(-1, output_dim)\n",
    "        trg = trg[:,1:].reshape(-1) #trg[:, 1:] remove the sos, e.g., \"i love sushi <eos>\" since in teaching forcing, the output does not have sos\n",
    "                \n",
    "        #output = [batch size * trg len - 1, output dim]\n",
    "        #trg    = [batch size * trg len - 1]\n",
    "            \n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / loader_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our evaluation loop is similar to our training loop, however as we aren't updating any parameters we don't need to pass an optimizer or a clip value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader, criterion, loader_length):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for src, src_len, trg in loader:\n",
    "        \n",
    "            src = src.to(device)\n",
    "            trg = trg.to(device)\n",
    "\n",
    "            output, _ = model(src, trg[:,:-1])\n",
    "            \n",
    "            #output = [batch size, trg len - 1, output dim]\n",
    "            #trg = [batch size, trg len]\n",
    "            \n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            output = output.contiguous().view(-1, output_dim)\n",
    "            trg = trg[:,1:].contiguous().view(-1)\n",
    "            \n",
    "            #output = [batch size * trg len - 1, output dim]\n",
    "            #trg = [batch size * trg len - 1]\n",
    "            \n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / loader_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting everything together\n",
    "\n",
    "Finally, we train our actual model. This model is almost 3x faster than the convolutional sequence-to-sequence model and also achieves a lower validation perplexity!\n",
    "\n",
    "**Note: similar to CNN, this model always has a teacher forcing ratio of 1, i.e. it will always use the ground truth next token from the target sequence (this is simply because CNN do everything in parallel so we cannot have the next token). This means we cannot compare perplexity values against the previous models when they are using a teacher forcing ratio that is not 1. To understand this, try run previous tutorials with teaching forcing ratio of 1, you will get very low perplexity.  **   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== additive =====\n",
      "Epoch: 01 | Time: 3m 53s\n",
      "\tTrain Loss: 5.314 | Train PPL: 203.219\n",
      "\t Val. Loss: 4.762 |  Val. PPL: 116.950\n",
      "Epoch: 02 | Time: 6m 6s\n",
      "\tTrain Loss: 4.574 | Train PPL:  96.924\n",
      "\t Val. Loss: 4.500 |  Val. PPL:  90.016\n",
      "Epoch: 03 | Time: 7m 10s\n",
      "\tTrain Loss: 4.237 | Train PPL:  69.183\n",
      "\t Val. Loss: 4.386 |  Val. PPL:  80.308\n",
      "Epoch: 04 | Time: 4m 55s\n",
      "\tTrain Loss: 3.966 | Train PPL:  52.796\n",
      "\t Val. Loss: 4.312 |  Val. PPL:  74.589\n",
      "Epoch: 05 | Time: 3m 50s\n",
      "\tTrain Loss: 3.729 | Train PPL:  41.634\n",
      "\t Val. Loss: 4.295 |  Val. PPL:  73.297\n",
      "Epoch: 06 | Time: 4m 14s\n",
      "\tTrain Loss: 3.504 | Train PPL:  33.255\n",
      "\t Val. Loss: 4.304 |  Val. PPL:  73.965\n",
      "Epoch: 07 | Time: 4m 21s\n",
      "\tTrain Loss: 3.298 | Train PPL:  27.052\n",
      "\t Val. Loss: 4.324 |  Val. PPL:  75.474\n",
      "Epoch: 08 | Time: 4m 2s\n",
      "\tTrain Loss: 3.104 | Train PPL:  22.288\n",
      "\t Val. Loss: 4.358 |  Val. PPL:  78.117\n",
      "Epoch: 09 | Time: 3m 41s\n",
      "\tTrain Loss: 2.918 | Train PPL:  18.507\n",
      "\t Val. Loss: 4.432 |  Val. PPL:  84.097\n",
      "Epoch: 10 | Time: 3m 44s\n",
      "\tTrain Loss: 2.741 | Train PPL:  15.498\n",
      "\t Val. Loss: 4.508 |  Val. PPL:  90.753\n",
      "\n",
      "===== multiplicative =====\n",
      "Epoch: 01 | Time: 0m 10s\n",
      "\tTrain Loss: 5.539 | Train PPL: 254.342\n",
      "\t Val. Loss: 5.122 |  Val. PPL: 167.710\n",
      "Epoch: 02 | Time: 0m 10s\n",
      "\tTrain Loss: 5.024 | Train PPL: 152.051\n",
      "\t Val. Loss: 4.922 |  Val. PPL: 137.228\n",
      "Epoch: 03 | Time: 0m 10s\n",
      "\tTrain Loss: 4.773 | Train PPL: 118.246\n",
      "\t Val. Loss: 4.758 |  Val. PPL: 116.567\n",
      "Epoch: 04 | Time: 0m 10s\n",
      "\tTrain Loss: 4.598 | Train PPL:  99.314\n",
      "\t Val. Loss: 4.701 |  Val. PPL: 110.036\n",
      "Epoch: 05 | Time: 0m 10s\n",
      "\tTrain Loss: 4.464 | Train PPL:  86.822\n",
      "\t Val. Loss: 4.623 |  Val. PPL: 101.773\n",
      "Epoch: 06 | Time: 0m 10s\n",
      "\tTrain Loss: 4.361 | Train PPL:  78.362\n",
      "\t Val. Loss: 4.587 |  Val. PPL:  98.183\n",
      "Epoch: 07 | Time: 0m 10s\n",
      "\tTrain Loss: 4.267 | Train PPL:  71.295\n",
      "\t Val. Loss: 4.563 |  Val. PPL:  95.896\n",
      "Epoch: 08 | Time: 0m 10s\n",
      "\tTrain Loss: 4.188 | Train PPL:  65.859\n",
      "\t Val. Loss: 4.549 |  Val. PPL:  94.529\n",
      "Epoch: 09 | Time: 0m 10s\n",
      "\tTrain Loss: 4.112 | Train PPL:  61.038\n",
      "\t Val. Loss: 4.564 |  Val. PPL:  95.949\n",
      "Epoch: 10 | Time: 0m 10s\n",
      "\tTrain Loss: 4.046 | Train PPL:  57.158\n",
      "\t Val. Loss: 4.526 |  Val. PPL:  92.366\n",
      "\n",
      "===== general =====\n",
      "Epoch: 01 | Time: 0m 10s\n",
      "\tTrain Loss: 5.592 | Train PPL: 268.262\n",
      "\t Val. Loss: 5.186 |  Val. PPL: 178.797\n",
      "Epoch: 02 | Time: 0m 10s\n",
      "\tTrain Loss: 5.105 | Train PPL: 164.789\n",
      "\t Val. Loss: 4.995 |  Val. PPL: 147.718\n",
      "Epoch: 03 | Time: 0m 10s\n",
      "\tTrain Loss: 4.864 | Train PPL: 129.566\n",
      "\t Val. Loss: 4.844 |  Val. PPL: 127.021\n",
      "Epoch: 04 | Time: 0m 9s\n",
      "\tTrain Loss: 4.665 | Train PPL: 106.144\n",
      "\t Val. Loss: 4.732 |  Val. PPL: 113.532\n",
      "Epoch: 05 | Time: 0m 10s\n",
      "\tTrain Loss: 4.519 | Train PPL:  91.762\n",
      "\t Val. Loss: 4.657 |  Val. PPL: 105.332\n",
      "Epoch: 06 | Time: 0m 9s\n",
      "\tTrain Loss: 4.396 | Train PPL:  81.112\n",
      "\t Val. Loss: 4.623 |  Val. PPL: 101.767\n",
      "Epoch: 07 | Time: 0m 10s\n",
      "\tTrain Loss: 4.288 | Train PPL:  72.798\n",
      "\t Val. Loss: 4.576 |  Val. PPL:  97.084\n",
      "Epoch: 08 | Time: 0m 10s\n",
      "\tTrain Loss: 4.194 | Train PPL:  66.262\n",
      "\t Val. Loss: 4.569 |  Val. PPL:  96.467\n",
      "Epoch: 09 | Time: 0m 9s\n",
      "\tTrain Loss: 4.108 | Train PPL:  60.806\n",
      "\t Val. Loss: 4.559 |  Val. PPL:  95.458\n",
      "Epoch: 10 | Time: 0m 10s\n",
      "\tTrain Loss: 4.031 | Train PPL:  56.311\n",
      "\t Val. Loss: 4.554 |  Val. PPL:  95.053\n",
      "\n",
      "===== scaled =====\n",
      "Epoch: 01 | Time: 0m 10s\n",
      "\tTrain Loss: 5.331 | Train PPL: 206.653\n",
      "\t Val. Loss: 4.789 |  Val. PPL: 120.227\n",
      "Epoch: 02 | Time: 0m 10s\n",
      "\tTrain Loss: 4.623 | Train PPL: 101.749\n",
      "\t Val. Loss: 4.533 |  Val. PPL:  93.015\n",
      "Epoch: 03 | Time: 0m 10s\n",
      "\tTrain Loss: 4.289 | Train PPL:  72.860\n",
      "\t Val. Loss: 4.376 |  Val. PPL:  79.482\n",
      "Epoch: 04 | Time: 0m 10s\n",
      "\tTrain Loss: 4.026 | Train PPL:  56.041\n",
      "\t Val. Loss: 4.311 |  Val. PPL:  74.519\n",
      "Epoch: 05 | Time: 0m 9s\n",
      "\tTrain Loss: 3.792 | Train PPL:  44.361\n",
      "\t Val. Loss: 4.283 |  Val. PPL:  72.486\n",
      "Epoch: 06 | Time: 0m 10s\n",
      "\tTrain Loss: 3.572 | Train PPL:  35.585\n",
      "\t Val. Loss: 4.253 |  Val. PPL:  70.335\n",
      "Epoch: 07 | Time: 0m 10s\n",
      "\tTrain Loss: 3.364 | Train PPL:  28.896\n",
      "\t Val. Loss: 4.278 |  Val. PPL:  72.087\n",
      "Epoch: 08 | Time: 0m 10s\n",
      "\tTrain Loss: 3.165 | Train PPL:  23.685\n",
      "\t Val. Loss: 4.291 |  Val. PPL:  73.035\n",
      "Epoch: 09 | Time: 0m 9s\n",
      "\tTrain Loss: 2.978 | Train PPL:  19.652\n",
      "\t Val. Loss: 4.365 |  Val. PPL:  78.683\n",
      "Epoch: 10 | Time: 0m 10s\n",
      "\tTrain Loss: 2.797 | Train PPL:  16.404\n",
      "\t Val. Loss: 4.393 |  Val. PPL:  80.860\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "input_dim   = len(vocab_transform[SRC_LANGUAGE])\n",
    "output_dim  = len(vocab_transform[TRG_LANGUAGE])\n",
    "\n",
    "batch_size = 64\n",
    "lr = 0.0005\n",
    "hid_dim = 256\n",
    "enc_layers = 3\n",
    "dec_layers = 3\n",
    "enc_heads = 8\n",
    "dec_heads = 8\n",
    "enc_pf_dim = 512\n",
    "dec_pf_dim = 512\n",
    "enc_dropout = 0.1\n",
    "dec_dropout = 0.1\n",
    "\n",
    "SRC_PAD_IDX = PAD_IDX\n",
    "TRG_PAD_IDX = PAD_IDX\n",
    "\n",
    "num_epochs = 10\n",
    "clip       = 1\n",
    "\n",
    "model_losses = {}\n",
    "\n",
    "for attn_variant in ['additive', 'multiplicative', 'general', 'scaled']:\n",
    "\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
    "    valid_loader = DataLoader(val_set,   batch_size=batch_size, shuffle=False, collate_fn=collate_batch)\n",
    "    test_loader  = DataLoader(test_set,  batch_size=batch_size, shuffle=False, collate_fn=collate_batch)\n",
    "\n",
    "    train_loader_length = len(list(iter(train_loader)))\n",
    "    val_loader_length   = len(list(iter(valid_loader)))\n",
    "    test_loader_length  = len(list(iter(test_loader)))\n",
    "\n",
    "    enc = Encoder(input_dim,\n",
    "                  hid_dim, \n",
    "                  enc_layers, \n",
    "                  enc_heads, \n",
    "                  enc_pf_dim, \n",
    "                  enc_dropout, \n",
    "                  attn_variant, \n",
    "                  device)\n",
    "\n",
    "    dec = Decoder(output_dim, \n",
    "                  hid_dim, \n",
    "                  dec_layers, \n",
    "                  dec_heads, \n",
    "                  dec_pf_dim, \n",
    "                  enc_dropout, \n",
    "                  attn_variant, \n",
    "                  device)\n",
    "\n",
    "    model = Seq2SeqTransformer(enc, dec, SRC_PAD_IDX, TRG_PAD_IDX, device).to(device)\n",
    "    model.apply(initialize_weights)\n",
    "\n",
    "    #training hyperparameters\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX) #combine softmax with cross entropy\n",
    "\n",
    "    save_path = f'models/{attn_variant}_{model.__class__.__name__}.pt'\n",
    "\n",
    "    best_valid_loss = float('inf')\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "\n",
    "    print(f'\\n===== {attn_variant} =====')\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        start_time = time.time()\n",
    "\n",
    "        train_loss = train(model, train_loader, optimizer, criterion, clip, train_loader_length)\n",
    "        valid_loss = evaluate(model, valid_loader, criterion, val_loader_length)\n",
    "        \n",
    "        #for plotting\n",
    "        train_losses.append(train_loss)\n",
    "        valid_losses.append(valid_loss)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        \n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "        \n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save([model.params, model.state_dict()], save_path)\n",
    "            # torch.save([model.params, model.state_dict()], save_path)\n",
    "        \n",
    "        print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "        print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
    "        \n",
    "        #lower perplexity is better\n",
    "\n",
    "    model_losses[attn_variant] = {\"train loss\": train_losses, \"valid loss\": valid_losses, \"best valid loss\": best_valid_loss}\n",
    "    \n",
    "    # empty gpu cache to clear memory\n",
    "    del enc, dec, model\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'loss')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(5, 3))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.plot(train_losses, label = 'train loss')\n",
    "ax.plot(valid_losses, label = 'valid loss')\n",
    "plt.legend()\n",
    "ax.set_xlabel('updates')\n",
    "ax.set_ylabel('loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Test Loss: 4.278 | Test PPL:  72.065 |\n"
     ]
    }
   ],
   "source": [
    "params, state = torch.load(save_path)\n",
    "model = Seq2SeqTransformer(**params, device=device).to(device)\n",
    "model.load_state_dict(state)\n",
    "test_loss = evaluate(model, test_loader, criterion, test_loader_length)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test on some random news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"No. It's fine. I'm just glad you're OK.\""
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set[0]['en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ไม่ ไม่เป็นไร ฉันดีใจที่คุณโอเค'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set[0]['th']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  2,   0,   4,   0,  11, 267,   4,   0,  37,  44, 653,   6,  38,   0,\n",
       "          4,   3], device='cuda:0')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_text = text_transform[SRC_LANGUAGE](test_set[0]['en']).to(device)\n",
    "src_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  2,   6,   4, 217,   4,   5, 518,   8,   7, 124,   3],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trg_text = text_transform[TRG_LANGUAGE](test_set[0]['th']).to(device)\n",
    "trg_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_text = src_text.reshape(1, -1)  #because batch_size is 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "trg_text = trg_text.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 16]), torch.Size([1, 11]))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_text.shape, trg_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_length = torch.tensor([src_text.size(0)]).to(dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's pick one of our model, in this case the additive model\n",
    "load_path = 'models/additive_Seq2SeqTransformer.pt'\n",
    "\n",
    "params, state = torch.load(save_path)\n",
    "model = Seq2SeqTransformer(**params, device=device).to(device)\n",
    "model.load_state_dict(state)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output, attentions = model(src_text, trg_text) #turn off teacher forcing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 11, 4243])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape #batch_size, trg_len, trg_output_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since batch size is 1, we just take off that dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = output.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([11, 4243])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall remove the first token since it's zeroes anyway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 4243])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = output[1:]\n",
    "output.shape #trg_len, trg_output_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we just take the top token with highest probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_max = output.argmax(1) #returns max indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4,  5,  4,  5, 64,  8, 62, 88,  3,  3], device='cuda:0')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the mapping of the target language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = vocab_transform[TRG_LANGUAGE].get_itos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "ฉัน\n",
      " \n",
      "ฉัน\n",
      "แค่\n",
      "ที่\n",
      "ดี\n",
      "พูด\n",
      "<eos>\n",
      "<eos>\n"
     ]
    }
   ],
   "source": [
    "for token in output_max:\n",
    "    print(mapping[token.item()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Attention\n",
    "\n",
    "Let's display the attentions to understand how the source text links with the generated text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 11, 16])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attentions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are 8 heads, we can look at just 1 head for sake of simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([11, 16])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention = attentions[0, 0, :, :]\n",
    "attention.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<sos>',\n",
       " 'No',\n",
       " '.',\n",
       " 'It',\n",
       " \"'s\",\n",
       " 'fine',\n",
       " '.',\n",
       " 'I',\n",
       " \"'m\",\n",
       " 'just',\n",
       " 'glad',\n",
       " 'you',\n",
       " \"'re\",\n",
       " 'OK',\n",
       " '.',\n",
       " '<eos>']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_tokens = ['<sos>'] + token_transform[SRC_LANGUAGE](test_set[0]['en']) + ['<eos>']\n",
    "src_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<sos>', ' ', 'ฉัน', ' ', 'ฉัน', 'แค่', 'ที่', 'ดี', 'พูด', '<eos>', '<eos>']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trg_tokens = ['<sos>'] + [mapping[token.item()] for token in output_max]\n",
    "trg_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker as ticker\n",
    "\n",
    "def display_attention(sentence, translation, attention):\n",
    "    \n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(111)\n",
    "    \n",
    "    attention = attention.squeeze(1).cpu().detach().numpy()\n",
    "    \n",
    "    cax = ax.matshow(attention, cmap='bone')\n",
    "   \n",
    "    ax.tick_params(labelsize=10)\n",
    "    \n",
    "    y_ticks =  [''] + translation\n",
    "    x_ticks =  [''] + sentence \n",
    "     \n",
    "    ax.set_xticklabels(x_ticks, rotation=45)\n",
    "    ax.set_yticklabels(y_ticks)\n",
    "\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_10016\\59549304.py:17: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
      "  ax.set_xticklabels(x_ticks, rotation=45)\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_10016\\59549304.py:18: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
      "  ax.set_yticklabels(y_ticks)\n",
      "c:\\Users\\Admin\\anaconda3\\envs\\dsai\\lib\\site-packages\\IPython\\core\\pylabtools.py:152: UserWarning: Glyph 3593 (\\N{THAI CHARACTER CHO CHING}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\Admin\\anaconda3\\envs\\dsai\\lib\\site-packages\\IPython\\core\\pylabtools.py:152: UserWarning: Glyph 3633 (\\N{THAI CHARACTER MAI HAN-AKAT}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\Admin\\anaconda3\\envs\\dsai\\lib\\site-packages\\IPython\\core\\pylabtools.py:152: UserWarning: Glyph 3609 (\\N{THAI CHARACTER NO NU}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\Admin\\anaconda3\\envs\\dsai\\lib\\site-packages\\IPython\\core\\pylabtools.py:152: UserWarning: Glyph 3649 (\\N{THAI CHARACTER SARA AE}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\Admin\\anaconda3\\envs\\dsai\\lib\\site-packages\\IPython\\core\\pylabtools.py:152: UserWarning: Glyph 3588 (\\N{THAI CHARACTER KHO KHWAI}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\Admin\\anaconda3\\envs\\dsai\\lib\\site-packages\\IPython\\core\\pylabtools.py:152: UserWarning: Glyph 3656 (\\N{THAI CHARACTER MAI EK}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\Admin\\anaconda3\\envs\\dsai\\lib\\site-packages\\IPython\\core\\pylabtools.py:152: UserWarning: Glyph 3607 (\\N{THAI CHARACTER THO THAHAN}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\Admin\\anaconda3\\envs\\dsai\\lib\\site-packages\\IPython\\core\\pylabtools.py:152: UserWarning: Glyph 3637 (\\N{THAI CHARACTER SARA II}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\Admin\\anaconda3\\envs\\dsai\\lib\\site-packages\\IPython\\core\\pylabtools.py:152: UserWarning: Glyph 3604 (\\N{THAI CHARACTER DO DEK}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\Admin\\anaconda3\\envs\\dsai\\lib\\site-packages\\IPython\\core\\pylabtools.py:152: UserWarning: Glyph 3614 (\\N{THAI CHARACTER PHO PHAN}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\Admin\\anaconda3\\envs\\dsai\\lib\\site-packages\\IPython\\core\\pylabtools.py:152: UserWarning: Glyph 3641 (\\N{THAI CHARACTER SARA UU}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_attention(src_tokens, trg_tokens, attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "714d3f4db9a58ba7d2f2a9a4fffe577af3df8551aebd380095064812e2e0a6a4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
