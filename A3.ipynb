{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Translation + Transformer\n",
    "\n",
    "<img src = \"../figures/transformer1.png\" >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
      "Requirement already satisfied: torch in c:\\users\\legion\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.1.2+cu121)\n",
      "Requirement already satisfied: torchvision in c:\\users\\legion\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.16.2+cu121)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\legion\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.1.2+cu121)\n",
      "Requirement already satisfied: torchtext in c:\\users\\legion\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.16.2+cpu)\n",
      "Requirement already satisfied: filelock in c:\\users\\legion\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.12.4)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\legion\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\legion\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\legion\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\legion\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\legion\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\legion\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torchvision) (1.24.4)\n",
      "Requirement already satisfied: requests in c:\\users\\legion\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torchvision) (2.31.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\legion\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torchvision) (10.0.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\legion\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torchtext) (4.66.1)\n",
      "Requirement already satisfied: torchdata==0.7.1 in c:\\users\\legion\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torchtext) (0.7.1)\n",
      "Requirement already satisfied: urllib3>=1.25 in c:\\users\\legion\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torchdata==0.7.1->torchtext) (1.26.16)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\legion\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\legion\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->torchvision) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\legion\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\legion\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->torchvision) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\legion\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\legion\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm->torchtext) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.1 -> 23.3.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# !pip install torch torchvision torchaudio torchtext --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch, torchdata, torchtext\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import random, math, time\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import re\n",
    "\n",
    "import os\n",
    "import chardet\n",
    "\n",
    "import datasets\n",
    "import gc\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "#make our work comparable if restarted the kernel\n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.2+cu121'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.16.2+cpu'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchtext.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 1. ETL: Loading the dataset\n",
    "\n",
    "**Note**: Here I chose to translate English to German, simply it is easier for myself, since I don't understand German so it is difficult for me to imagine a sentence during inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"opus100\",'en-ne')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 406381\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'translation': {'en': 'Activate directory-only selection',\n",
       "  'ne': 'चयन निर्दिशिका मात्र सक्रिय पार्नुहोस्'}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][60]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. EDA - simple investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import default_rng\n",
    "rng = default_rng(seed=SEED)\n",
    "\n",
    "select_index = rng.choice(len(dataset['train']), size=406381, replace = False)\n",
    "dataset['train']= dataset['train'].filter(lambda data, index:index in select_index, with_indices=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_new_col = lambda data , lang:{lang:data['translation'][lang]}\n",
    "dataset = dataset.map(get_new_col, fn_kwargs={'lang':\"ne\"})\n",
    "dataset = dataset.map(get_new_col, remove_columns=['translation'],fn_kwargs={'lang':\"en\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ne': 'Inv', 'en': '_Inv'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's take a look at one example of train\n",
    "sample = next(iter(dataset['train']))\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "406381"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size = len(list(iter(dataset['train'])))\n",
    "train_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since 29001 is plenty,, we gonna call `random_split` to train, val and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['ne', 'en'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['ne', 'en'],\n",
       "        num_rows: 406381\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['ne', 'en'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 3. Preprocessing \n",
    "\n",
    "### Tokenizing\n",
    "\n",
    "**Note**: the models must first be downloaded using the following on the command line: \n",
    "```\n",
    "python3 -m spacy download en_core_web_sm\n",
    "python3 -m spacy download de_core_news_sm\n",
    "```\n",
    "\n",
    "First, since we have two languages, let's create some constants to represent that.  Also, let's create two dicts: one for holding our tokenizers and one for holding all the vocabs with assigned numbers for each unique word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place-holders\n",
    "token_transform = {}\n",
    "vocab_transform = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_LANGUAGE = 'en'\n",
    "TRG_LANGUAGE = 'ne'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from nepalitokenizers import WordPiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_transform[\"en\"] = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "token_transform[\"ne\"] = WordPiece()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence:  मिलेका रेखाहरुको माथि पिक्सलहरु\n",
      "Tokenization:  ['[CLS]', 'मिले', '##का', 'रेखा', '##हरुको', 'माथि', 'पिक', '##्स', '##ल', '##हरु', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "#example of tokenization of the english part\n",
    "print(\"Sentence: \", dataset['train']['ne'][2])\n",
    "print(\"Tokenization: \", token_transform['ne'].encode(dataset['train']['ne'][2]).tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccb34241c1ed4465968cca9b91898c8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93b6df2b910649039ea956ed82061f56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/406381 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20b0340cb33c44d9ac2a8cd90f442bf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3db5b474332453c9d5e3987d206d307",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a05d1aee19941f5a0b8e3d55ab4879e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/406381 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "227756f112fe4999ace99d5f92079a00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_the_data(data,lang):\n",
    "    try:\n",
    "        return{lang:token_transform[lang](data[lang].lower())}\n",
    "    except:\n",
    "        return{lang:token_transform[lang].encode(data[lang].lower()).tokens}\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_the_data, remove_columns=[SRC_LANGUAGE], fn_kwargs={'lang':SRC_LANGUAGE})\n",
    "tokenized_dataset = tokenized_dataset.map(tokenize_the_data, remove_columns=[TRG_LANGUAGE], fn_kwargs={'lang':TRG_LANGUAGE})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ne': ['[CLS]',\n",
       "  'क्लि',\n",
       "  '##प',\n",
       "  '##बोर्ड',\n",
       "  'साइन',\n",
       "  'गर्न',\n",
       "  'सकिएन',\n",
       "  '।',\n",
       "  '[SEP]'],\n",
       " 'en': ['the', 'clipboard', 'could', 'not', 'be', 'signed', '.']}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset['train'][5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function to tokenize our input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to yield list of tokens\n",
    "# here data can be `train` or `val` or `test`\n",
    "def yield_tokens(data, language):\n",
    "    language_index = {'ne': 0, 'en': 1}\n",
    "\n",
    "    for data_sample in data:\n",
    "        yield token_transform[language](data_sample[language_index[language]]) #either first or second index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we tokenize, let's define some special symbols so our neural network understand the embeddings of these symbols, namely the unknown, the padding, the start of sentence, and end of sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define special symbols and indices\n",
    "UNK_IDX, PAD_IDX, SOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
    "special_symbols = ['<unk>', '<pad>', '<sos>', '<eos>']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Text to integers (Numericalization)\n",
    "\n",
    "Next we gonna create function (torchtext called vocabs) that turn these tokens into integers.  Here we use built in factory function <code>build_vocab_from_iterator</code> which accepts iterator that yield list or iterator of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "for ln in [SRC_LANGUAGE, TRG_LANGUAGE]:\n",
    "    # Create torchtext's Vocab object \n",
    "    vocab_transform[ln] = build_vocab_from_iterator(tokenized_dataset['train'][ln], \n",
    "                                                    min_freq=2,   #if not, everything will be treated as UNK\n",
    "                                                    specials=special_symbols,\n",
    "                                                    special_first=True) #indicates whether to insert symbols at the beginning or at the end                                            \n",
    "# Set UNK_IDX as the default index. This index is returned when the token is not found. \n",
    "# If not set, it throws RuntimeError when the queried token is not found in the Vocabulary. \n",
    "for ln in [SRC_LANGUAGE, TRG_LANGUAGE]:\n",
    "    vocab_transform[ln].set_default_index(UNK_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#see some example\n",
    "vocab_transform[SRC_LANGUAGE](['रेखा'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'backwards'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we can reverse it....\n",
    "mapping = vocab_transform[SRC_LANGUAGE].get_itos()\n",
    "\n",
    "#print 1816, for example\n",
    "mapping[1891]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<unk>'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's try unknown vocab\n",
    "mapping[0]\n",
    "#they will all map to <unk> which has 0 as integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<pad>', '<sos>', '<eos>')"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's try special symbols\n",
    "mapping[1], mapping[2], mapping[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22083"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check unique vocabularies\n",
    "len(mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4. Preparing the dataloader\n",
    "\n",
    "One thing we change here is the <code>collate_fn</code> which now also returns the length of sentence.  This is required for <code>packed_padded_sequence</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# helper function to club together sequential operations\n",
    "def sequential_transforms(*transforms):\n",
    "    def func(txt_input):\n",
    "        for transform in transforms:\n",
    "            txt_input = transform(txt_input)\n",
    "        return txt_input\n",
    "    return func\n",
    "\n",
    "# function to add BOS/EOS and create tensor for input sequence indices\n",
    "def tensor_transform(token_ids):\n",
    "    return torch.cat((torch.tensor([SOS_IDX]), \n",
    "                      torch.tensor(token_ids), \n",
    "                      torch.tensor([EOS_IDX])))\n",
    "\n",
    "# src and trg language text transforms to convert raw strings into tensors indices\n",
    "text_transform = {}\n",
    "for ln in [SRC_LANGUAGE, TRG_LANGUAGE]:\n",
    "    text_transform[ln] = sequential_transforms(token_transform[ln], #Tokenization\n",
    "                                               vocab_transform[ln], #Numericalization\n",
    "                                               tensor_transform) # Add BOS/EOS and create tensor\n",
    "\n",
    "\n",
    "# function to collate data samples into batch tesors\n",
    "def collate_batch(batch):\n",
    "    src_batch, src_len_batch, trg_batch = [], [], []\n",
    "    for src_sample, trg_sample in batch:\n",
    "        processed_text = text_transform[SRC_LANGUAGE](src_sample.rstrip(\"\\n\"))\n",
    "        src_batch.append(processed_text)\n",
    "        trg_batch.append(text_transform[TRG_LANGUAGE](trg_sample.rstrip(\"\\n\")))\n",
    "        src_len_batch.append(processed_text.size(0))\n",
    "\n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX, batch_first = True) #<----need this because we use linear layers mostly\n",
    "    trg_batch = pad_sequence(trg_batch, padding_value=PAD_IDX, batch_first = True)\n",
    "    return src_batch, torch.tensor(src_len_batch, dtype=torch.int64), trg_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create train, val, and test dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_loader = DataLoader(dataset['train'], batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
    "valid_loader = DataLoader(dataset['validation'],   batch_size=batch_size, shuffle=False, collate_fn=collate_batch)\n",
    "test_loader  = DataLoader(dataset['test'],  batch_size=batch_size, shuffle=False, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'WordPiece' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[94], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ne, en \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\LEGION\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\LEGION\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\LEGION\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[92], line 34\u001b[0m, in \u001b[0;36mcollate_batch\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m     32\u001b[0m     processed_text \u001b[38;5;241m=\u001b[39m text_transform[SRC_LANGUAGE](src_sample\u001b[38;5;241m.\u001b[39mrstrip(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m     33\u001b[0m     src_batch\u001b[38;5;241m.\u001b[39mappend(processed_text)\n\u001b[1;32m---> 34\u001b[0m     trg_batch\u001b[38;5;241m.\u001b[39mappend(\u001b[43mtext_transform\u001b[49m\u001b[43m[\u001b[49m\u001b[43mTRG_LANGUAGE\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrg_sample\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrstrip\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     35\u001b[0m     src_len_batch\u001b[38;5;241m.\u001b[39mappend(processed_text\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m))\n\u001b[0;32m     37\u001b[0m src_batch \u001b[38;5;241m=\u001b[39m pad_sequence(src_batch, padding_value\u001b[38;5;241m=\u001b[39mPAD_IDX, batch_first \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;66;03m#<----need this because we use linear layers mostly\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[92], line 10\u001b[0m, in \u001b[0;36msequential_transforms.<locals>.func\u001b[1;34m(txt_input)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunc\u001b[39m(txt_input):\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m transform \u001b[38;5;129;01min\u001b[39;00m transforms:\n\u001b[1;32m---> 10\u001b[0m         txt_input \u001b[38;5;241m=\u001b[39m \u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtxt_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m txt_input\n",
      "\u001b[1;31mTypeError\u001b[0m: 'WordPiece' object is not callable"
     ]
    }
   ],
   "source": [
    "for ne, en in train_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['ne', 'en'],\n",
       "    num_rows: 406381\n",
       "})"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ne'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[70], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnglish shape: \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43men\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m)  \u001b[38;5;66;03m# (batch_size, seq len)\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGerman shape: \u001b[39m\u001b[38;5;124m\"\u001b[39m, ne\u001b[38;5;241m.\u001b[39mshape)   \u001b[38;5;66;03m# (batch_size, seq len)\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "print(\"English shape: \", en.shape)  # (batch_size, seq len)\n",
    "print(\"German shape: \", ne.shape)   # (batch_size, seq len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Design the model\n",
    "\n",
    "<img src=\"../figures/transformer-encoder.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, pf_dim, dropout, device):\n",
    "        super().__init__()\n",
    "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.ff_layer_norm        = nn.LayerNorm(hid_dim)\n",
    "        self.self_attention       = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
    "        self.feedforward          = PositionwiseFeedforwardLayer(hid_dim, pf_dim, dropout)\n",
    "        self.dropout              = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src, src_mask):\n",
    "        #src = [batch size, src len, hid dim]\n",
    "        #src_mask = [batch size, 1, 1, src len]   #if the token is padding, it will be 1, otherwise 0\n",
    "        _src, _ = self.self_attention(src, src, src, src_mask)\n",
    "        src     = self.self_attn_layer_norm(src + self.dropout(_src))\n",
    "        #src: [batch_size, src len, hid dim]\n",
    "        \n",
    "        _src    = self.feedforward(src)\n",
    "        src     = self.ff_layer_norm(src + self.dropout(_src))\n",
    "        #src: [batch_size, src len, hid dim]\n",
    "        \n",
    "        return src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hid_dim, n_layers, n_heads, pf_dim, dropout, device, max_length = 100):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.tok_embedding = nn.Embedding(input_dim, hid_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
    "        self.layers        = nn.ModuleList([EncoderLayer(hid_dim, n_heads, pf_dim, dropout, device)\n",
    "                                           for _ in range(n_layers)])\n",
    "        self.dropout       = nn.Dropout(dropout)\n",
    "        self.scale         = torch.sqrt(torch.FloatTensor([hid_dim])).to(self.device)\n",
    "        \n",
    "    def forward(self, src, src_mask):\n",
    "        \n",
    "        #src = [batch size, src len]\n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "        \n",
    "        batch_size = src.shape[0]\n",
    "        src_len    = src.shape[1]\n",
    "        \n",
    "        pos        = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
    "        #pos: [batch_size, src_len]\n",
    "        \n",
    "        src        = self.dropout((self.tok_embedding(src) * self.scale) + self.pos_embedding(pos))\n",
    "        #src: [batch_size, src_len, hid_dim]\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            src = layer(src, src_mask)\n",
    "        #src: [batch_size, src_len, hid_dim]\n",
    "        \n",
    "        return src\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mutli Head Attention Layer\n",
    "\n",
    "<img src = \"../figures/transformer-attention.png\" width=\"700\">\n",
    "\n",
    "$$ \\text{Attention}(Q, K, V) = \\text{Softmax} \\big( \\frac{QK^T}{\\sqrt{d_k}} \\big)V $$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, dropout, device):\n",
    "        super().__init__()\n",
    "        assert hid_dim % n_heads == 0\n",
    "        self.hid_dim  = hid_dim\n",
    "        self.n_heads  = n_heads\n",
    "        self.head_dim = hid_dim // n_heads\n",
    "        \n",
    "        self.fc_q     = nn.Linear(hid_dim, hid_dim)\n",
    "        self.fc_k     = nn.Linear(hid_dim, hid_dim)\n",
    "        self.fc_v     = nn.Linear(hid_dim, hid_dim)\n",
    "        \n",
    "        self.fc_o     = nn.Linear(hid_dim, hid_dim)\n",
    "        \n",
    "        self.dropout  = nn.Dropout(dropout)\n",
    "        \n",
    "        self.scale    = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
    "                \n",
    "    def forward(self, query, key, value, mask = None):\n",
    "        #src, src, src, src_mask\n",
    "        #query = [batch size, query len, hid dim]\n",
    "        #key = [batch size, key len, hid dim]\n",
    "        #value = [batch size, value len, hid dim]\n",
    "        \n",
    "        batch_size = query.shape[0]\n",
    "        \n",
    "        Q = self.fc_q(query)\n",
    "        K = self.fc_k(key)\n",
    "        V = self.fc_v(value)\n",
    "        #Q=K=V: [batch_size, src len, hid_dim]\n",
    "        \n",
    "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        #Q = [batch_size, n heads, query len, head_dim]\n",
    "        \n",
    "        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n",
    "        #Q = [batch_size, n heads, query len, head_dim] @ K = [batch_size, n heads, head_dim, key len]\n",
    "        #energy = [batch_size, n heads, query len, key len]\n",
    "        \n",
    "        #for making attention to padding to 0\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, -1e10)\n",
    "            \n",
    "        attention = torch.softmax(energy, dim = -1)\n",
    "        #attention = [batch_size, n heads, query len, key len]\n",
    "        \n",
    "        x = torch.matmul(self.dropout(attention), V)\n",
    "        #[batch_size, n heads, query len, key len] @ [batch_size, n heads, value len, head_dim]\n",
    "        #x = [batch_size, n heads, query len, head dim]\n",
    "        \n",
    "        x = x.permute(0, 2, 1, 3).contiguous()  #we can perform .view\n",
    "        #x = [batch_size, query len, n heads, head dim]\n",
    "        \n",
    "        x = x.view(batch_size, -1, self.hid_dim)\n",
    "        #x = [batch_size, query len, hid dim]\n",
    "        \n",
    "        x = self.fc_o(x)\n",
    "        #x = [batch_size, query len, hid dim]\n",
    "        \n",
    "        return x, attention\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = torch.randn(32, 100, 256)\n",
    "keys = torch.clone(queries)\n",
    "values = torch.clone(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = torch.bmm(queries, keys.transpose(1, 2)) / (256 ** 0.5)\n",
    "softmax = nn.Softmax(dim=2)\n",
    "attention = softmax(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100, 256])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.bmm(attention, values).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100, 100])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Position-wise Feedforward Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedforwardLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, pf_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(hid_dim, pf_dim)\n",
    "        self.fc2 = nn.Linear(pf_dim, hid_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #x = [batch size, src len, hid dim]\n",
    "        x = self.dropout(torch.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Decoder Layer\n",
    "\n",
    "<img src = \"../figures/transformer-decoder.png\" >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, pf_dim, dropout, device):\n",
    "        super().__init__()\n",
    "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.enc_attn_layer_norm  = nn.LayerNorm(hid_dim)\n",
    "        self.ff_layer_norm        = nn.LayerNorm(hid_dim)\n",
    "        self.self_attention       = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
    "        self.encoder_attention    = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
    "        self.feedforward          = PositionwiseFeedforwardLayer(hid_dim, pf_dim, dropout)\n",
    "        self.dropout              = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "        \n",
    "        #trg = [batch size, trg len, hid dim]\n",
    "        #enc_src = [batch size, src len, hid dim]\n",
    "        #trg_mask = [batch size, 1, trg len, trg len]\n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "        \n",
    "        _trg, _ = self.self_attention(trg, trg, trg, trg_mask)\n",
    "        trg     = self.self_attn_layer_norm(trg + self.dropout(_trg))\n",
    "        #trg = [batch_size, trg len, hid dim]\n",
    "        \n",
    "        _trg, attention = self.encoder_attention(trg, enc_src, enc_src, src_mask)\n",
    "        trg             = self.enc_attn_layer_norm(trg + self.dropout(_trg))\n",
    "        #trg = [batch_size, trg len, hid dim]\n",
    "        #attention = [batch_size, n heads, trg len, src len]\n",
    "        \n",
    "        _trg = self.feedforward(trg)\n",
    "        trg  = self.ff_layer_norm(trg + self.dropout(_trg))\n",
    "        #trg = [batch_size, trg len, hid dim]\n",
    "        \n",
    "        return trg, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, hid_dim, n_layers, n_heads, \n",
    "                 pf_dim, dropout, device,max_length = 100):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.tok_embedding = nn.Embedding(output_dim, hid_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
    "        self.layers        = nn.ModuleList([DecoderLayer(hid_dim, n_heads, pf_dim, dropout, device)\n",
    "                                            for _ in range(n_layers)])\n",
    "        self.fc_out        = nn.Linear(hid_dim, output_dim)\n",
    "        self.dropout       = nn.Dropout(dropout)\n",
    "        self.scale         = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
    "        \n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "        \n",
    "        #trg = [batch size, trg len]\n",
    "        #enc_src = [batch size, src len, hid dim]\n",
    "        #trg_mask = [batch size, 1, trg len, trg len]\n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "        \n",
    "        batch_size = trg.shape[0]\n",
    "        trg_len    = trg.shape[1]\n",
    "        \n",
    "        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
    "        #pos: [batch_size, trg len]\n",
    "        \n",
    "        trg = self.dropout((self.tok_embedding(trg) * self.scale) + self.pos_embedding(pos))\n",
    "        #trg: [batch_size, trg len, hid dim]\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            trg, attention = layer(trg, enc_src, trg_mask, src_mask)\n",
    "            \n",
    "        #trg: [batch_size, trg len, hid dim]\n",
    "        #attention: [batch_size, n heads, trg len, src len]\n",
    "        \n",
    "        output = self.fc_out(trg)\n",
    "        #output = [batch_size, trg len, output_dim]\n",
    "        \n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting them together (become Seq2Seq!)\n",
    "\n",
    "Our `trg_sub_mask` will look something like this (for a target with 5 tokens):\n",
    "\n",
    "$$\\begin{matrix}\n",
    "1 & 0 & 0 & 0 & 0\\\\\n",
    "1 & 1 & 0 & 0 & 0\\\\\n",
    "1 & 1 & 1 & 0 & 0\\\\\n",
    "1 & 1 & 1 & 1 & 0\\\\\n",
    "1 & 1 & 1 & 1 & 1\\\\\n",
    "\\end{matrix}$$\n",
    "\n",
    "The \"subsequent\" mask is then logically anded with the padding mask, this combines the two masks ensuring both the subsequent tokens and the padding tokens cannot be attended to. For example if the last two tokens were `<pad>` tokens the mask would look like:\n",
    "\n",
    "$$\\begin{matrix}\n",
    "1 & 0 & 0 & 0 & 0\\\\\n",
    "1 & 1 & 0 & 0 & 0\\\\\n",
    "1 & 1 & 1 & 0 & 0\\\\\n",
    "1 & 1 & 1 & 0 & 0\\\\\n",
    "1 & 1 & 1 & 0 & 0\\\\\n",
    "\\end{matrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(self, encoder, decoder, src_pad_idx, trg_pad_idx, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        self.device = device\n",
    "        \n",
    "    def make_src_mask(self, src):\n",
    "        \n",
    "        #src = [batch size, src len]\n",
    "        \n",
    "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "\n",
    "        return src_mask\n",
    "    \n",
    "    def make_trg_mask(self, trg):\n",
    "        \n",
    "        #trg = [batch size, trg len]\n",
    "        \n",
    "        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        #trg_pad_mask = [batch size, 1, 1, trg len]\n",
    "        \n",
    "        trg_len = trg.shape[1]\n",
    "        \n",
    "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device = self.device)).bool()\n",
    "        #trg_sub_mask = [trg len, trg len]\n",
    "            \n",
    "        trg_mask = trg_pad_mask & trg_sub_mask\n",
    "        #trg_mask = [batch size, 1, trg len, trg len]\n",
    "        \n",
    "        return trg_mask\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        \n",
    "        #src = [batch size, src len]\n",
    "        #trg = [batch size, trg len]\n",
    "                \n",
    "        src_mask = self.make_src_mask(src)\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "        \n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "        #trg_mask = [batch size, 1, trg len, trg len]\n",
    "        \n",
    "        enc_src = self.encoder(src, src_mask)\n",
    "        #enc_src = [batch size, src len, hid dim]\n",
    "                \n",
    "        output, attention = self.decoder(trg, enc_src, trg_mask, src_mask)\n",
    "        \n",
    "        #output = [batch size, trg len, output dim]\n",
    "        #attention = [batch size, n heads, trg len, src len]\n",
    "        \n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 6. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(m):\n",
    "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
    "        nn.init.xavier_uniform_(m.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(vocab_transform[SRC_LANGUAGE])\n",
    "OUTPUT_DIM = len(vocab_transform[TRG_LANGUAGE])\n",
    "HID_DIM = 256\n",
    "ENC_LAYERS = 3\n",
    "DEC_LAYERS = 3\n",
    "ENC_HEADS = 8\n",
    "DEC_HEADS = 8\n",
    "ENC_PF_DIM = 512\n",
    "DEC_PF_DIM = 512\n",
    "ENC_DROPOUT = 0.1\n",
    "DEC_DROPOUT = 0.1\n",
    "\n",
    "enc = Encoder(INPUT_DIM, \n",
    "              HID_DIM, \n",
    "              ENC_LAYERS, \n",
    "              ENC_HEADS, \n",
    "              ENC_PF_DIM, \n",
    "              ENC_DROPOUT, \n",
    "              device)\n",
    "\n",
    "dec = Decoder(OUTPUT_DIM, \n",
    "              HID_DIM, \n",
    "              DEC_LAYERS, \n",
    "              DEC_HEADS, \n",
    "              DEC_PF_DIM, \n",
    "              DEC_DROPOUT, \n",
    "              device)\n",
    "\n",
    "SRC_PAD_IDX = PAD_IDX\n",
    "TRG_PAD_IDX = PAD_IDX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2SeqTransformer(\n",
       "  (encoder): Encoder(\n",
       "    (tok_embedding): Embedding(22083, 256)\n",
       "    (pos_embedding): Embedding(100, 256)\n",
       "    (layers): ModuleList(\n",
       "      (0-2): 3 x EncoderLayer(\n",
       "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feedforward): PositionwiseFeedforwardLayer(\n",
       "          (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (tok_embedding): Embedding(11135, 256)\n",
       "    (pos_embedding): Embedding(100, 256)\n",
       "    (layers): ModuleList(\n",
       "      (0-2): 3 x DecoderLayer(\n",
       "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (enc_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feedforward): PositionwiseFeedforwardLayer(\n",
       "          (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (fc_out): Linear(in_features=256, out_features=11135, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dim   = len(vocab_transform[SRC_LANGUAGE])\n",
    "output_dim  = len(vocab_transform[TRG_LANGUAGE])\n",
    "hid_dim = 256\n",
    "enc_layers = 3\n",
    "dec_layers = 3\n",
    "enc_heads = 8\n",
    "dec_heads = 8\n",
    "enc_pf_dim = 512\n",
    "dec_pf_dim = 512\n",
    "enc_dropout = 0.1\n",
    "dec_dropout = 0.1\n",
    "\n",
    "SRC_PAD_IDX = PAD_IDX\n",
    "TRG_PAD_IDX = PAD_IDX\n",
    "\n",
    "enc = Encoder(input_dim, \n",
    "              hid_dim, \n",
    "              enc_layers, \n",
    "              enc_heads, \n",
    "              enc_pf_dim, \n",
    "              enc_dropout, \n",
    "              device)\n",
    "\n",
    "dec = Decoder(output_dim, \n",
    "              hid_dim, \n",
    "              dec_layers, \n",
    "              dec_heads, \n",
    "              dec_pf_dim, \n",
    "              enc_dropout, \n",
    "              device)\n",
    "\n",
    "model = Seq2SeqTransformer(enc, dec, SRC_PAD_IDX, TRG_PAD_IDX, device).to(device)\n",
    "model.apply(initialize_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5653248\n",
      " 25600\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      "131072\n",
      "   512\n",
      "131072\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      "131072\n",
      "   512\n",
      "131072\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      "131072\n",
      "   512\n",
      "131072\n",
      "   256\n",
      "2850560\n",
      " 25600\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      "131072\n",
      "   512\n",
      "131072\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      "131072\n",
      "   512\n",
      "131072\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      "131072\n",
      "   512\n",
      "131072\n",
      "   256\n",
      "2850560\n",
      " 11135\n",
      "______\n",
      "15370367\n"
     ]
    }
   ],
   "source": [
    "#we can print the complexity by the number of parameters\n",
    "def count_parameters(model):\n",
    "    params = [p.numel() for p in model.parameters() if p.requires_grad]\n",
    "    for item in params:\n",
    "        print(f'{item:>6}')\n",
    "    print(f'______\\n{sum(params):>6}')\n",
    "    \n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "lr = 0.0005\n",
    "\n",
    "#training hyperparameters\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX) #combine softmax with cross entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we'll define our training loop. This is the exact same as the one used in the previous tutorial.\n",
    "\n",
    "As we want our model to predict the `<eos>` token but not have it be an input into our model we simply slice the `<eos>` token off the end of the sequence. Thus:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\text{trg} &= [sos, x_1, x_2, x_3, eos]\\\\\n",
    "\\text{trg[:-1]} &= [sos, x_1, x_2, x_3]\n",
    "\\end{align*}$$\n",
    "\n",
    "$x_i$ denotes actual target sequence element. We then feed this into the model to get a predicted sequence that should hopefully predict the `<eos>` token:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\text{output} &= [y_1, y_2, y_3, eos]\n",
    "\\end{align*}$$\n",
    "\n",
    "$y_i$ denotes predicted target sequence element. We then calculate our loss using the original `trg` tensor with the `<sos>` token sliced off the front, leaving the `<eos>` token:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\text{output} &= [y_1, y_2, y_3, eos]\\\\\n",
    "\\text{trg[1:]} &= [x_1, x_2, x_3, eos]\n",
    "\\end{align*}$$\n",
    "\n",
    "We then calculate our losses and update our parameters as is standard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, optimizer, criterion, clip, loader_length):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for src, src_len, trg in loader:\n",
    "        \n",
    "        src = src.to(device)\n",
    "        trg = trg.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #trg[:, :-1] remove the eos, e.g., \"<sos> I love sushi\" since teaching forcing, the input does not need to have eos\n",
    "        output, _ = model(src, trg[:,:-1])\n",
    "                \n",
    "        #output = [batch size, trg len - 1, output dim]\n",
    "        #trg    = [batch size, trg len]\n",
    "            \n",
    "        output_dim = output.shape[-1]\n",
    "            \n",
    "        output = output.reshape(-1, output_dim)\n",
    "        trg = trg[:,1:].reshape(-1) #trg[:, 1:] remove the sos, e.g., \"i love sushi <eos>\" since in teaching forcing, the output does not have sos\n",
    "                \n",
    "        #output = [batch size * trg len - 1, output dim]\n",
    "        #trg    = [batch size * trg len - 1]\n",
    "            \n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / loader_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our evaluation loop is similar to our training loop, however as we aren't updating any parameters we don't need to pass an optimizer or a clip value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader, criterion, loader_length):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for src, src_len, trg in loader:\n",
    "        \n",
    "            src = src.to(device)\n",
    "            trg = trg.to(device)\n",
    "\n",
    "            output, _ = model(src, trg[:,:-1])\n",
    "            \n",
    "            #output = [batch size, trg len - 1, output dim]\n",
    "            #trg = [batch size, trg len]\n",
    "            \n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            output = output.contiguous().view(-1, output_dim)\n",
    "            trg = trg[:,1:].contiguous().view(-1)\n",
    "            \n",
    "            #output = [batch size * trg len - 1, output dim]\n",
    "            #trg = [batch size * trg len - 1]\n",
    "            \n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / loader_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting everything together\n",
    "\n",
    "Finally, we train our actual model. This model is almost 3x faster than the convolutional sequence-to-sequence model and also achieves a lower validation perplexity!\n",
    "\n",
    "**Note: similar to CNN, this model always has a teacher forcing ratio of 1, i.e. it will always use the ground truth next token from the target sequence (this is simply because CNN do everything in parallel so we cannot have the next token). This means we cannot compare perplexity values against the previous models when they are using a teacher forcing ratio that is not 1. To understand this, try run previous tutorials with teaching forcing ratio of 1, you will get very low perplexity.  **   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'WordPiece' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[63], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_loader_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m      2\u001b[0m val_loader_length   \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28miter\u001b[39m(valid_loader)))\n\u001b[0;32m      3\u001b[0m test_loader_length  \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28miter\u001b[39m(test_loader)))\n",
      "File \u001b[1;32mc:\\Users\\LEGION\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\LEGION\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\LEGION\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[46], line 34\u001b[0m, in \u001b[0;36mcollate_batch\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m     32\u001b[0m     processed_text \u001b[38;5;241m=\u001b[39m text_transform[SRC_LANGUAGE](src_sample\u001b[38;5;241m.\u001b[39mrstrip(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m     33\u001b[0m     src_batch\u001b[38;5;241m.\u001b[39mappend(processed_text)\n\u001b[1;32m---> 34\u001b[0m     trg_batch\u001b[38;5;241m.\u001b[39mappend(\u001b[43mtext_transform\u001b[49m\u001b[43m[\u001b[49m\u001b[43mTRG_LANGUAGE\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrg_sample\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrstrip\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     35\u001b[0m     src_len_batch\u001b[38;5;241m.\u001b[39mappend(processed_text\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m))\n\u001b[0;32m     37\u001b[0m src_batch \u001b[38;5;241m=\u001b[39m pad_sequence(src_batch, padding_value\u001b[38;5;241m=\u001b[39mPAD_IDX, batch_first \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;66;03m#<----need this because we use linear layers mostly\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[46], line 10\u001b[0m, in \u001b[0;36msequential_transforms.<locals>.func\u001b[1;34m(txt_input)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunc\u001b[39m(txt_input):\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m transform \u001b[38;5;129;01min\u001b[39;00m transforms:\n\u001b[1;32m---> 10\u001b[0m         txt_input \u001b[38;5;241m=\u001b[39m \u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtxt_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m txt_input\n",
      "\u001b[1;31mTypeError\u001b[0m: 'WordPiece' object is not callable"
     ]
    }
   ],
   "source": [
    "train_loader_length = len(list(iter(train_loader)))\n",
    "val_loader_length   = len(list(iter(valid_loader)))\n",
    "test_loader_length  = len(list(iter(test_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_loader_length' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[65], line 14\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m     12\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 14\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m train(model, train_loader, optimizer, criterion, clip, \u001b[43mtrain_loader_length\u001b[49m)\n\u001b[0;32m     15\u001b[0m     valid_loss \u001b[38;5;241m=\u001b[39m evaluate(model, valid_loader, criterion, val_loader_length)\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;66;03m#for plotting\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_loader_length' is not defined"
     ]
    }
   ],
   "source": [
    "best_valid_loss = float('inf')\n",
    "num_epochs = 1\n",
    "clip       = 1\n",
    "\n",
    "save_path = f'models/{model.__class__.__name__}.pt'\n",
    "\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss = train(model, train_loader, optimizer, criterion, clip, train_loader_length)\n",
    "    valid_loss = evaluate(model, valid_loader, criterion, val_loader_length)\n",
    "    \n",
    "    #for plotting\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
    "    \n",
    "    #lower perplexity is better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'loss')"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(5, 3))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.plot(train_losses, label = 'train loss')\n",
    "ax.plot(valid_losses, label = 'valid loss')\n",
    "plt.legend()\n",
    "ax.set_xlabel('updates')\n",
    "ax.set_ylabel('loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(save_path))\n",
    "test_loss = evaluate(model, test_loader, criterion, test_loader_length)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test on some random news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_text = text_transform[SRC_LANGUAGE](sample[0]).to(device)\n",
    "src_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trg_text = text_transform[TRG_LANGUAGE](sample[1]).to(device)\n",
    "trg_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_text = src_text.reshape(1, -1)  #because batch_size is 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trg_text = trg_text.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_text.shape, trg_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_length = torch.tensor([src_text.size(0)]).to(dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(save_path))\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output, attentions = model(src_text, trg_text) #turn off teacher forcing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.shape #batch_size, trg_len, trg_output_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since batch size is 1, we just take off that dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = output.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall remove the first token since it's zeroes anyway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = output[1:]\n",
    "output.shape #trg_len, trg_output_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we just take the top token with highest probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_max = output.argmax(1) #returns max indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the mapping of the target language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = vocab_transform[TRG_LANGUAGE].get_itos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in output_max:\n",
    "    print(mapping[token.item()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Attention\n",
    "\n",
    "Let's display the attentions to understand how the source text links with the generated text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attentions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are 8 heads, we can look at just 1 head for sake of simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention = attentions[0, 0, :, :]\n",
    "attention.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_tokens = ['<sos>'] + token_transform[SRC_LANGUAGE](sample[0]) + ['<eos>']\n",
    "src_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trg_tokens = ['<sos>'] + [mapping[token.item()] for token in output_max]\n",
    "trg_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker as ticker\n",
    "\n",
    "def display_attention(sentence, translation, attention):\n",
    "    \n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(111)\n",
    "    \n",
    "    attention = attention.squeeze(1).cpu().detach().numpy()\n",
    "    \n",
    "    cax = ax.matshow(attention, cmap='bone')\n",
    "   \n",
    "    ax.tick_params(labelsize=10)\n",
    "    \n",
    "    y_ticks =  [''] + translation\n",
    "    x_ticks =  [''] + sentence \n",
    "     \n",
    "    ax.set_xticklabels(x_ticks, rotation=45)\n",
    "    ax.set_yticklabels(y_ticks)\n",
    "\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_attention(src_tokens, trg_tokens, attention)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "714d3f4db9a58ba7d2f2a9a4fffe577af3df8551aebd380095064812e2e0a6a4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
